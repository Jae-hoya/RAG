{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader, PDFPlumberLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain.retrievers import BM25Retriever, EnsembleRetriever\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from kiwipiepy import Kiwi\n",
    "\n",
    "# reorder\n",
    "from langchain_community.document_transformers import LongContextReorder\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "kiwi = Kiwi()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kiwi_tokenize(docs):\n",
    "    return [token.form for token in kiwi.tokenize(docs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaders = [\n",
    "    PDFPlumberLoader(\"data/Adaptive_RAG.pdf\"),\n",
    "    PDFPlumberLoader(\"data/Naive_RAG.pdf\"),\n",
    "    PDFPlumberLoader('data/RAPTOR_RAG.pdf'),\n",
    "    PDFPlumberLoader('data/Self_RAG.pdf')\n",
    "]\n",
    "\n",
    "docs = []\n",
    "\n",
    "for loader in loaders:\n",
    "    docs.extend(loader.load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "87"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensemble Retriever\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=50)\n",
    "\n",
    "embedding = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "documents = text_splitter.split_documents(docs)\n",
    "\n",
    "chroma_vector_store = Chroma.from_documents(documents, embedding)\n",
    "chroma_retriever = chroma_vector_store.as_retriever()\n",
    "\n",
    "kiwi_vector_store = BM25Retriever.from_documents(documents, preprocess_func=kiwi_tokenize)\n",
    "\n",
    "ensemble_retriever = EnsembleRetriever(\n",
    "    retrievers=[chroma_retriever, kiwi_vector_store],\n",
    "    weights=[0.5, 0.5],\n",
    "    search_kwargs={\"k\": 10}\n",
    "    #search_type=\"mmr\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MultiQUery Retriever\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "multiquery_retriever = MultiQueryRetriever.from_llm(\n",
    "    retriever = ensemble_retriever,\n",
    "    llm=llm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reorder\n",
    "def reorder_documents(docs):\n",
    "    # 재정렬\n",
    "    reordering = LongContextReorder()\n",
    "    reordered_docs = reordering.transform_documents(docs)\n",
    "    return reordered_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt\n",
    "template = \"\"\"\n",
    "    당신은 주어진 문서에 대해서 QA를 해주는 assistant bot입니다.\n",
    "    주어진 문서를 이용해서 답변해 주세요\n",
    "    만약 주어진 질문에 대해서 모른다면, 모른다고 답변해 주세요\n",
    "    문서의 출처와 페이지 넘버를 작성해주세요\n",
    "    한국어로 답변해주세요\n",
    "\n",
    "    #Example Format:\n",
    "    (brief summary of the answer)\n",
    "    (table)\n",
    "    (detailed answer to the question)\n",
    "\n",
    "    **출처**\n",
    "    - (page source and page number)\n",
    "\n",
    "    # Question:\n",
    "    {question}\n",
    "\n",
    "    # Context:\n",
    "    {context}\n",
    "\n",
    "    # Answer\n",
    "\"\"\"\n",
    "prompt = PromptTemplate.from_template(template, input_variable=[\"question\", \"context\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chain\n",
    "chain = ({\"context\":ensemble_retriever| RunnableLambda(reorder_documents), \"question\": RunnablePassthrough()}\n",
    "         |prompt\n",
    "         |llm\n",
    "         |StrOutputParser()\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adaptive-RAG는 적응형 검색 기반 생성 모델로, 복잡한 질문에 대해 효과적으로 대응하기 위해 설계되었습니다. 이 모델은 질문의 복잡성을 분류하는 오라클 분류기를 사용하여, 단순한 질문과 복잡한 질문에 대해 각각 다른 접근 방식을 적용합니다. Adaptive-RAG는 LLM(대형 언어 모델)과 검색기를 반복적으로 활용하여 최적의 답변을 도출합니다.\n",
      "\n",
      "| 특징                | 설명                                                                 |\n",
      "|-------------------|--------------------------------------------------------------------|\n",
      "| 모델 유형           | 적응형 검색 기반 생성 모델                                             |\n",
      "| 질문 처리 방식      | 질문의 복잡성에 따라 다른 전략을 적용                                   |\n",
      "| 사용 기술           | LLM과 검색기를 반복적으로 활용하여 답변 도출                          |\n",
      "| 성능               | 복잡한 질문에 대해 효과적이며, 효율성 또한 높음                        |\n",
      "\n",
      "Adaptive-RAG는 단순한 질문에 대해서는 LLM의 패러미터 지식을 활용하고, 복잡한 질문에 대해서는 검색 기반 접근 방식을 통해 답변을 생성합니다. 이 모델은 다양한 질문 유형에 적응할 수 있는 능력을 가지고 있으며, 실험 결과에서 다른 모델들보다 더 효과적이고 효율적인 성능을 보여주었습니다.\n",
      "\n",
      "**출처**\n",
      "- Adaptive_RAG.pdf, 페이지 6, 7, 13\n"
     ]
    }
   ],
   "source": [
    "print(chain.invoke(\"AdaptiveRAG에 대해서 설명해 주세요\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Self-RAG(자기 반영적 검색 증강 생성)는 대형 언어 모델(LLM)의 품질과 사실성을 향상시키기 위해 설계된 프레임워크입니다. 이 방법은 검색과 자기 반영을 통해 LLM이 필요에 따라 텍스트를 생성하고, 생성된 결과를 비판적으로 평가할 수 있도록 학습합니다. Self-RAG는 일반적인 RAG 접근 방식과 달리, 검색된 정보의 완전한 지원을 보장하며, LLM의 창의성과 다재다능성을 해치지 않으면서도 사실적 정확성을 높이는 데 중점을 둡니다.\n",
      "\n",
      "| Self-RAG의 주요 특징 |\n",
      "|---------------------|\n",
      "| 검색과 자기 반영을 통한 품질 향상 |\n",
      "| 필요에 따라 텍스트 생성 |\n",
      "| 생성된 결과에 대한 비판적 평가 |\n",
      "| 사실적 정확성 유지 |\n",
      "| LLM의 창의성과 다재다능성 보존 |\n",
      "\n",
      "Self-RAG는 LLM이 작업 입력에 따라 자신의 생성 과정을 반영하고, 필요할 경우 검색된 구문을 기반으로 텍스트를 생성하도록 훈련됩니다. 이 과정에서 LLM은 '반영 토큰'을 생성하여 검색의 필요성을 신호하거나 출력의 관련성, 지원 또는 완전성을 확인합니다. 이러한 방식은 LLM이 제공된 구문에서 사실을 활용하고 따르도록 명시적으로 훈련되지 않은 기존 RAG 접근 방식의 한계를 극복합니다.\n",
      "\n",
      "**출처**\n",
      "- Self-RAG.pdf, 페이지 2, 0, 1, 9, 15\n"
     ]
    }
   ],
   "source": [
    "print(chain.invoke(\"Self-RAG에 대해서 설명해 주세요\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-kr-gLkynrUQ-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
