{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델 컴포넌트 실습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='chatcmpl-AArePtP6mIBH6iVKyTKm51NwuR50f', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='2002년 FIFA 월드컵의 4강 국가들은 다음과 같습니다:\\n\\n1. 브라질\\n2. 독일\\n3. 터키\\n4. 대한민국\\n\\n최종 결승에서는 브라질이 독일을 이기고 우승을 차지했습니다.', refusal=None, role='assistant', function_call=None, tool_calls=None))], created=1727152877, model='gpt-4o-mini-2024-07-18', object='chat.completion', service_tier=None, system_fingerprint='fp_e9627b5346', usage=CompletionUsage(completion_tokens=59, prompt_tokens=19, total_tokens=78, completion_tokens_details=CompletionTokensDetails(reasoning_tokens=0)))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "client.chat.completions.create(\n",
    "    model = \"gpt-4o-mini\",\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\" : \"user\",\n",
    "            \"content\": \"2002년 월드컵 4강 국가 알려줘\"\n",
    "        }\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='안녕하세요! 저는 AI 언어 모델로, 다양한 질문에 답변하고 정보를 제공하는 역할을 합니다. 글쓰기, 정보 검색, 문제 해결 등 여러 가지 주제에 대해 도와드릴 수 있습니다. 무엇을 도와드릴까요?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 53, 'prompt_tokens': 17, 'total_tokens': 70, 'completion_tokens_details': {'reasoning_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_1bb46167f9', 'finish_reason': 'stop', 'logprobs': None}, id='run-469fe5f1-5c76-4deb-a85f-603bb622eb15-0', usage_metadata={'input_tokens': 17, 'output_tokens': 53, 'total_tokens': 70})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "chat = ChatOpenAI(\n",
    "    model_name=\"gpt-4o-mini\"\n",
    ")\n",
    "chat.invoke(\"안녕? 너를 소개해줄래?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PromptTemplate 실습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['개수', '재료'], template='\\n    너는 20년차 전문 요리사야 내가 가진 재료들을 갖고 만들수 있는 요리를 {개수}추천하고,\\n    그 요리의 레시피를 제시해줘. 내가 가진 재료는 다음과 같아\\n    <재료>\\n    {재료}\\n\\n    ')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    너는 20년차 전문 요리사야 내가 가진 재료들을 갖고 만들수 있는 요리를 {개수}추천하고,\n",
    "    그 요리의 레시피를 제시해줘. 내가 가진 재료는 다음과 같아\n",
    "    <재료>\n",
    "    {재료}\n",
    "\n",
    "    \"\"\"\n",
    ")\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StringPromptValue(text='\\n    너는 20년차 전문 요리사야 내가 가진 재료들을 갖고 만들수 있는 요리를 6추천하고,\\n    그 요리의 레시피를 제시해줘. 내가 가진 재료는 다음과 같아\\n    <재료>\\n    사괘, 잼\\n\\n    ')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt.invoke({\"개수\":6, \"재료\":\"사괘, 잼\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "chat_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        # 시스템 프롬프트로 유용한 챗봇이라고하며, 이름을 전달\n",
    "        (\"system\", \"너는 유용한 ai봇이야. 너의 이름은 {name}이야.\"),\n",
    "        # 히스토리 대입\n",
    "        (\"human\", \"안녕 오늘 좀 어떄?\"),\n",
    "        (\"ai\", \"좋아! 고마워\"),\n",
    "        # 휴먼메세지로 프롬프트 전달\n",
    "        (\"human\", \"{user_input}\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SystemMessage(content='너는 유용한 ai봇이야. 너의 이름은 jaeho이야.'), HumanMessage(content='안녕 오늘 좀 어떄?'), AIMessage(content='좋아! 고마워'), HumanMessage(content='너의 이름은 뭐야?')]\n"
     ]
    }
   ],
   "source": [
    "messages = chat_template.format_messages(name=\"jaeho\", user_input=\"너의 이름은 뭐야?\")\n",
    "print(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LECL로 Chain 구축하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_template(\"Tell me short joke about {topic}\")\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "chain = prompt| model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Why did the robot go on a diet?\\n\\nBecause it had too many bytes!', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 16, 'prompt_tokens': 13, 'total_tokens': 29, 'completion_tokens_details': {'reasoning_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_1bb46167f9', 'finish_reason': 'stop', 'logprobs': None}, id='run-79ac7673-6651-4faf-8cb0-232d38e20d44-0', usage_metadata={'input_tokens': 13, 'output_tokens': 16, 'total_tokens': 29})"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"topic\": \"AI\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Why did the AI go to therapy?\\n\\nBecause it had too many unresolved algorithms!'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"topic\": \"AI\"}).content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_template(\"Tell me short joke about {topic}\")\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "chain = prompt| model | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Why did the AI go broke?\\n\\nBecause it lost its cache!'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"topic\": \"AI\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why do bears have hairy coats?\n",
      "\n",
      "Because they look silly in sweaters!"
     ]
    }
   ],
   "source": [
    "model = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "prompt = ChatPromptTemplate.from_template(\"tell me a joke about {topic}\")\n",
    "chain | prompt | model \n",
    "# 스트리밍으로 출력\n",
    "\n",
    "for s in chain.stream({\"topic\": \"bears\"}):\n",
    "    print(s.content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt 연습하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why do bears have hairy coats?\n",
      "\n",
      "Because they look silly in sweaters!"
     ]
    }
   ],
   "source": [
    "model = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "prompt = ChatPromptTemplate.from_template(\"tell me a joke about {topic}\")\n",
    "chain | prompt | model\n",
    "# 스트리밍으로 출력\n",
    "\n",
    "for s in chain.stream({\"topic\": \"bears\"}):\n",
    "    print(s.content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output Parser 실습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"['다크 나이트', '매드 맥스: 분노의 도로', '존 윅']\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 24, 'prompt_tokens': 73, 'total_tokens': 97, 'completion_tokens_details': {'reasoning_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_1bb46167f9', 'finish_reason': 'stop', 'logprobs': None}, id='run-c5380425-38af-4967-abf1-09be6fe612c8-0', usage_metadata={'input_tokens': 73, 'output_tokens': 24, 'total_tokens': 97})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import HumanMessagePromptTemplate\n",
    "from langchain_core.messages import SystemMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "chat_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessage(\n",
    "            content=(\n",
    "                \"너는 영화 전문가 ai야. 사용자가 원하는 장르의 영화를 리스트 형태로 추천해줘\"\n",
    "                \"ex)   Query: SF영화 3개 추천해줘. / 답변 ['인터스텔라', '스페이스오디세이', '혹성탈출']\"\n",
    "            )\n",
    "        ),\n",
    "        HumanMessagePromptTemplate.from_template(\"{text}\")\n",
    "    ],\n",
    ")\n",
    "\n",
    "chain = chat_template|llm\n",
    "chain.invoke(\"액션\")\n",
    "# print(chain.invoke(\"액션\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['사이코', '샤이닝', '엑소시스트', '할로윈', '콘저링', '그루지', '곡성', '이웃사람', '원인모를 죽음', '미드소마']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 콤마로 출력할수 있게 하는 방법\n",
    "from langchain_core.output_parsers import CommaSeparatedListOutputParser\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "output_parser = CommaSeparatedListOutputParser()\n",
    "\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template = \"List {subject}. answer in korean \\n {format_instructions}\",\n",
    "    input_variables = [\"subject\"],\n",
    "    partial_variables={\"format_instructions\": format_instructions}\n",
    ")\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "chain = prompt | model | output_parser\n",
    "chain.invoke({\"subject\": \"공포 영화\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Your response should be a list of comma separated values, eg: `foo, bar, baz` or `foo,bar,baz`'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "format_instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'continent': '아시아', 'population': '약 5,200만'}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Json Output Parser - Pydantic\n",
    "from typing import List\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "class Country(BaseModel):\n",
    "    continent: str = Field(description=\"사용자가 물어본 나라가 속한 대륙\")\n",
    "    population: str = Field(description=\"사용자가 물어본 나라의 인구(int형식)\")\n",
    "\n",
    "parser = JsonOutputParser(pydantic_object=Country)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template = \"Answer the user query and Korean. \\n {format_instructions}\\n{query}\\n\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\":parser.get_format_instructions()}\n",
    ")\n",
    "\n",
    "chain = prompt | model | parser\n",
    "\n",
    "contry_query = \"대한민국은 어떤나라야?\"\n",
    "chain.invoke({\"query\": contry_query})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LCEL Runnable 객체에 대해 알아보기\n",
    "\n",
    "runnable 객체로 연결되어 있고, 이것을 체인으로 엮는다.\n",
    "\n",
    "- Runnable은 함수형 프로그래밍 스타일을 활용하여, 단일 또는 다중 입력을 받아들여 처리하는 객체를 생성하고 실행하는 데 사용됩니다.\n",
    "\n",
    "### RunnablePassthrough\n",
    "들어온 객체를 그대로 출력한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'안녕하세요'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "RunnablePassthrough().invoke(\"안녕하세요\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"다음 한글 문장을 영어로 번역해줘. \n",
    "                                          # Sentence\n",
    "                                          {sentence}\n",
    "                                          #English sentence: \n",
    "\n",
    "                                          \"\"\")\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "runnable_chain = ({\"sentence\": RunnablePassthrough()}\n",
    "                  | prompt\n",
    "                  | model\n",
    "                  | output_parser)\n",
    "\n",
    "response = runnable_chain.invoke(\"안녕 내이름은 재호야. 나는 진원이를 정말 사랑해\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, my name is Jae-ho. I really love Jin-won.\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'num': 3, 'mult': 9}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# assign기능 : 특정 변수를 하나 더 만들 수 있다. 이것은 입력된 입력값에 대한 변수를 만드는 함수이다.\n",
    "(RunnablePassthrough.assign(mult=lambda x: x[\"num\"]*3)).invoke({\"num\":3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'extra': {'num': 5, 'mult': 15}, 'modified': 6}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 병렬처리하는 RunnablePassthrough\n",
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "\n",
    "runnable = RunnableParallel(\n",
    "    extra=RunnablePassthrough.assign(mult=lambda x: x[\"num\"] * 3),\n",
    "    modified=lambda x: x[\"num\"] + 1,\n",
    ")\n",
    "\n",
    "runnable.invoke({\"num\" : 5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RunnableLambda : 사용자가 정의한 임의의 함수에 Runnable 객체를 넣어주는것\n",
    "def add_smile(x):\n",
    "    return x + \":)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "add_smile = RunnableLambda(add_smile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "prompt_str = \"{topic}의 역사에 대해 세문장으로 설명해 주세요\"\n",
    "prompt = ChatPromptTemplate.from_template(prompt_str)\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "chain = prompt | model | output_parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Runnable Parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "def add_thank(x):\n",
    "    return x + \" 들어주셔서 감사합니다 :)\"\n",
    "add_thank = RunnableLambda(add_thank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "인공지능(AI)의 역사는 1950년대에 앨런 튜링의 \"튜링 테스트\" 개념과 함께 시작되었습니다. 1960년대와 70년대에는 기계 학습과 전문가 시스템 개발이 활발히 이루어졌으나, 기술적 한계로 인해 'AI 겨울'이라 불리는 침체기가 찾아왔습니다. 21세기에 들어서면서 빅데이터와 강력한 컴퓨팅 파워의 발전으로 AI는 다시금 급속히 발전하며 다양한 분야에서 응용되고 있습니다. 들어주셔서 감사합니다 :)\n"
     ]
    }
   ],
   "source": [
    "chain = prompt | model | output_parser | add_thank\n",
    "response = chain.invoke(\"ai\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'passed': {'num': 1}, 'modified': 2}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "\n",
    "runnable = RunnableParallel(\n",
    "    passed=RunnablePassthrough(),\n",
    "    modified=lambda x: x[\"num\"] + 1,\n",
    ")\n",
    "\n",
    "runnable.invoke({\"num\": 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'passed': '안녕하세요.', 'modified': '안녕하세요. 들어주셔서 감사합니다 :)'}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "runnable = RunnableParallel(\n",
    "    passed=RunnablePassthrough(),\n",
    "    modified=add_thank,\n",
    ")\n",
    "runnable.invoke(\"안녕하세요.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'history': 'AI는 \"Artificial Intelligence\"의 약자로, 한국어로는 \"인공지능\"이라고 합니다. 인공지능은 컴퓨터 시스템이 인간의 지능을 모방하여 학습, 추론, 문제 해결, 이해 등의 작업을 수행할 수 있도록 하는 기술을 의미합니다.', 'celeb': 'AI 분야의 유명인사 중 세 명은 다음과 같습니다:\\n\\n1. **앤드류 응 (Andrew Ng)**: 스탠포드 대학교 교수이자 구글 브레인(Google Brain) 공동 창립자이며, Coursera의 공동 창립자입니다. 그는 머신러닝과 딥러닝 분야에서 많은 기여를 했습니다.\\n\\n2. **제프리 힌튼 (Geoffrey Hinton)**: 딥러닝의 아버지로 불리며, 인공지능 분야에서 중요한 연구를 수행한 인물입니다. 그는 토론토 대학교'}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableParallel\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\", max_tokens=128, temperature=0)\n",
    "\n",
    "history_prompt = ChatPromptTemplate.from_template(\"{topic}가 무엇의 약자인지 알려주세요\")\n",
    "celeb_prompt = ChatPromptTemplate.from_template(\"{topic}분야의 유명인사 3명만 알려주세요\")\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "history_chain = history_prompt | model | output_parser\n",
    "celeb_chain = celeb_prompt | model | output_parser\n",
    "\n",
    "map_chain = RunnableParallel(history=history_chain, celeb=celeb_chain)\n",
    "\n",
    "result = map_chain.invoke({\"topic\": \"AI\"})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-kr-gLkynrUQ-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
