{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 사용자 정의 Evaluator로평가\n",
    "\n",
    "사용자 정의 LLM평가자를 구성하거나 Heuristic평가자를 구성할 수 있다.\n",
    "\n",
    "LLM이 답변이 좋은지 나쁜지 판단하는데, 룰 기반의 평가자를 만들수도 있다.\n",
    "\n",
    "어떻게보면 이게 제일 유용할 수 있다. \n",
    "\n",
    "prompting방식으로 평가자를 만드는 것이다.\n",
    "\n",
    "**RUN은 LLM의 답변이 담기는 매개변수, Example은 정답 답변이다.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangSmith 추적을 시작합니다.\n",
      "[프로젝트명]\n",
      "CH16-Evaluations\n"
     ]
    }
   ],
   "source": [
    "from langchain_teddynote import logging\n",
    "logging.langsmith(\"CH16-Evaluations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"삼성전자가 자체 개발한 생성형 AI의 이름은 '삼성 가우스'입니다.\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from myrag import PDFRAG\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "rag = PDFRAG(\n",
    "    \"data/SPRI_AI_Brief_2023년12월호_F.pdf\",\n",
    "    ChatOpenAI(model=\"gpt-4o-mini\")\n",
    ")\n",
    "\n",
    "retriever = rag.create_retriever()\n",
    "\n",
    "chain = rag.create_chain(retriever)\n",
    "\n",
    "chain.invoke(\"삼성전자가 자체 개발한 생성형 AI의 이름은?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_question(inputs:dict):\n",
    "    return {\"answer\": chain.invoke(inputs[\"question\"])}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 사용자 정의 Evaluator 구성\n",
    "아래의 사용자 정의 함수의 입력 매개변수와 반환 값 형식을 지켜서 생성할 수 있습니다.\n",
    "\n",
    "**사용자 정의 함수**\n",
    "\n",
    "- 입력으로는 `Run` 과 `Example` 을 받고 출력으로는 `dict` 를 반환합니다.\n",
    "- 반환 값은 `{\"key\": \"score_name\", \"score\": score}` 형식으로 구성됩니다.\n",
    "아래는 간단한 예시 함수를 정의하였습니다. 답변에 상관없이 1~10 사이의 랜덤 점수를 반환합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith.schemas import Run, Example\n",
    "import random\n",
    "\n",
    "def random_score_evaluator(run: Run, example: Example) -> dict:\n",
    "    score = random.randint(0,10)\n",
    "    return{\"key\": \"randon_score\", \"score\": score}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'CUSTOM-EVAL-a8a44c5e' at:\n",
      "https://smith.langchain.com/o/2d0ce887-3f7f-59af-8d5e-12c1371ef5d5/datasets/334d943e-3086-4d03-91ae-4b5145224ffc/compare?selectedSessions=3bcb15ef-c0b9-46b0-890c-70f02851ae2b\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b114b12c38664c8d82265241e1034512",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error running target function: 'question'\n",
      "Error running target function: 'question'\n",
      "Error running target function: 'question'\n",
      "Error running target function: 'question'\n"
     ]
    }
   ],
   "source": [
    "from langsmith.evaluation import evaluate\n",
    "\n",
    "dataset_name = \"RAG_EVAL_DATASET_teddynote\"\n",
    "\n",
    "experiment_results = evaluate(\n",
    "    ask_question,\n",
    "    data=dataset_name,\n",
    "    evaluators=[random_score_evaluator],\n",
    "    experiment_prefix=\"CUSTOM-EVAL\",\n",
    "    metadata={\n",
    "        \"variant\": \"랜덤 점수 평가자\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom LLM-as-Judge\n",
    "이번에는 LLM Chain 을 만들어서 평가자로 활용하겠습니다.\n",
    "\n",
    "먼저, `context`, `answer`, `question` 을 반환하는 함수를 정의합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def context_answer_rag_answer(inputs:dict):\n",
    "    context = retriever .invoke(inputs[\"question\"])\n",
    "    return {\n",
    "        \"context\": \"\\n\".join([doc.page_content for doc in context]),\n",
    "        \"answer\": chain.invoke(inputs[\"question\"]),\n",
    "        \"question\": inputs[\"question\"]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "As an LLM evaluator (judge), please assess the LLM's response to the given question. Evaluate the response's accuracy, comprehensiveness, and context precision based on the provided context. After your evaluation, return only the numerical scores in the following format:\n",
      "Accuracy: [score]\n",
      "Comprehensiveness: [score]\n",
      "Context Precision: [score]\n",
      "Final: [normalized score]\n",
      "Grading rubric:\n",
      "\n",
      "Accuracy (0-10 points):\n",
      "Evaluate how well the answer aligns with the information provided in the given context.\n",
      "\n",
      "0 points: The answer is completely inaccurate or contradicts the provided context\n",
      "4 points: The answer partially aligns with the context but contains significant inaccuracies\n",
      "7 points: The answer mostly aligns with the context but has minor inaccuracies or omissions\n",
      "10 points: The answer fully aligns with the provided context and is completely accurate\n",
      "\n",
      "\n",
      "Comprehensiveness (0-10 points):\n",
      "\n",
      "0 points: The answer is completely inadequate or irrelevant\n",
      "3 points: The answer is accurate but too brief to fully address the question\n",
      "7 points: The answer covers main aspects but lacks detail or misses minor points\n",
      "10 points: The answer comprehensively covers all aspects of the question\n",
      "\n",
      "\n",
      "Context Precision (0-10 points):\n",
      "Evaluate how precisely the answer uses the information from the provided context.\n",
      "\n",
      "0 points: The answer doesn't use any information from the context or uses it entirely incorrectly\n",
      "4 points: The answer uses some information from the context but with significant misinterpretations\n",
      "7 points: The answer uses most of the relevant context information correctly but with minor misinterpretations\n",
      "10 points: The answer precisely and correctly uses all relevant information from the context\n",
      "\n",
      "\n",
      "Final Normalized Score:\n",
      "Calculate by summing the scores for accuracy, comprehensiveness, and context precision, then dividing by 30 to get a score between 0 and 1.\n",
      "Formula: (Accuracy + Comprehensiveness + Context Precision) / 30\n",
      "\n",
      "#Given question:\n",
      "\u001b[33;1m\u001b[1;3m{question}\u001b[0m\n",
      "\n",
      "#LLM's response:\n",
      "\u001b[33;1m\u001b[1;3m{answer}\u001b[0m\n",
      "\n",
      "#Provided context:\n",
      "\u001b[33;1m\u001b[1;3m{context}\u001b[0m\n",
      "\n",
      "Please evaluate the LLM's response according to the criteria above. \n",
      "\n",
      "In your output, include only the numerical scores for FINAL NORMALIZED SCORE without any additional explanation or reasoning.\n",
      "ex) 0.81\n",
      "\n",
      "#Final Normalized Score(Just the number):\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain import hub\n",
    "\n",
    "llm_evaluator_prompt = hub.pull(\"teddynote/context-answer-evaluator\")\n",
    "llm_evaluator_prompt.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "custom_llm_evaluator = (\n",
    "    llm_evaluator_prompt\n",
    "    | ChatOpenAI(temperature=0, model=\"gpt-4o-mini\")\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = context_answer_rag_answer(\n",
    "    {\"question\": \"삼성전자가 자체 개발한 생성형 AI의 이름은?\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'context': '▹ 삼성전자, 자체 개발 생성 AI ‘삼성 가우스’ 공개 ··························································· 10\\n   ▹ 구글, 앤스로픽에 20억 달러 투자로 생성 AI 협력 강화 ················································ 11\\n   ▹ IDC, 2027년 AI 소프트웨어 매출 2,500억 달러 돌파 전망··········································· 12\\nSPRi AI Brief |  \\n2023-12월호\\n10\\n삼성전자, 자체 개발 생성 AI ‘삼성 가우스’ 공개\\nn 삼성전자가 온디바이스에서 작동 가능하며 언어, 코드, 이미지의 3개 모델로 구성된 자체 개발 생성 \\nAI 모델 ‘삼성 가우스’를 공개\\nn 삼성전자는 삼성 가우스를 다양한 제품에 단계적으로 탑재할 계획으로, 온디바이스 작동이 가능한 \\n삼성 가우스는 외부로 사용자 정보가 유출될 위험이 없다는 장점을 보유\\nKEY Contents\\n£ 언어, 코드, 이미지의 3개 모델로 구성된 삼성 가우스, 온디바이스 작동 지원\\n£ 언어, 코드, 이미지의 3개 모델로 구성된 삼성 가우스, 온디바이스 작동 지원\\nn 삼성전자가 2023년 11월 8일 열린 ‘삼성 AI 포럼 2023’ 행사에서 자체 개발한 생성 AI 모델 \\n‘삼성 가우스’를 최초 공개\\n∙정규분포 이론을 정립한 천재 수학자 가우스(Gauss)의 이름을 본뜬 삼성 가우스는 다양한 상황에 \\n최적화된 크기의 모델 선택이 가능\\n∙삼성 가우스는 라이선스나 개인정보를 침해하지 않는 안전한 데이터를 통해 학습되었으며, \\n온디바이스에서 작동하도록 설계되어 외부로 사용자의 정보가 유출되지 않는 장점을 보유\\n어시스턴트를 적용한 구글 픽셀(Pixel)과 경쟁할 것으로 예상\\n☞ 출처 : 삼성전자, ‘삼성 AI 포럼’서 자체 개발 생성형 AI ‘삼성 가우스’ 공개, 2023.11.08.\\n삼성전자, ‘삼성 개발자 콘퍼런스 코리아 2023’ 개최, 2023.11.14.\\nTechRepublic, Samsung Gauss: Samsung Research Reveals Generative AI, 2023.11.08.', 'answer': \"삼성전자가 자체 개발한 생성형 AI의 이름은 '삼성 가우스'입니다.\", 'question': '삼성전자가 자체 개발한 생성형 AI의 이름은?'}\n"
     ]
    }
   ],
   "source": [
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`custom_evaluator` 함수를 정의합니다.\n",
    "\n",
    "- `run.outputs`: RAG 체인이 생성한 answer, context, question 을 가져옵니다.\n",
    "- `example.outputs`: 데이터셋의 정답 답변을 가져옵니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith.schemas import Run, Example\n",
    "\n",
    "def custom_evaluator(run: Run, example: Example) -> dict:\n",
    "    llm_answer = run.outputs.get(\"answer\", \"\")\n",
    "    context = run.outputs.get(\"context\", \"\")\n",
    "    question = example.outputs.get(\"question\", \"\")\n",
    "\n",
    "    # 랜덤 점수 반환\n",
    "    score = custom_llm_evaluator.invoke(\n",
    "        {\"question\": question, \"answer\": llm_answer, \"context\": context}\n",
    "    )\n",
    "    return {\"key\": \"custom_score\", \"score\": float(score)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'CUSTOM-LMM-EVAL-ea406cb2' at:\n",
      "https://smith.langchain.com/o/2d0ce887-3f7f-59af-8d5e-12c1371ef5d5/datasets/334d943e-3086-4d03-91ae-4b5145224ffc/compare?selectedSessions=8c023e7a-744b-4d7a-842c-42708c84d1c1\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2026f6cff104f6abfe7593d5c408e79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error running target function: 'question'\n",
      "Error running target function: 'question'\n",
      "Error running target function: 'question'\n",
      "Error running target function: 'question'\n"
     ]
    }
   ],
   "source": [
    "from langsmith.evaluation import evaluate\n",
    "\n",
    "dataset_name = \"RAG_EVAL_DATASET_teddynote\"\n",
    "\n",
    "experiment_results = evaluate(\n",
    "    context_answer_rag_answer,\n",
    "    data=dataset_name,\n",
    "    evaluators=[custom_evaluator],\n",
    "    experiment_prefix=\"CUSTOM-LMM-EVAL\",\n",
    "    metadata={\n",
    "        \"variant\": \"Custom LLM Evaluator 활용한 평가\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-kr-gLkynrUQ-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
