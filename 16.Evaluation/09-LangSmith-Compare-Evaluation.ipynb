{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "#실험(Experiment) 평가 비교\n",
    "\n",
    "LangSmith 에서 제공하는 Compare 기능을 활용하여 실험 결과를 쉽게 비교할 수 있습니다.\n",
    "\n",
    "**참고**\n",
    "\n",
    "https://docs.smith.langchain.com/cookbook/tracing-examples/traceable#using-the-decorator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangSmith 추적을 시작합니다.\n",
      "[프로젝트명]\n",
      "CH16-Evaluations\n"
     ]
    }
   ],
   "source": [
    "from langchain_teddynote import logging\n",
    "logging.langsmith(\"CH16-Evaluations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from myrag import PDFRAG\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "def ask_question_with_llm(llm):\n",
    "    rag=PDFRAG(\n",
    "        \"data/SPRI_AI_Brief_2023년12월호_F.pdf\",\n",
    "        ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "    )\n",
    "    retriever = rag.create_retriever()\n",
    "    rag_chain = rag.create_chain(retriever)\n",
    "\n",
    "    def _ask_question(inputs: dict):\n",
    "        context = retriever.invoke(inputs[\"question\"])\n",
    "        context = \"\\n\".join([doc.page_content for doc in context])\n",
    "        return {\n",
    "            \"question\": inputs[\"question\"],\n",
    "            \"context\": context,\n",
    "            \"answer\": rag_chain.invoke(inputs[\"question\"])\n",
    "        }\n",
    "    return _ask_question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='안녕하세요! 저는 도움이 되고 존중하는 조수로서 여러분의 질문에 최선을 다해 답변하고, 안전하고 유익한 정보를 제공하기 위해 여기에 있습니다.\\r\\n\\r\\n질문이나 요청 사항이 있으시면 언제든 물어보세요. 정확하고 유용한 답변을 드리기 위해 최선을 다하겠습니다.\\r\\n\\r\\n도움이 되셨길 바라며, 궁금한 점이나 걱정되는 점이 있으면 언제든지 편하게 말씀해 주세요!', response_metadata={'model': 'EEVE-Korean-10.8B:latest', 'created_at': '2024-10-06T14:51:35.3247923Z', 'message': {'role': 'assistant', 'content': ''}, 'done_reason': 'stop', 'done': True, 'total_duration': 10492631300, 'load_duration': 7394053300, 'prompt_eval_count': 50, 'prompt_eval_duration': 124952000, 'eval_count': 84, 'eval_duration': 2967248000}, id='run-4c0b3903-e5d8-408b-8f9d-34acde642586-0')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.chat_models import ChatOllama\n",
    "\n",
    "# Ollama 모델을 불러옵니다.\n",
    "ollama = ChatOllama(model=\"EEVE-Korean-10.8B:latest\")\n",
    "\n",
    "# Ollama 모델 호출\n",
    "ollama.invoke(\"안녕하세요?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='하이! 안녕하세요! 어떻게 도와드릴까요?', response_metadata={'model': 'llama3.2:latest', 'created_at': '2024-10-06T14:51:47.8401906Z', 'message': {'role': 'assistant', 'content': ''}, 'done_reason': 'stop', 'done': True, 'total_duration': 3374108800, 'load_duration': 3166296200, 'prompt_eval_count': 28, 'prompt_eval_duration': 50015000, 'eval_count': 14, 'eval_duration': 154335000}, id='run-c80e3ba1-69d0-4c35-a2c8-460d07f18362-0')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.chat_models import ChatOllama\n",
    "\n",
    "# Ollama 모델을 불러옵니다.\n",
    "ollama = ChatOllama(model=\"llama3.2:latest\")\n",
    "\n",
    "# Ollama 모델 호출\n",
    "ollama.invoke(\"안녕하세요?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "gpt_chain = ask_question_with_llm(ChatOpenAI(model=\"gpt-4o-mini\", temperature=0))\n",
    "ollama_chain = ask_question_with_llm(ChatOllama(model=\"EEVE-Korean-10.8B:latest\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith.evaluation import evaluate, LangChainStringEvaluator\n",
    "\n",
    "# qa 평가자 생성\n",
    "cot_qa_evalulator = LangChainStringEvaluator(\n",
    "    \"cot_qa\",\n",
    "    config={\"llm\": ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)},\n",
    "    prepare_data=lambda run, example: {\n",
    "        \"prediction\": run.outputs[\"answer\"],\n",
    "        \"reference\": run.outputs[\"context\"],\n",
    "        \"input\": example.inputs[\"question\"],\n",
    "    },\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'MODEL_COMPARE_EVAL-b7c9d702' at:\n",
      "https://smith.langchain.com/o/2d0ce887-3f7f-59af-8d5e-12c1371ef5d5/datasets/334d943e-3086-4d03-91ae-4b5145224ffc/compare?selectedSessions=43196ad0-54f8-43a0-aa03-e01c4b8e865c\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55adb21ee88a4cf1869255afab867d26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error running target function: 'question'\n",
      "Error running target function: 'question'\n",
      "Error running target function: 'question'\n",
      "Error running target function: 'question'\n",
      "Error running evaluator <DynamicRunEvaluator evaluate> on run 817f431b-1ed4-49ee-9e24-02c022e01d37: KeyError('answer')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\skyop\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\langchain-kr-gLkynrUQ-py3.11\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1345, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\skyop\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\langchain-kr-gLkynrUQ-py3.11\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 327, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"c:\\Users\\skyop\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\langchain-kr-gLkynrUQ-py3.11\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 646, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\skyop\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\langchain-kr-gLkynrUQ-py3.11\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 643, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\skyop\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\langchain-kr-gLkynrUQ-py3.11\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 258, in evaluate\n",
      "    else self._prepare_data(run, example)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\skyop\\AppData\\Local\\Temp\\ipykernel_12168\\383441650.py\", line 8, in <lambda>\n",
      "    \"prediction\": run.outputs[\"answer\"],\n",
      "                  ~~~~~~~~~~~^^^^^^^^^^\n",
      "KeyError: 'answer'\n",
      "Error running evaluator <DynamicRunEvaluator evaluate> on run 52b3a80b-1279-4a38-98ef-bf9f48e404fd: KeyError('answer')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\skyop\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\langchain-kr-gLkynrUQ-py3.11\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1345, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\skyop\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\langchain-kr-gLkynrUQ-py3.11\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 327, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"c:\\Users\\skyop\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\langchain-kr-gLkynrUQ-py3.11\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 646, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\skyop\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\langchain-kr-gLkynrUQ-py3.11\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 643, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\skyop\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\langchain-kr-gLkynrUQ-py3.11\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 258, in evaluate\n",
      "    else self._prepare_data(run, example)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\skyop\\AppData\\Local\\Temp\\ipykernel_12168\\383441650.py\", line 8, in <lambda>\n",
      "    \"prediction\": run.outputs[\"answer\"],\n",
      "                  ~~~~~~~~~~~^^^^^^^^^^\n",
      "KeyError: 'answer'\n",
      "Error running evaluator <DynamicRunEvaluator evaluate> on run 8ee516d4-86ac-44e0-b5bc-8f800f49654c: KeyError('answer')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\skyop\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\langchain-kr-gLkynrUQ-py3.11\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1345, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\skyop\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\langchain-kr-gLkynrUQ-py3.11\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 327, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"c:\\Users\\skyop\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\langchain-kr-gLkynrUQ-py3.11\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 646, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\skyop\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\langchain-kr-gLkynrUQ-py3.11\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 643, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\skyop\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\langchain-kr-gLkynrUQ-py3.11\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 258, in evaluate\n",
      "    else self._prepare_data(run, example)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\skyop\\AppData\\Local\\Temp\\ipykernel_12168\\383441650.py\", line 8, in <lambda>\n",
      "    \"prediction\": run.outputs[\"answer\"],\n",
      "                  ~~~~~~~~~~~^^^^^^^^^^\n",
      "KeyError: 'answer'\n",
      "Error running evaluator <DynamicRunEvaluator evaluate> on run 8fbe2f20-6fe7-46f8-8479-dbb1520fab57: KeyError('answer')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\skyop\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\langchain-kr-gLkynrUQ-py3.11\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1345, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\skyop\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\langchain-kr-gLkynrUQ-py3.11\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 327, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"c:\\Users\\skyop\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\langchain-kr-gLkynrUQ-py3.11\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 646, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\skyop\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\langchain-kr-gLkynrUQ-py3.11\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 643, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\skyop\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\langchain-kr-gLkynrUQ-py3.11\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 258, in evaluate\n",
      "    else self._prepare_data(run, example)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\skyop\\AppData\\Local\\Temp\\ipykernel_12168\\383441650.py\", line 8, in <lambda>\n",
      "    \"prediction\": run.outputs[\"answer\"],\n",
      "                  ~~~~~~~~~~~^^^^^^^^^^\n",
      "KeyError: 'answer'\n"
     ]
    }
   ],
   "source": [
    "dataset_name = \"RAG_EVAL_DATASET_teddynote\"\n",
    "# 평가 실행\n",
    "experiment_results1 = evaluate(\n",
    "    gpt_chain,\n",
    "    data=dataset_name,\n",
    "    evaluators=[cot_qa_evalulator],\n",
    "    experiment_prefix=\"MODEL_COMPARE_EVAL\",\n",
    "    # 실험 메타데이터 지정\n",
    "    metadata={\n",
    "        \"variant\": \"GPT-4o-mini 평가 (cot_qa)\",\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'MODEL_COMPARE_EVAL-2e17983d' at:\n",
      "https://smith.langchain.com/o/2d0ce887-3f7f-59af-8d5e-12c1371ef5d5/datasets/334d943e-3086-4d03-91ae-4b5145224ffc/compare?selectedSessions=c9c22a10-e18c-45d5-91cd-e9edd1154040\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f486fb7cdc2f4ac39619905dd223f5b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error running target function: 'question'\n",
      "Error running target function: 'question'\n",
      "Error running target function: 'question'\n",
      "Error running evaluator <DynamicRunEvaluator evaluate> on run c2451f11-f046-4f85-ba30-ea596d3a0227: KeyError('answer')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\skyop\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\langchain-kr-gLkynrUQ-py3.11\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1345, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\skyop\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\langchain-kr-gLkynrUQ-py3.11\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 327, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"c:\\Users\\skyop\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\langchain-kr-gLkynrUQ-py3.11\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 646, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\skyop\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\langchain-kr-gLkynrUQ-py3.11\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 643, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\skyop\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\langchain-kr-gLkynrUQ-py3.11\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 258, in evaluate\n",
      "    else self._prepare_data(run, example)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\skyop\\AppData\\Local\\Temp\\ipykernel_12168\\383441650.py\", line 8, in <lambda>\n",
      "    \"prediction\": run.outputs[\"answer\"],\n",
      "                  ~~~~~~~~~~~^^^^^^^^^^\n",
      "KeyError: 'answer'\n",
      "Error running target function: 'question'\n",
      "Error running evaluator <DynamicRunEvaluator evaluate> on run 76beb123-74b6-4add-8f6c-a38fb5ea4ab2: KeyError('answer')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\skyop\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\langchain-kr-gLkynrUQ-py3.11\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1345, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\skyop\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\langchain-kr-gLkynrUQ-py3.11\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 327, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"c:\\Users\\skyop\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\langchain-kr-gLkynrUQ-py3.11\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 646, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\skyop\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\langchain-kr-gLkynrUQ-py3.11\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 643, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\skyop\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\langchain-kr-gLkynrUQ-py3.11\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 258, in evaluate\n",
      "    else self._prepare_data(run, example)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\skyop\\AppData\\Local\\Temp\\ipykernel_12168\\383441650.py\", line 8, in <lambda>\n",
      "    \"prediction\": run.outputs[\"answer\"],\n",
      "                  ~~~~~~~~~~~^^^^^^^^^^\n",
      "KeyError: 'answer'\n",
      "Error running evaluator <DynamicRunEvaluator evaluate> on run 65bcb609-4a39-45c0-b28b-4e555d1cc0f6: KeyError('answer')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\skyop\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\langchain-kr-gLkynrUQ-py3.11\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1345, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\skyop\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\langchain-kr-gLkynrUQ-py3.11\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 327, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"c:\\Users\\skyop\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\langchain-kr-gLkynrUQ-py3.11\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 646, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\skyop\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\langchain-kr-gLkynrUQ-py3.11\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 643, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\skyop\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\langchain-kr-gLkynrUQ-py3.11\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 258, in evaluate\n",
      "    else self._prepare_data(run, example)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\skyop\\AppData\\Local\\Temp\\ipykernel_12168\\383441650.py\", line 8, in <lambda>\n",
      "    \"prediction\": run.outputs[\"answer\"],\n",
      "                  ~~~~~~~~~~~^^^^^^^^^^\n",
      "KeyError: 'answer'\n",
      "Error running evaluator <DynamicRunEvaluator evaluate> on run 6f42435e-7d80-487a-be41-54fe60fcc00b: KeyError('answer')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\skyop\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\langchain-kr-gLkynrUQ-py3.11\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1345, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\skyop\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\langchain-kr-gLkynrUQ-py3.11\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 327, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"c:\\Users\\skyop\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\langchain-kr-gLkynrUQ-py3.11\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 646, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\skyop\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\langchain-kr-gLkynrUQ-py3.11\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 643, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\skyop\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\langchain-kr-gLkynrUQ-py3.11\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 258, in evaluate\n",
      "    else self._prepare_data(run, example)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\skyop\\AppData\\Local\\Temp\\ipykernel_12168\\383441650.py\", line 8, in <lambda>\n",
      "    \"prediction\": run.outputs[\"answer\"],\n",
      "                  ~~~~~~~~~~~^^^^^^^^^^\n",
      "KeyError: 'answer'\n"
     ]
    }
   ],
   "source": [
    "# 평가 실행\n",
    "experiment_results2 = evaluate(\n",
    "    ollama_chain,\n",
    "    data=dataset_name,\n",
    "    evaluators=[cot_qa_evalulator],\n",
    "    experiment_prefix=\"MODEL_COMPARE_EVAL\",\n",
    "    # 실험 메타데이터 지정\n",
    "    metadata={\n",
    "        \"variant\": \"Ollama(EEVE-Korean-10.8B:latest) 평가 (cot_qa)\",\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-kr-gLkynrUQ-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
