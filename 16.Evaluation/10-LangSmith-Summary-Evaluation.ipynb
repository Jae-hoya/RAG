{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### 요약(Summary) Evaluators\n",
    "일부 메트릭은 실험의 개별 실행이 아닌 전체 실험 수준에서만 정의할 수 있습니다.\n",
    "\n",
    "예를 들어, 데이터 세트에서 시작된 실험의 **모든 실행에 걸쳐 분류자의 평가 점수**를 계산 하고 싶을 수 있습니다.\n",
    "\n",
    "이를 `summary_evaluators` 라고 합니다.\n",
    "\n",
    "이러한 평가자는 하나의 Run과 Example 대신 각각의 목록을 가져옵니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangSmith 추적을 시작합니다.\n",
      "[프로젝트명]\n",
      "CH16-Evaluations\n"
     ]
    }
   ],
   "source": [
    "from langchain_teddynote import logging\n",
    "\n",
    "# 프로젝트 이름을 입력합니다.\n",
    "logging.langsmith(\"CH16-Evaluations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAG 성능 테스트를 위한 함수 정의\n",
    "테스트에 활용할 RAG 시스템을 생성하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from myrag import PDFRAG\n",
    "\n",
    "def ask_question_with_llm(llm):\n",
    "    rag=PDFRAG(\n",
    "        \"data/SPRI_AI_Brief_2023년12월호_F.pdf\",\n",
    "        llm,\n",
    "    )\n",
    "    retriever = rag.create_retriever()\n",
    "\n",
    "    rag_chain = rag.create_chain(retriever)\n",
    "\n",
    "    def _ask_question(inputs:dict):\n",
    "        context = retriever.invoke(inputs[\"question\"])\n",
    "        context = \"\\n\".join([doc.page_content for doc in context])\n",
    "        return {\n",
    "            \"question\": inputs[\"question\"],\n",
    "            \"context\": context,\n",
    "            \"answer\": rag_chain.invoke(inputs[\"question\"])\n",
    "        }\n",
    "    return _ask_question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OpenAIRelevanceGrader 는 질문(Question), 컨텍스트(Context), 답변(Answer) 가 관련성이 있는지 여부를 평가하는 데 사용됩니다.\n",
    "\n",
    "- target=\"retrieval-question\": 질문과 컨텍스트가 관련성이 있는지 여부를 평가합니다.\n",
    "- target=\"retrieval-answer\": 답변과 컨텍스트가 관련성이 있는지 여부를 평가합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_teddynote.evaluator import OpenAIRelevanceGrader\n",
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ChatOllama' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m gpt_chain \u001b[38;5;241m=\u001b[39m ask_question_with_llm(ChatOpenAI(model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt-4o-mini\u001b[39m\u001b[38;5;124m\"\u001b[39m, temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m))\n\u001b[1;32m----> 2\u001b[0m ollama_chain \u001b[38;5;241m=\u001b[39m ask_question_with_llm(\u001b[43mChatOllama\u001b[49m(model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEEVE-Korean-10.8B:latest\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'ChatOllama' is not defined"
     ]
    }
   ],
   "source": [
    "gpt_chain = ask_question_with_llm(ChatOpenAI(model=\"gpt-4o-mini\", temperature=0))\n",
    "ollama_chain = ask_question_with_llm(ChatOllama(model=\"EEVE-Korean-10.8B:latest\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "rq_grader = OpenAIRelevanceGrader(\n",
    "    llm=ChatOpenAI(model=\"gpt-4o-mini\", temperature=0), target=\"retrieval-question\"\n",
    ").create()\n",
    "\n",
    "ra_grader = OpenAIRelevanceGrader(\n",
    "    llm=ChatOpenAI(model=\"gpt-4o-mini\", temperature=0), target=\"retrieval-answer\"\n",
    ").create()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 관련성(Relevance) 평가를 종합하는 Summary Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from langsmith.schemas import Example, Run\n",
    "from langsmith.evaluation import evaluate\n",
    "\n",
    "\n",
    "def relevance_score_summary_evaluator(runs: List[Run], examples: List[Example]) -> dict:\n",
    "    rq_scores = 0  # 질문 관련성 점수\n",
    "    ra_scores = 0  # 답변 관련성 점수\n",
    "\n",
    "    for run, example in zip(runs, examples):\n",
    "        question = example.inputs[\"question\"]\n",
    "        context = run.outputs[\"context\"]\n",
    "        prediction = run.outputs[\"answer\"]\n",
    "\n",
    "        # 질문 관련성 평가\n",
    "        rq_score = rq_grader.invoke(\n",
    "            {\n",
    "                \"input\": question,\n",
    "                \"context\": context,\n",
    "            }\n",
    "        )\n",
    "        # 답변 관련성 평가\n",
    "        ra_score = ra_grader.invoke(\n",
    "            {\n",
    "                \"input\": prediction,\n",
    "                \"context\": context,\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # 관련성 점수 누적\n",
    "        if rq_score.score == \"yes\":\n",
    "            rq_scores += 1\n",
    "        if ra_score.score == \"yes\":\n",
    "            ra_scores += 1\n",
    "\n",
    "    # 최종 관련성 점수 계산 (질문 관련성과 답변 관련성의 평균)\n",
    "    final_score = ((rq_scores / len(runs)) + (ra_scores / len(runs))) / 2\n",
    "\n",
    "    return {\"key\": \"relevance_score\", \"score\": final_score}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'SUMMARY_EVAL-8fa0b230' at:\n",
      "https://smith.langchain.com/o/2d0ce887-3f7f-59af-8d5e-12c1371ef5d5/datasets/334d943e-3086-4d03-91ae-4b5145224ffc/compare?selectedSessions=195c32b6-c7a5-4c23-9f87-e90d83088f57\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36bf80752c0b496391eebf34ce22672d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error running target function: 'question'\n",
      "Error running target function: 'question'\n",
      "Error running target function: 'question'\n",
      "Error running target function: 'question'\n",
      "Error running summary evaluator <function relevance_score_summary_evaluator at 0x0000027D87F68400>: 'question'\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'ollama_chain' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 17\u001b[0m\n\u001b[0;32m      4\u001b[0m experiment_result1 \u001b[38;5;241m=\u001b[39m evaluate(\n\u001b[0;32m      5\u001b[0m     gpt_chain,\n\u001b[0;32m      6\u001b[0m     data\u001b[38;5;241m=\u001b[39mdataset_name,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     12\u001b[0m     },\n\u001b[0;32m     13\u001b[0m )\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# 평가 실행\u001b[39;00m\n\u001b[0;32m     16\u001b[0m experiment_result2 \u001b[38;5;241m=\u001b[39m evaluate(\n\u001b[1;32m---> 17\u001b[0m     \u001b[43mollama_chain\u001b[49m,\n\u001b[0;32m     18\u001b[0m     data\u001b[38;5;241m=\u001b[39mdataset_name,\n\u001b[0;32m     19\u001b[0m     summary_evaluators\u001b[38;5;241m=\u001b[39m[relevance_score_summary_evaluator],\n\u001b[0;32m     20\u001b[0m     experiment_prefix\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSUMMARY_EVAL\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;66;03m# 실험 메타데이터 지정\u001b[39;00m\n\u001b[0;32m     22\u001b[0m     metadata\u001b[38;5;241m=\u001b[39m{\n\u001b[0;32m     23\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvariant\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOllama(EEVE-Korean-10.8B:latest) 사용: summary_evaluator 를 활용한 relevance 평가\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     24\u001b[0m     },\n\u001b[0;32m     25\u001b[0m )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'ollama_chain' is not defined"
     ]
    }
   ],
   "source": [
    "# 평가 실행\n",
    "dataset_name = \"RAG_EVAL_DATASET_teddynote\"\n",
    "\n",
    "experiment_result1 = evaluate(\n",
    "    gpt_chain,\n",
    "    data=dataset_name,\n",
    "    summary_evaluators=[relevance_score_summary_evaluator],\n",
    "    experiment_prefix=\"SUMMARY_EVAL\",\n",
    "    # 실험 메타데이터 지정\n",
    "    metadata={\n",
    "        \"variant\": \"GPT-4o-mini 사용: summary_evaluator 를 활용한 relevance 평가\",\n",
    "    },\n",
    ")\n",
    "\n",
    "# 평가 실행\n",
    "experiment_result2 = evaluate(\n",
    "    ollama_chain,\n",
    "    data=dataset_name,\n",
    "    summary_evaluators=[relevance_score_summary_evaluator],\n",
    "    experiment_prefix=\"SUMMARY_EVAL\",\n",
    "    # 실험 메타데이터 지정\n",
    "    metadata={\n",
    "        \"variant\": \"Ollama(EEVE-Korean-10.8B:latest) 사용: summary_evaluator 를 활용한 relevance 평가\",\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-kr-gLkynrUQ-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
