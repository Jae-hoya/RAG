{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader, PDFPlumberLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain.retrievers import BM25Retriever, EnsembleRetriever\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from kiwipiepy import Kiwi\n",
    "\n",
    "# reorder\n",
    "from langchain_community.document_transformers import LongContextReorder\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "# multi-query\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "\n",
    "# reranker\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import CrossEncoderReranker\n",
    "from langchain_community.cross_encoders import HuggingFaceCrossEncoder\n",
    "\n",
    "kiwi = Kiwi()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from langchain.retrievers.document_compressors import Cross"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kiwi 함수\n",
    "def kiwi_tokenize(docs):\n",
    "    return [token.form for token in kiwi.tokenize(docs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaders = [\n",
    "    PDFPlumberLoader(\"data/RAG/Adaptive_RAG.pdf\"),\n",
    "    PDFPlumberLoader(\"data/RAG/Naive_RAG.pdf\"),\n",
    "    PDFPlumberLoader('data/RAG/RAPTOR_RAG.pdf'),\n",
    "    PDFPlumberLoader('data/RAG/Self_RAG.pdf')\n",
    "]\n",
    "\n",
    "docs = []\n",
    "\n",
    "for loader in loaders:\n",
    "    docs.extend(loader.load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "87"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensemble Retriever (kiwi bm25 + chroma similarity)\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=50)\n",
    "\n",
    "embedding = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "documents = text_splitter.split_documents(docs)\n",
    "\n",
    "chroma_vector_store = Chroma.from_documents(documents, embedding)\n",
    "chroma_retriever = chroma_vector_store.as_retriever()\n",
    "\n",
    "kiwi_vector_store = BM25Retriever.from_documents(documents, preprocess_func=kiwi_tokenize)\n",
    "\n",
    "ensemble_retriever = EnsembleRetriever(\n",
    "    retrievers=[chroma_retriever, kiwi_vector_store],\n",
    "    weights=[0.5, 0.5],\n",
    "    search_kwargs={\"k\": 10}\n",
    "    #search_type=\"mmr\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MultiQUery Retriever\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "multiquery_retriever = MultiQueryRetriever.from_llm(\n",
    "    retriever = ensemble_retriever,\n",
    "    llm=llm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\skyop\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\langchain-kr-gLkynrUQ-py3.11\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "# Reranker\n",
    "\n",
    "reranker_model = HuggingFaceCrossEncoder(model_name=\"BAAI/bge-reranker-v2-m3\")\n",
    "\n",
    "# 상위 3개 모델 선택\n",
    "compressor = CrossEncoderReranker(model=reranker_model, top_n=3)\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor = compressor, base_retriever = ensemble_retriever #multiquery_retriever\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reorder\n",
    "def reorder_documents(docs):\n",
    "    # 재정렬\n",
    "    reordering = LongContextReorder()\n",
    "    reordered_docs = reordering.transform_documents(docs)\n",
    "    return reordered_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt\n",
    "template = \"\"\"\n",
    "    당신은 주어진 문서에 대해서 QA를 해주는 assistant bot입니다.\n",
    "    주어진 문서를 이용해서 답변해 주세요\n",
    "    만약 주어진 질문에 대해서 모른다면, 모른다고 답변해 주세요\n",
    "    문서의 출처와 페이지 넘버를 작성해주세요\n",
    "    한국어로 답변해주세요\n",
    "\n",
    "    #Example Format:\n",
    "    (brief summary of the answer)\n",
    "    (table)\n",
    "    (detailed answer to the question)\n",
    "\n",
    "    **출처**\n",
    "    - (page source and page number)\n",
    "\n",
    "    # Question:\n",
    "    {question}\n",
    "\n",
    "    # Context:\n",
    "    {context}\n",
    "\n",
    "    # Answer\n",
    "\"\"\"\n",
    "prompt = PromptTemplate.from_template(template, input_variable=[\"question\", \"context\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chain\n",
    "chain = ({\"context\":compression_retriever| RunnableLambda(reorder_documents), \"question\": RunnablePassthrough()}\n",
    "         |prompt\n",
    "         |llm\n",
    "         |StrOutputParser()\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adaptive-RAG는 반복적으로 검색기(retriever)와 대형 언어 모델(LLM)에 접근하여 Chain-of-Thought 추론을 수행하는 방식으로, 문제의 해결책을 도출하거나 최대 단계 수에 도달할 때까지 진행됩니다. 이 모델은 복잡한 쿼리를 완벽하게 분류할 수 있는 오라클 분류기를 갖춘 이상적인 시나리오에서 작동합니다. Adaptive-RAG는 적응형 전략의 필요성을 강조하며, 최적의 성능을 달성하기 위해 개선된 분류기를 개발하는 방향을 제안합니다.\n",
      "\n",
      "| Adaptive-RAG의 특징 | 설명 |\n",
      "|---------------------|------|\n",
      "| 접근 방식          | 검색기와 LLM의 반복적 접근 |\n",
      "| 추론 방식          | Chain-of-Thought 추론 |\n",
      "| 성능               | 효율적이고 효과적임 |\n",
      "| 구현 세부사항      | A100 GPU 사용, PyTorch 및 Transformers 라이브러리로 구현 |\n",
      "\n",
      "Adaptive-RAG는 복잡한 쿼리 처리에서 뛰어난 성능을 보이며, 다양한 검색 증강 생성 접근 방식과 비교했을 때도 우수한 성과를 나타냅니다.\n",
      "\n",
      "**출처**\n",
      "- data/RAG/Adaptive_RAG.pdf, 페이지 6, 13\n"
     ]
    }
   ],
   "source": [
    "print(chain.invoke(\"AdaptiveRAG에 대해서 설명해 주세요\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELF-RAG는 대형 언어 모델(LLM)의 품질과 사실성을 향상시키기 위해 설계된 새로운 프레임워크입니다. 이 방법은 필요에 따라 정보를 검색하고, 생성하며, 자기 반성을 통해 텍스트 구문을 비판하는 과정을 포함합니다. SELF-RAG는 LLM이 자신의 생성 과정을 반영하도록 학습하여, 더 높은 품질의 응답을 생성할 수 있도록 돕습니다.\n",
      "\n",
      "| SELF-RAG의 주요 특징 |\n",
      "|---------------------|\n",
      "| 1. 필요에 따른 정보 검색 |\n",
      "| 2. 자기 반성을 통한 비판적 생성 |\n",
      "| 3. 사실성 향상 |\n",
      "| 4. LLM의 유연성 유지 |\n",
      "\n",
      "SELF-RAG는 LLM이 주어진 작업 입력에 대해 출력과 중간의 특별한 출력을 생성함으로써 자신의 생성 과정을 반영하도록 훈련됩니다. 이 과정은 LLM이 더 높은 품질의 응답을 생성하고, 사실적 정확성을 높이며, 불필요한 정보 검색을 줄이는 데 기여합니다. 연구 결과, SELF-RAG는 기존의 RAG 방법보다 더 나은 성능을 보였습니다.\n",
      "\n",
      "**출처**\n",
      "- Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection, 페이지 9.\n"
     ]
    }
   ],
   "source": [
    "print(chain.invoke(\"Self-RAG에 대해서 설명해 주세요\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-kr-gLkynrUQ-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
