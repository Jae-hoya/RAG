{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# memory\n",
    "from langchain_community.document_loaders import PDFPlumberLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Pinecone\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain.retrievers import BM25Retriever, EnsembleRetriever\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.messages import ChatMessage\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableWithMessageHistory\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "341"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data load and split\n",
    "loaders = [\n",
    "    PDFPlumberLoader(\"./data/RAG/Adaptive_RAG.pdf\"),\n",
    "    PDFPlumberLoader(\"./data/RAG/Naive_RAG.pdf\"),\n",
    "    PDFPlumberLoader(\"./data/RAG/RAPTOR_RAG.pdf\"),\n",
    "    PDFPlumberLoader(\"./data/RAG/Self_RAG.pdf\")\n",
    "]\n",
    "\n",
    "docs = []\n",
    "\n",
    "for loader in loaders:\n",
    "    docs.extend(loader.load())\n",
    "\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=50)\n",
    "documents = text_splitter.split_documents(docs)\n",
    "\n",
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': None,\n",
       " 'metadata': {'source': './data/RAG/Adaptive_RAG.pdf',\n",
       "  'file_path': './data/RAG/Adaptive_RAG.pdf',\n",
       "  'page': 0,\n",
       "  'total_pages': 15,\n",
       "  'Author': '',\n",
       "  'CreationDate': 'D:20240329003444Z',\n",
       "  'Creator': 'LaTeX with hyperref',\n",
       "  'Keywords': '',\n",
       "  'ModDate': 'D:20240329003444Z',\n",
       "  'PTEX.Fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5',\n",
       "  'Producer': 'pdfTeX-1.40.25',\n",
       "  'Subject': '',\n",
       "  'Title': '',\n",
       "  'Trapped': 'False'},\n",
       " 'page_content': 'Adaptive-RAG: Learning to Adapt Retrieval-Augmented\\nLarge Language Models through Question Complexity\\nSoyeongJeong1 JinheonBaek2 SukminCho1 SungJuHwang1,2 JongC.Park1*\\nSchoolofComputing1 GraduateSchoolofAI2\\nKoreaAdvancedInstituteofScienceandTechnology1,2\\n{starsuzi,jinheon.baek,nelllpic,sjhwang82,jongpark}@kaist.ac.kr\\nAbstract\\n51\\nRetrieval-AugmentedLargeLanguageModels 50\\n(LLMs),whichincorporatethenon-parametric 49\\nknowledgefromexternalknowledgebasesinto 48\\nLLMs,haveemergedasapromisingapproach 47\\ntoenhancingresponseaccuracyinseveraltasks, 0.5 1.0 1.5 2.0 2.5 3.0 3.5\\nTime per Query\\nsuchasQuestion-Answering(QA).However,\\neventhoughtherearevariousapproachesdeal-\\ningwithqueriesofdifferentcomplexities,they\\neitherhandlesimplequerieswithunnecessary\\ncomputational overhead or fail to adequately\\naddress complex multi-step queries; yet, not\\nalluserrequestsfallintoonlyoneofthesim-\\nple or complex categories. In this work, we\\nproposeanoveladaptiveQAframeworkthat',\n",
       " 'type': 'Document'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vars(documents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': './data/RAG/Adaptive_RAG.pdf',\n",
       " 'file_path': './data/RAG/Adaptive_RAG.pdf',\n",
       " 'page': 0,\n",
       " 'total_pages': 15,\n",
       " 'Author': '',\n",
       " 'CreationDate': 'D:20240329003444Z',\n",
       " 'Creator': 'LaTeX with hyperref',\n",
       " 'Keywords': '',\n",
       " 'ModDate': 'D:20240329003444Z',\n",
       " 'PTEX.Fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5',\n",
       " 'Producer': 'pdfTeX-1.40.25',\n",
       " 'Subject': '',\n",
       " 'Title': '',\n",
       " 'Trapped': 'False'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0].metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "파인콘\n",
    "preprocess - sparse encoder생성(kiwi tokenizer, stopwords(불용어)) - 인덱스 생성 또는 불러오기 - DB에 데이터 추가 - init pinecone - kiwihybridretriever 생성\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b10b9ef9e7ad4aa7af0d396e296b1054",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/341 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# preprocessing documents for pinecone\n",
    "from langchain_teddynote.community.pinecone import preprocess_documents\n",
    "\n",
    "contents, metadatas = preprocess_documents(\n",
    "    split_docs = documents,\n",
    "    metadata_keys = [\"source\", \"page\"],\n",
    "    min_length=5,\n",
    "    use_basename=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "341\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "print(len(contents))\n",
    "print(len(metadatas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['source', 'page'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadatas.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Adaptive-RAG: Learning to Adapt Retrieval-Augmented\\nLarge Language Models through Question Complexity\\nSoyeongJeong1 JinheonBaek2 SukminCho1 SungJuHwang1,2 JongC.Park1*\\nSchoolofComputing1 GraduateSchoolofAI2\\nKoreaAdvancedInstituteofScienceandTechnology1,2\\n{starsuzi,jinheon.baek,nelllpic,sjhwang82,jongpark}@kaist.ac.kr\\nAbstract\\n51\\nRetrieval-AugmentedLargeLanguageModels 50\\n(LLMs),whichincorporatethenon-parametric 49\\nknowledgefromexternalknowledgebasesinto 48\\nLLMs,haveemergedasapromisingapproach 47\\ntoenhancingresponseaccuracyinseveraltasks, 0.5 1.0 1.5 2.0 2.5 3.0 3.5\\nTime per Query\\nsuchasQuestion-Answering(QA).However,\\neventhoughtherearevariousapproachesdeal-\\ningwithqueriesofdifferentcomplexities,they\\neitherhandlesimplequerieswithunnecessary\\ncomputational overhead or fail to adequately\\naddress complex multi-step queries; yet, not\\nalluserrequestsfallintoonlyoneofthesim-\\nple or complex categories. In this work, we\\nproposeanoveladaptiveQAframeworkthat',\n",
       " 'proposeanoveladaptiveQAframeworkthat\\ncandynamicallyselectthemostsuitablestrat-\\negyfor(retrieval-augmented)LLMsfromthe\\nsimplesttothemostsophisticatedonesbased\\non the query complexity. Also, this selec-\\ntion process is operationalized with a classi-\\nfier,whichisasmallerLMtrainedtopredict\\nthecomplexitylevelofincomingquerieswith\\nautomaticallycollectedlabels,obtainedfrom\\nactual predicted outcomes of models and in-\\nherent inductive biases in datasets. This ap-\\nproachoffersabalancedstrategy, seamlessly\\nadaptingbetweentheiterativeandsingle-step\\nretrieval-augmentedLLMs,aswellastheno-\\nretrieval methods, in response to a range of\\nquery complexities. We validate our model\\non a set of open-domain QA datasets, cov-\\nering multiple query complexities, and show\\nthat ours enhances the overall efficiency and\\naccuracy of QA systems, compared to rele-\\nvantbaselinesincludingtheadaptiveretrieval\\napproaches. Code is available at: https://\\ngithub.com/starsuzi/Adaptive-RAG.\\n1 Introduction',\n",
       " 'github.com/starsuzi/Adaptive-RAG.\\n1 Introduction\\nRecent Large Language Models (LLMs) (Brown\\netal.,2020;OpenAI,2023;Touvronetal.,2023;\\nAnil et al., 2023) have shown overwhelming per-\\nformancesacrossdiversetasks,includingquestion-\\n* Correspondingauthor\\n)1F(\\necnamrofreP\\nPerformance vs Time with GPT-3.5\\nAdaptive-RAG (Ours) Multi-step Approach\\nNo Retrieval Adaptive Retrieval\\nSingle-step Approach\\nFigure1:QAperformance(F1)andefficiency(Time/Query)\\nfordifferentretrieval-augmentedgenerationapproaches.We\\nusetheGPT-3.5-Turbo-InstructasthebaseLLM.\\nanswering(QA)(Yangetal.,2018;Kwiatkowski\\net al., 2019). However, they still generate factu-\\nallyincorrectanswerssincetheirknowledgesolely\\nrelies on their parametric memory (Kasai et al.,\\n2022;Mallenetal.,2023). Meanwhile,memoriz-\\ningallthe(ever-changing)worldknowledgemay\\nnotbepossible. Toaddressthisproblem,retrieval-\\naugmentedLLMs(Borgeaudetal.,2022;Izacard\\net al., 2023; Shi et al., 2023), which incorporate\\nnon-parametric knowledge into LLMs with addi-',\n",
       " 'non-parametric knowledge into LLMs with addi-\\ntionalretrievalmodules,havegainedmuchincreas-\\ning attention. Specifically, these models access\\na knowledge base, which serves as an extensive\\nrepository of information across various subjects\\nanddisciplines,toretrieveinformationrelevantto\\nthegiveninput,andthenincorporatetheretrieved\\ninformationintoLLMs,whichenablesthemtostay\\naccurateandcurrentwiththeworldknowledge.\\nA particularly salient application of retrieval-\\naugmentedLLMsistohandlingQAtasks,whose\\ngoal is to provide correct answers in response to\\nuserqueries,especiallythoseofhighcomplexity.\\nEarlyworkonretrieval-augmentedLLMsfocuses\\nprimarilyonsingle-hopqueries(Lazaridouetal.,\\n2022; Ram et al., 2023), whose answers are typ-\\nically found within a single document; therefore,\\nthis approach involves retrieving a relevant doc-\\nument based on the query and subsequently inte-\\ngratingthisinformationintoQAmodelstoformu-\\nlate a response. However, unlike this single-hop',\n",
       " 'late a response. However, unlike this single-hop\\nQA,somequeriesrequireconnectingandaggregat-\\ning multiple documents, which are, furthermore,\\n4202\\nraM\\n82\\n]LC.sc[\\n2v30441.3042:viXra']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contents[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(341, 341, 341)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(contents), len(metadatas[\"source\"]), len(metadatas['page'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5652704722734b219db8d1c605eb1991",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/341 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[fit_sparse_encoder]\n",
      "Saved Sparse Encoder to: ./sparse_encoder.pkl\n"
     ]
    }
   ],
   "source": [
    "# # pinecone : Sparse Encoder생성 - KiwiTokenizer + 불용어(stopwords)\n",
    "# from langchain_teddynote.korean import stopwords\n",
    "# from langchain_teddynote.community.pinecone import create_sparse_encoder, fit_sparse_encoder\n",
    "\n",
    "# stopword = stopwords()\n",
    "# stopword\n",
    "\n",
    "# sparse_encoder = create_sparse_encoder(stopword, mode='kiwi')\n",
    "\n",
    "# save_path = fit_sparse_encoder(\n",
    "#     sparse_encoder=sparse_encoder, contents=contents, save_path=\"./sparse_encoder.pkl\"\n",
    "# )\n",
    "# # 저장한 sparse encoder불러오기\n",
    "# # from langchain_teddynote.community.pinecone import load_sparse_encoder\n",
    "\n",
    "# # sparse_encoder = load_sparse_encoder(\"./sparse_encoder.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[create_index]\n",
      "{'dimension': 1536,\n",
      " 'index_fullness': 0.0,\n",
      " 'namespaces': {'jaehonote-namespace': {'vector_count': 1458},\n",
      "                'jaehonote-namespace-0': {'vector_count': 619},\n",
      "                'jaehonote-namespace-2': {'vector_count': 341}},\n",
      " 'total_vector_count': 2418}\n"
     ]
    }
   ],
   "source": [
    "# # pinecone : create index\n",
    "# import os\n",
    "# from langchain_teddynote.community.pinecone import create_index\n",
    "# pc_index = create_index(\n",
    "#     api_key=os.environ[\"PINECONE_API_KEY\"],\n",
    "#     index_name=\"jaeho-note-index\", # 인덱스 이름을 지정합니다.\n",
    "#     dimension=1536, # Embedding 차원과 맞춥니다. (OpenAIEmbeddings: 1536, UpstageEmbeddings: 4096)\n",
    "#     metric=\"dotproduct\" # 유사도 측정 방법을 지정합니다. (dotproduct, euclidean, cosine) Hybrid search를 위해선 dotproduct를 사용\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pinecone: DB index에추가(Upsert)\n",
    "# import os\n",
    "\n",
    "# from langchain_teddynote.community.pinecone import upsert_documents_parallel\n",
    "# openai_embedding = OpenAIEmbeddings(model='text-embedding-3-small')\n",
    "\n",
    "# upsert_documents_parallel(\n",
    "#     index=pc_index,\n",
    "#     namespace=\"jaehonote-namespace-2\",\n",
    "#     contents=contents,\n",
    "#     metadatas=metadatas,\n",
    "#     sparse_encoder=sparse_encoder,\n",
    "#     embedder=openai_embedding,\n",
    "#     batch_size=32,\n",
    "#     max_workers=30\n",
    "# )\n",
    "# 2077+341"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[init_pinecone_index]\n",
      "{'dimension': 1536,\n",
      " 'index_fullness': 0.0,\n",
      " 'namespaces': {'jaehonote-namespace': {'vector_count': 1458},\n",
      "                'jaehonote-namespace-0': {'vector_count': 619},\n",
      "                'jaehonote-namespace-2': {'vector_count': 341}},\n",
      " 'total_vector_count': 2418}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_teddynote.korean import stopwords\n",
    "stopword = stopwords()\n",
    "# pinecone index\n",
    "from langchain_teddynote.community.pinecone import init_pinecone_index\n",
    "openai_embedding = OpenAIEmbeddings(model='text-embedding-3-small')\n",
    "pinecone_params = init_pinecone_index(\n",
    "    index_name=\"jaeho-note-index\",\n",
    "    namespace=\"jaehonote-namespace-2\",\n",
    "    api_key=os.environ[\"PINECONE_API_KEY\"],\n",
    "    sparse_encoder_path=\"./sparse_encoder.pkl\",\n",
    "    stopwords=stopword,\n",
    "    tokenizer=\"kiwi\",\n",
    "    embeddings=openai_embedding,\n",
    "    top_k=5,\n",
    "    alpha=0.5\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pinecone kiwi hybridretriever\n",
    "from langchain_teddynote.community.pinecone import PineconeKiwiHybridRetriever\n",
    "\n",
    "pinecone_retriever = PineconeKiwiHybridRetriever(**pinecone_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'page': 9.0, 'source': 'Self_RAG.pdf', 'score': 0.2701767}, page_content='outputissupportedbythecitedevidence). Humanannotatorsfind SELF-RAG answersareoften\\nplausibleandsupportedbyrelevantpassageswithhigherS&Pscoresonshort-formPopQA,whichis\\nconsistentwithMenicketal.(2022). Humanannotatorsalsofind ISREL and ISSUP reflectiontoken\\npredictionsaremostlyalignedwiththeirassessments. AppendixTable6showsseveralannotated\\nexamplesandexplanationsonassessments.\\n6 CONCLUSION\\nThisworkintroducesSELF-RAG,anewframeworktoenhancethequalityandfactualityofLLMs\\nthroughretrievalondemandandself-reflection. SELF-RAGtrainsanLMtolearntoretrieve,generate,\\nand critique text passages and its own generation by predicting the next tokens from its original\\nvocabularyaswellasnewlyaddedspecialtokens,calledreflectiontokens. SELF-RAGfurtherenables\\nthetailoringofLMbehaviorsattesttimebyleveragingreflectiontokens. Ourholisticevaluationson\\nsixtasksusingmultiplemetricsdemonstratethatSELF-RAGsignificantlyoutperformsLLMswith\\nmoreparametersorwithconventionalretrieval-augmentedgenerationapproaches.\\n10'),\n",
       " Document(metadata={'page': 0.0, 'source': 'Self_RAG.pdf', 'score': 0.2672921}, page_content='Preprint.\\nSELF-RAG: LEARNING TO RETRIEVE, GENERATE, AND\\nCRITIQUE THROUGH SELF-REFLECTION\\nAkariAsai†,ZeqiuWu†,YizhongWang†§,AvirupSil‡,HannanehHajishirzi†§\\n†UniversityofWashington §AllenInstituteforAI ‡IBMResearchAI\\n{akari,zeqiuwu,yizhongw,hannaneh}@cs.washington.edu,avi@us.ibm.com\\nABSTRACT\\nDespitetheirremarkablecapabilities,largelanguagemodels(LLMs)oftenproduce\\nresponsescontainingfactualinaccuraciesduetotheirsolerelianceontheparamet-\\nricknowledgetheyencapsulate. Retrieval-AugmentedGeneration(RAG),anad\\nhocapproachthataugmentsLMswithretrievalofrelevantknowledge,decreases\\nsuchissues. However,indiscriminatelyretrievingandincorporatingafixednumber\\nofretrievedpassages,regardlessofwhetherretrievalisnecessary,orpassagesare\\nrelevant,diminishesLMversatilityorcanleadtounhelpfulresponsegeneration.\\nWeintroduceanewframeworkcalledSelf-ReflectiveRetrieval-AugmentedGen-\\neration(SELF-RAG)thatenhancesanLM’squalityandfactualitythroughretrieval'),\n",
       " Document(metadata={'page': 16.0, 'source': 'Self_RAG.pdf', 'score': 0.25702706}, page_content='Preprint.\\nA SELF-RAG DETAILS\\nA.1 REFLECTIONTOKENS.\\nDefinitionsofreflectiontokens. Below,weprovideadetaileddefinitionofreflectiontypeand\\noutputtokens. Thefirstthreeaspectswillbeprovidedateachsegmentlevel,whilethefinalaspectis\\nonlygivenateachoutputlevel.\\n• Retrieval-on-demand (Retrieve): Given an input and previous-step generation (if applicable),\\nan LM determines whether the continuation requires factual grounding. No indicates retrieval\\nis unnecessary as the sequence does not require factual grounding or may not be enhanced by\\nknowledge retrieval, Yes indicates retrieval is necessary. We additionally have continue\\nto use evidence,whichindicatesthatamodelcancontinuetousetheevidenceretrieved\\npreviously. For instance, a passage may contain rich factual information, and thus SELF-RAG\\ngeneratesmultiplesegmentsbasedonthepassage.\\n• Relevant( ISREL ): Retrievedknowledgemaynotbealwaysrelevanttotheinput. Thisaspect\\nindicateswhethertheevidenceprovidesusefulinformation(Relevant)ornot(Irrelevant).'),\n",
       " Document(metadata={'page': 1.0, 'source': 'Self_RAG.pdf', 'score': 0.2493408}, page_content='assessitsownpredictionsaftereachgeneratedsegmentasanintegralpartofthegenerationoutput.\\nSELF-RAG furtherenablesacustomizabledecodingalgorithmtosatisfyhardorsoftconstraints,\\nwhicharedefinedbyreflectiontokenpredictions. Inparticular,ourinference-timealgorithmenables\\nusto(1)flexiblyadjustretrievalfrequencyfordifferentdownstreamapplicationsand(2)customize\\nmodels’behaviorstouserpreferencesbyleveragingreflectiontokensthroughsegment-levelbeam\\nsearchusingtheweightedlinearsumofthereflectiontokenprobabilitiesassegmentscore.\\nEmpiricalresultsonsixtasks,includingreasoningandlong-formgeneration,demonstratethatSELF-\\nRAGsignificantlyoutperformspre-trainedandinstruction-tunedLLMsthathavemoreparametersand\\nwidelyadoptedRAGapproacheswithhighercitationaccuracy. Inparticular,SELF-RAGoutperforms\\nretrieval-augmentedChatGPTonfourtasks,Llama2-chat(Touvronetal.,2023)andAlpaca(Dubois\\netal.,2023)onalltasks. Ouranalysisdemonstratestheeffectivenessoftrainingandinferencewith'),\n",
       " Document(metadata={'page': 15.0, 'source': 'Self_RAG.pdf', 'score': 0.24746148}, page_content='Preprint.\\nAPPENDIX\\nA SELF-RAGDetails 17\\nA.1 ReflectionTokens. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17\\nA.2 SELF-RAGTraining. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17\\nA.3 SELF-RAGInference . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19\\nB ExperimentalDetails 19\\nB.1 MoreDetailsofTraining . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19\\nB.2 MoreDetailsofEvaluations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20\\nC Results 20\\nC.1 Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20\\nC.2 HumanEvaluationExamples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21\\nC.3 QualitativeExamples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21\\nD FullListofInstructionsandDemonstrationsforGPT-4 21\\n16')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pinecone_retriever.invoke(\"self-rag에 대해서 알려줘\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputissupportedbythecitedevidence). Humanannotatorsfind SELF-RAG answersareoften\n",
      "plausibleandsupportedbyrelevantpassageswithhigherS&Pscoresonshort-formPopQA,whichis\n",
      "consistentwithMenicketal.(2022). Humanannotatorsalsofind ISREL and ISSUP reflectiontoken\n",
      "predictionsaremostlyalignedwiththeirassessments. AppendixTable6showsseveralannotated\n",
      "examplesandexplanationsonassessments.\n",
      "6 CONCLUSION\n",
      "ThisworkintroducesSELF-RAG,anewframeworktoenhancethequalityandfactualityofLLMs\n",
      "throughretrievalondemandandself-reflection. SELF-RAGtrainsanLMtolearntoretrieve,generate,\n",
      "and critique text passages and its own generation by predicting the next tokens from its original\n",
      "vocabularyaswellasnewlyaddedspecialtokens,calledreflectiontokens. SELF-RAGfurtherenables\n",
      "thetailoringofLMbehaviorsattesttimebyleveragingreflectiontokens. Ourholisticevaluationson\n",
      "sixtasksusingmultiplemetricsdemonstratethatSELF-RAGsignificantlyoutperformsLLMswith\n",
      "moreparametersorwithconventionalretrieval-augmentedgenerationapproaches.\n",
      "10\n",
      "{'page': 9.0, 'source': 'Self_RAG.pdf', 'score': 0.21755427}\n",
      "\n",
      "\n",
      "\n",
      "Preprint.\n",
      "SELF-RAG: LEARNING TO RETRIEVE, GENERATE, AND\n",
      "CRITIQUE THROUGH SELF-REFLECTION\n",
      "AkariAsai†,ZeqiuWu†,YizhongWang†§,AvirupSil‡,HannanehHajishirzi†§\n",
      "†UniversityofWashington §AllenInstituteforAI ‡IBMResearchAI\n",
      "{akari,zeqiuwu,yizhongw,hannaneh}@cs.washington.edu,avi@us.ibm.com\n",
      "ABSTRACT\n",
      "Despitetheirremarkablecapabilities,largelanguagemodels(LLMs)oftenproduce\n",
      "responsescontainingfactualinaccuraciesduetotheirsolerelianceontheparamet-\n",
      "ricknowledgetheyencapsulate. Retrieval-AugmentedGeneration(RAG),anad\n",
      "hocapproachthataugmentsLMswithretrievalofrelevantknowledge,decreases\n",
      "suchissues. However,indiscriminatelyretrievingandincorporatingafixednumber\n",
      "ofretrievedpassages,regardlessofwhetherretrievalisnecessary,orpassagesare\n",
      "relevant,diminishesLMversatilityorcanleadtounhelpfulresponsegeneration.\n",
      "WeintroduceanewframeworkcalledSelf-ReflectiveRetrieval-AugmentedGen-\n",
      "eration(SELF-RAG)thatenhancesanLM’squalityandfactualitythroughretrieval\n",
      "{'page': 0.0, 'source': 'Self_RAG.pdf', 'score': 0.21059124}\n",
      "\n",
      "\n",
      "\n",
      "Preprint.\n",
      "A SELF-RAG DETAILS\n",
      "A.1 REFLECTIONTOKENS.\n",
      "Definitionsofreflectiontokens. Below,weprovideadetaileddefinitionofreflectiontypeand\n",
      "outputtokens. Thefirstthreeaspectswillbeprovidedateachsegmentlevel,whilethefinalaspectis\n",
      "onlygivenateachoutputlevel.\n",
      "• Retrieval-on-demand (Retrieve): Given an input and previous-step generation (if applicable),\n",
      "an LM determines whether the continuation requires factual grounding. No indicates retrieval\n",
      "is unnecessary as the sequence does not require factual grounding or may not be enhanced by\n",
      "knowledge retrieval, Yes indicates retrieval is necessary. We additionally have continue\n",
      "to use evidence,whichindicatesthatamodelcancontinuetousetheevidenceretrieved\n",
      "previously. For instance, a passage may contain rich factual information, and thus SELF-RAG\n",
      "generatesmultiplesegmentsbasedonthepassage.\n",
      "• Relevant( ISREL ): Retrievedknowledgemaynotbealwaysrelevanttotheinput. Thisaspect\n",
      "indicateswhethertheevidenceprovidesusefulinformation(Relevant)ornot(Irrelevant).\n",
      "{'page': 16.0, 'source': 'Self_RAG.pdf', 'score': 0.20446342}\n",
      "\n",
      "\n",
      "\n",
      "Preprint.\n",
      "APPENDIX\n",
      "A SELF-RAGDetails 17\n",
      "A.1 ReflectionTokens. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17\n",
      "A.2 SELF-RAGTraining. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17\n",
      "A.3 SELF-RAGInference . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19\n",
      "B ExperimentalDetails 19\n",
      "B.1 MoreDetailsofTraining . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19\n",
      "B.2 MoreDetailsofEvaluations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20\n",
      "C Results 20\n",
      "C.1 Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20\n",
      "C.2 HumanEvaluationExamples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21\n",
      "C.3 QualitativeExamples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21\n",
      "D FullListofInstructionsandDemonstrationsforGPT-4 21\n",
      "16\n",
      "{'page': 15.0, 'source': 'Self_RAG.pdf', 'score': 0.1968712}\n",
      "\n",
      "\n",
      "\n",
      "EthicsStatement HannanehHajishirzi.2024. Self-RAG:Learningto\n",
      "retrieve,generate,andcritiquethroughself-reflection.\n",
      "The experimental results on Adaptive-RAG vali- InTheTwelfthInternationalConferenceonLearning\n",
      "dateitsapplicabilityinrealisticscenarios,wherea Representations.\n",
      "widerangeofdiverseuserqueriesexist. Nonethe-\n",
      "JinheonBaek,SoyeongJeong,MinkiKang,JongPark,\n",
      "less,giventhepotentialdiversityofreal-worlduser andSungJuHwang.2023. Knowledge-augmented\n",
      "inputs,itiscrucialtoalsoconsiderscenarioswhere languagemodelverification. InProceedingsofthe\n",
      "2023ConferenceonEmpiricalMethodsinNatural\n",
      "these inputs might be offensive or harmful. We\n",
      "LanguageProcessing,EMNLP2023,Singapore,De-\n",
      "shouldbeawarethatsuchinputscouldleadtothe\n",
      "cember6-10,2023,pages1720–1736.Association\n",
      "retrieval of offensive documents and the genera- forComputationalLinguistics.\n",
      "tion of inappropriate responses by the retrieval-\n",
      "SebastianBorgeaud,ArthurMensch,JordanHoffmann,\n",
      "augmented LLMs. To address this challenge, de-\n",
      "{'page': 9.0, 'source': 'Adaptive_RAG.pdf', 'score': 0.1960268}\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = pinecone_retriever.invoke(\"self-rag에 대해서 알려줘.\")\n",
    "for result in response:\n",
    "    print(result.page_content)\n",
    "    print(result.metadata)\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multi-query\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "multiquery_retriever = MultiQueryRetriever.from_llm(\n",
    "    retriever = pinecone_retriever,\n",
    "    llm=llm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputissupportedbythecitedevidence). Humanannotatorsfind SELF-RAG answersareoften\n",
      "plausibleandsupportedbyrelevantpassageswithhigherS&Pscoresonshort-formPopQA,whichis\n",
      "consistentwithMenicketal.(2022). Humanannotatorsalsofind ISREL and ISSUP reflectiontoken\n",
      "predictionsaremostlyalignedwiththeirassessments. AppendixTable6showsseveralannotated\n",
      "examplesandexplanationsonassessments.\n",
      "6 CONCLUSION\n",
      "ThisworkintroducesSELF-RAG,anewframeworktoenhancethequalityandfactualityofLLMs\n",
      "throughretrievalondemandandself-reflection. SELF-RAGtrainsanLMtolearntoretrieve,generate,\n",
      "and critique text passages and its own generation by predicting the next tokens from its original\n",
      "vocabularyaswellasnewlyaddedspecialtokens,calledreflectiontokens. SELF-RAGfurtherenables\n",
      "thetailoringofLMbehaviorsattesttimebyleveragingreflectiontokens. Ourholisticevaluationson\n",
      "sixtasksusingmultiplemetricsdemonstratethatSELF-RAGsignificantlyoutperformsLLMswith\n",
      "moreparametersorwithconventionalretrieval-augmentedgenerationapproaches.\n",
      "10\n",
      "{'page': 9.0, 'source': 'Self_RAG.pdf', 'score': 0.2462969}\n",
      "\n",
      "\n",
      "\n",
      "Preprint.\n",
      "A SELF-RAG DETAILS\n",
      "A.1 REFLECTIONTOKENS.\n",
      "Definitionsofreflectiontokens. Below,weprovideadetaileddefinitionofreflectiontypeand\n",
      "outputtokens. Thefirstthreeaspectswillbeprovidedateachsegmentlevel,whilethefinalaspectis\n",
      "onlygivenateachoutputlevel.\n",
      "• Retrieval-on-demand (Retrieve): Given an input and previous-step generation (if applicable),\n",
      "an LM determines whether the continuation requires factual grounding. No indicates retrieval\n",
      "is unnecessary as the sequence does not require factual grounding or may not be enhanced by\n",
      "knowledge retrieval, Yes indicates retrieval is necessary. We additionally have continue\n",
      "to use evidence,whichindicatesthatamodelcancontinuetousetheevidenceretrieved\n",
      "previously. For instance, a passage may contain rich factual information, and thus SELF-RAG\n",
      "generatesmultiplesegmentsbasedonthepassage.\n",
      "• Relevant( ISREL ): Retrievedknowledgemaynotbealwaysrelevanttotheinput. Thisaspect\n",
      "indicateswhethertheevidenceprovidesusefulinformation(Relevant)ornot(Irrelevant).\n",
      "{'page': 16.0, 'source': 'Self_RAG.pdf', 'score': 0.23371285}\n",
      "\n",
      "\n",
      "\n",
      "Preprint.\n",
      "SELF-RAG: LEARNING TO RETRIEVE, GENERATE, AND\n",
      "CRITIQUE THROUGH SELF-REFLECTION\n",
      "AkariAsai†,ZeqiuWu†,YizhongWang†§,AvirupSil‡,HannanehHajishirzi†§\n",
      "†UniversityofWashington §AllenInstituteforAI ‡IBMResearchAI\n",
      "{akari,zeqiuwu,yizhongw,hannaneh}@cs.washington.edu,avi@us.ibm.com\n",
      "ABSTRACT\n",
      "Despitetheirremarkablecapabilities,largelanguagemodels(LLMs)oftenproduce\n",
      "responsescontainingfactualinaccuraciesduetotheirsolerelianceontheparamet-\n",
      "ricknowledgetheyencapsulate. Retrieval-AugmentedGeneration(RAG),anad\n",
      "hocapproachthataugmentsLMswithretrievalofrelevantknowledge,decreases\n",
      "suchissues. However,indiscriminatelyretrievingandincorporatingafixednumber\n",
      "ofretrievedpassages,regardlessofwhetherretrievalisnecessary,orpassagesare\n",
      "relevant,diminishesLMversatilityorcanleadtounhelpfulresponsegeneration.\n",
      "WeintroduceanewframeworkcalledSelf-ReflectiveRetrieval-AugmentedGen-\n",
      "eration(SELF-RAG)thatenhancesanLM’squalityandfactualitythroughretrieval\n",
      "{'page': 0.0, 'source': 'Self_RAG.pdf', 'score': 0.22811034}\n",
      "\n",
      "\n",
      "\n",
      "Preprint.\n",
      "APPENDIX\n",
      "A SELF-RAGDetails 17\n",
      "A.1 ReflectionTokens. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17\n",
      "A.2 SELF-RAGTraining. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17\n",
      "A.3 SELF-RAGInference . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19\n",
      "B ExperimentalDetails 19\n",
      "B.1 MoreDetailsofTraining . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19\n",
      "B.2 MoreDetailsofEvaluations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20\n",
      "C Results 20\n",
      "C.1 Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20\n",
      "C.2 HumanEvaluationExamples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21\n",
      "C.3 QualitativeExamples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21\n",
      "D FullListofInstructionsandDemonstrationsforGPT-4 21\n",
      "16\n",
      "{'page': 15.0, 'source': 'Self_RAG.pdf', 'score': 0.22085619}\n",
      "\n",
      "\n",
      "\n",
      "and LM on instruction-tuning datasets in two steps. While we also train our model on diverse\n",
      "instruction-following datasets, SELF-RAG enables retrieval on demand and selection of the best\n",
      "possiblemodeloutputviafine-grainedself-reflection,makingitwidelyapplicableandmorerobust\n",
      "andcontrollable. Yoranetal.(2023)useanaturallanguageinferencemodelandXuetal.(2023)use\n",
      "asummarizationmodeltofilteroutorcompressretrievedpassagesbeforeusingthemtopromptthe\n",
      "LMtogeneratetheoutput. SELF-RAGprocessespassagesinparallelandfiltersoutirrelevantones\n",
      "throughself-reflection,withoutrelyingonexternalmodelsatinference. Moreover,ourself-reflection\n",
      "mechanismalsoevaluatesotheraspectsofthemodeloutputqualityincludingfactuality. LATS(Zhou\n",
      "etal.,2023)promptoff-the-shelfLMstosearchforrelevantinformationforquestionansweringtasks\n",
      "andtogeneratewithtreesearch,guidedbyLM-generatedvaluescores. Whiletheirvaluefunction\n",
      "simplyindicatesanoverallscoreofeachgeneration,SELF-RAGtrainstoanarbitraryLMtolearnto\n",
      "{'page': 2.0, 'source': 'Self_RAG.pdf', 'score': 0.21513344}\n",
      "\n",
      "\n",
      "\n",
      "outputissupportedbythecitedevidence). Humanannotatorsfind SELF-RAG answersareoften\n",
      "plausibleandsupportedbyrelevantpassageswithhigherS&Pscoresonshort-formPopQA,whichis\n",
      "consistentwithMenicketal.(2022). Humanannotatorsalsofind ISREL and ISSUP reflectiontoken\n",
      "predictionsaremostlyalignedwiththeirassessments. AppendixTable6showsseveralannotated\n",
      "examplesandexplanationsonassessments.\n",
      "6 CONCLUSION\n",
      "ThisworkintroducesSELF-RAG,anewframeworktoenhancethequalityandfactualityofLLMs\n",
      "throughretrievalondemandandself-reflection. SELF-RAGtrainsanLMtolearntoretrieve,generate,\n",
      "and critique text passages and its own generation by predicting the next tokens from its original\n",
      "vocabularyaswellasnewlyaddedspecialtokens,calledreflectiontokens. SELF-RAGfurtherenables\n",
      "thetailoringofLMbehaviorsattesttimebyleveragingreflectiontokens. Ourholisticevaluationson\n",
      "sixtasksusingmultiplemetricsdemonstratethatSELF-RAGsignificantlyoutperformsLLMswith\n",
      "moreparametersorwithconventionalretrieval-augmentedgenerationapproaches.\n",
      "10\n",
      "{'page': 9.0, 'source': 'Self_RAG.pdf', 'score': 0.2509181}\n",
      "\n",
      "\n",
      "\n",
      "Preprint.\n",
      "SELF-RAG: LEARNING TO RETRIEVE, GENERATE, AND\n",
      "CRITIQUE THROUGH SELF-REFLECTION\n",
      "AkariAsai†,ZeqiuWu†,YizhongWang†§,AvirupSil‡,HannanehHajishirzi†§\n",
      "†UniversityofWashington §AllenInstituteforAI ‡IBMResearchAI\n",
      "{akari,zeqiuwu,yizhongw,hannaneh}@cs.washington.edu,avi@us.ibm.com\n",
      "ABSTRACT\n",
      "Despitetheirremarkablecapabilities,largelanguagemodels(LLMs)oftenproduce\n",
      "responsescontainingfactualinaccuraciesduetotheirsolerelianceontheparamet-\n",
      "ricknowledgetheyencapsulate. Retrieval-AugmentedGeneration(RAG),anad\n",
      "hocapproachthataugmentsLMswithretrievalofrelevantknowledge,decreases\n",
      "suchissues. However,indiscriminatelyretrievingandincorporatingafixednumber\n",
      "ofretrievedpassages,regardlessofwhetherretrievalisnecessary,orpassagesare\n",
      "relevant,diminishesLMversatilityorcanleadtounhelpfulresponsegeneration.\n",
      "WeintroduceanewframeworkcalledSelf-ReflectiveRetrieval-AugmentedGen-\n",
      "eration(SELF-RAG)thatenhancesanLM’squalityandfactualitythroughretrieval\n",
      "{'page': 0.0, 'source': 'Self_RAG.pdf', 'score': 0.2421792}\n",
      "\n",
      "\n",
      "\n",
      "Preprint.\n",
      "A SELF-RAG DETAILS\n",
      "A.1 REFLECTIONTOKENS.\n",
      "Definitionsofreflectiontokens. Below,weprovideadetaileddefinitionofreflectiontypeand\n",
      "outputtokens. Thefirstthreeaspectswillbeprovidedateachsegmentlevel,whilethefinalaspectis\n",
      "onlygivenateachoutputlevel.\n",
      "• Retrieval-on-demand (Retrieve): Given an input and previous-step generation (if applicable),\n",
      "an LM determines whether the continuation requires factual grounding. No indicates retrieval\n",
      "is unnecessary as the sequence does not require factual grounding or may not be enhanced by\n",
      "knowledge retrieval, Yes indicates retrieval is necessary. We additionally have continue\n",
      "to use evidence,whichindicatesthatamodelcancontinuetousetheevidenceretrieved\n",
      "previously. For instance, a passage may contain rich factual information, and thus SELF-RAG\n",
      "generatesmultiplesegmentsbasedonthepassage.\n",
      "• Relevant( ISREL ): Retrievedknowledgemaynotbealwaysrelevanttotheinput. Thisaspect\n",
      "indicateswhethertheevidenceprovidesusefulinformation(Relevant)ornot(Irrelevant).\n",
      "{'page': 16.0, 'source': 'Self_RAG.pdf', 'score': 0.22770627}\n",
      "\n",
      "\n",
      "\n",
      "assessitsownpredictionsaftereachgeneratedsegmentasanintegralpartofthegenerationoutput.\n",
      "SELF-RAG furtherenablesacustomizabledecodingalgorithmtosatisfyhardorsoftconstraints,\n",
      "whicharedefinedbyreflectiontokenpredictions. Inparticular,ourinference-timealgorithmenables\n",
      "usto(1)flexiblyadjustretrievalfrequencyfordifferentdownstreamapplicationsand(2)customize\n",
      "models’behaviorstouserpreferencesbyleveragingreflectiontokensthroughsegment-levelbeam\n",
      "searchusingtheweightedlinearsumofthereflectiontokenprobabilitiesassegmentscore.\n",
      "Empiricalresultsonsixtasks,includingreasoningandlong-formgeneration,demonstratethatSELF-\n",
      "RAGsignificantlyoutperformspre-trainedandinstruction-tunedLLMsthathavemoreparametersand\n",
      "widelyadoptedRAGapproacheswithhighercitationaccuracy. Inparticular,SELF-RAGoutperforms\n",
      "retrieval-augmentedChatGPTonfourtasks,Llama2-chat(Touvronetal.,2023)andAlpaca(Dubois\n",
      "etal.,2023)onalltasks. Ouranalysisdemonstratestheeffectivenessoftrainingandinferencewith\n",
      "{'page': 1.0, 'source': 'Self_RAG.pdf', 'score': 0.22047843}\n",
      "\n",
      "\n",
      "\n",
      "guideddecodingframework,buttheyfocusonlyonreasoningtaskswithoneevaluationdimension\n",
      "(reasoningpathconsistency)andwithoutretrieval. RecentworkonLLMrefinement(Dhuliawala\n",
      "etal.,2023;Madaanetal.,2023;Pauletal.,2023)promptsamodeltogeneratetaskoutput,natural\n",
      "languagefeedbackandrefinedtaskoutputiteratively,butatthecostofinferenceefficiency.\n",
      "3 SELF-RAG: LEARNING TO RETRIEVE, GENERATE AND CRITIQUE\n",
      "We introduce Self-Reflective Retrieval-Augmented Generation (SELF-RAG), shown in Figure 1.\n",
      "SELF-RAGisaframeworkthatenhancesthequalityandfactualityofanLLMthroughretrievaland\n",
      "self-reflection,withoutsacrificingLLM’soriginalcreativityandversatility. Ourend-to-endtraining\n",
      "letsanLMMgeneratetextinformedbyretrievedpassages,ifneeded,andcriticizetheoutputby\n",
      "learningtogeneratespecialtokens. Thesereflectiontokens(Table1)signaltheneedforretrieval\n",
      "orconfirmtheoutput’srelevance,support,orcompleteness. Incontrast,commonRAGapproaches\n",
      "retrievepassagesindiscriminately,withoutensuringcompletesupportfromcitedsources.\n",
      "{'page': 2.0, 'source': 'Self_RAG.pdf', 'score': 0.21950878}\n",
      "\n",
      "\n",
      "\n",
      "outputissupportedbythecitedevidence). Humanannotatorsfind SELF-RAG answersareoften\n",
      "plausibleandsupportedbyrelevantpassageswithhigherS&Pscoresonshort-formPopQA,whichis\n",
      "consistentwithMenicketal.(2022). Humanannotatorsalsofind ISREL and ISSUP reflectiontoken\n",
      "predictionsaremostlyalignedwiththeirassessments. AppendixTable6showsseveralannotated\n",
      "examplesandexplanationsonassessments.\n",
      "6 CONCLUSION\n",
      "ThisworkintroducesSELF-RAG,anewframeworktoenhancethequalityandfactualityofLLMs\n",
      "throughretrievalondemandandself-reflection. SELF-RAGtrainsanLMtolearntoretrieve,generate,\n",
      "and critique text passages and its own generation by predicting the next tokens from its original\n",
      "vocabularyaswellasnewlyaddedspecialtokens,calledreflectiontokens. SELF-RAGfurtherenables\n",
      "thetailoringofLMbehaviorsattesttimebyleveragingreflectiontokens. Ourholisticevaluationson\n",
      "sixtasksusingmultiplemetricsdemonstratethatSELF-RAGsignificantlyoutperformsLLMswith\n",
      "moreparametersorwithconventionalretrieval-augmentedgenerationapproaches.\n",
      "10\n",
      "{'page': 9.0, 'source': 'Self_RAG.pdf', 'score': 0.2160148}\n",
      "\n",
      "\n",
      "\n",
      "Preprint.\n",
      "SELF-RAG: LEARNING TO RETRIEVE, GENERATE, AND\n",
      "CRITIQUE THROUGH SELF-REFLECTION\n",
      "AkariAsai†,ZeqiuWu†,YizhongWang†§,AvirupSil‡,HannanehHajishirzi†§\n",
      "†UniversityofWashington §AllenInstituteforAI ‡IBMResearchAI\n",
      "{akari,zeqiuwu,yizhongw,hannaneh}@cs.washington.edu,avi@us.ibm.com\n",
      "ABSTRACT\n",
      "Despitetheirremarkablecapabilities,largelanguagemodels(LLMs)oftenproduce\n",
      "responsescontainingfactualinaccuraciesduetotheirsolerelianceontheparamet-\n",
      "ricknowledgetheyencapsulate. Retrieval-AugmentedGeneration(RAG),anad\n",
      "hocapproachthataugmentsLMswithretrievalofrelevantknowledge,decreases\n",
      "suchissues. However,indiscriminatelyretrievingandincorporatingafixednumber\n",
      "ofretrievedpassages,regardlessofwhetherretrievalisnecessary,orpassagesare\n",
      "relevant,diminishesLMversatilityorcanleadtounhelpfulresponsegeneration.\n",
      "WeintroduceanewframeworkcalledSelf-ReflectiveRetrieval-AugmentedGen-\n",
      "eration(SELF-RAG)thatenhancesanLM’squalityandfactualitythroughretrieval\n",
      "{'page': 0.0, 'source': 'Self_RAG.pdf', 'score': 0.21204671}\n",
      "\n",
      "\n",
      "\n",
      "Preprint.\n",
      "APPENDIX\n",
      "A SELF-RAGDetails 17\n",
      "A.1 ReflectionTokens. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17\n",
      "A.2 SELF-RAGTraining. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17\n",
      "A.3 SELF-RAGInference . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19\n",
      "B ExperimentalDetails 19\n",
      "B.1 MoreDetailsofTraining . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19\n",
      "B.2 MoreDetailsofEvaluations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20\n",
      "C Results 20\n",
      "C.1 Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20\n",
      "C.2 HumanEvaluationExamples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21\n",
      "C.3 QualitativeExamples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21\n",
      "D FullListofInstructionsandDemonstrationsforGPT-4 21\n",
      "16\n",
      "{'page': 15.0, 'source': 'Self_RAG.pdf', 'score': 0.19946057}\n",
      "\n",
      "\n",
      "\n",
      "EthicsStatement HannanehHajishirzi.2024. Self-RAG:Learningto\n",
      "retrieve,generate,andcritiquethroughself-reflection.\n",
      "The experimental results on Adaptive-RAG vali- InTheTwelfthInternationalConferenceonLearning\n",
      "dateitsapplicabilityinrealisticscenarios,wherea Representations.\n",
      "widerangeofdiverseuserqueriesexist. Nonethe-\n",
      "JinheonBaek,SoyeongJeong,MinkiKang,JongPark,\n",
      "less,giventhepotentialdiversityofreal-worlduser andSungJuHwang.2023. Knowledge-augmented\n",
      "inputs,itiscrucialtoalsoconsiderscenarioswhere languagemodelverification. InProceedingsofthe\n",
      "2023ConferenceonEmpiricalMethodsinNatural\n",
      "these inputs might be offensive or harmful. We\n",
      "LanguageProcessing,EMNLP2023,Singapore,De-\n",
      "shouldbeawarethatsuchinputscouldleadtothe\n",
      "cember6-10,2023,pages1720–1736.Association\n",
      "retrieval of offensive documents and the genera- forComputationalLinguistics.\n",
      "tion of inappropriate responses by the retrieval-\n",
      "SebastianBorgeaud,ArthurMensch,JordanHoffmann,\n",
      "augmented LLMs. To address this challenge, de-\n",
      "{'page': 9.0, 'source': 'Adaptive_RAG.pdf', 'score': 0.19772303}\n",
      "\n",
      "\n",
      "\n",
      "and LM on instruction-tuning datasets in two steps. While we also train our model on diverse\n",
      "instruction-following datasets, SELF-RAG enables retrieval on demand and selection of the best\n",
      "possiblemodeloutputviafine-grainedself-reflection,makingitwidelyapplicableandmorerobust\n",
      "andcontrollable. Yoranetal.(2023)useanaturallanguageinferencemodelandXuetal.(2023)use\n",
      "asummarizationmodeltofilteroutorcompressretrievedpassagesbeforeusingthemtopromptthe\n",
      "LMtogeneratetheoutput. SELF-RAGprocessespassagesinparallelandfiltersoutirrelevantones\n",
      "throughself-reflection,withoutrelyingonexternalmodelsatinference. Moreover,ourself-reflection\n",
      "mechanismalsoevaluatesotheraspectsofthemodeloutputqualityincludingfactuality. LATS(Zhou\n",
      "etal.,2023)promptoff-the-shelfLMstosearchforrelevantinformationforquestionansweringtasks\n",
      "andtogeneratewithtreesearch,guidedbyLM-generatedvaluescores. Whiletheirvaluefunction\n",
      "simplyindicatesanoverallscoreofeachgeneration,SELF-RAGtrainstoanarbitraryLMtolearnto\n",
      "{'page': 2.0, 'source': 'Self_RAG.pdf', 'score': 0.19497271}\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 멀티쿼리가 정확도가 떨어질수도 있다.\n",
    "response = multiquery_retriever.invoke(\"self-rag에 대해서 알려줘.\")\n",
    "for result in response:\n",
    "    print(result.page_content)\n",
    "    print(result.metadata)\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reranker\n",
    "\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import CrossEncoderReranker\n",
    "from langchain_community.cross_encoders import HuggingFaceCrossEncoder\n",
    "\n",
    "reranker_model = HuggingFaceCrossEncoder(model_name=\"BAAI/bge-reranker-v2-m3\")\n",
    "\n",
    "# 상위 3개모델 선택\n",
    "compressor = CrossEncoderReranker(model=reranker_model, top_n=3)\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor, base_retriever=multiquery_retriever\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reoder : retirever에 이것을 덧댄다.\n",
    "from langchain_community.document_transformers import LongContextReorder\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "def reorder_documents(docs):\n",
    "    #재정렬\n",
    "    reordering = LongContextReorder()\n",
    "    reordered_docs = reordering.transform_documents(docs)\n",
    "    return reordered_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "# prompt\n",
    "template = \"\"\"\n",
    "    당신은 주어진 문서에 대해서 QA를 해주는 assistant bot입니다.\n",
    "    주어진 문서를 이용해서 답변해 주세요\n",
    "    만약 주어진 질문에 대해서 모른다면, 모른다고 답변해 주세요\n",
    "    문서의 출처와 페이지 넘버를 작성해주세요\n",
    "    한국어로 답변해주세요\n",
    "\n",
    "    #Example Format:\n",
    "    (brief summary of the answer)\n",
    "    (table)\n",
    "    (detailed answer to the question)\n",
    "\n",
    "    **출처**\n",
    "    - (page source and page number)\n",
    "\n",
    "    # Question:\n",
    "    {question}\n",
    "\n",
    "    # Context:\n",
    "    {context}\n",
    "\n",
    "    # Answer\n",
    "\"\"\"\n",
    "prompt = PromptTemplate.from_template(template, input_variable=[\"question\", \"context\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chain\n",
    "chain = ({\"context\":compression_retriever| RunnableLambda(reorder_documents), \"question\": RunnablePassthrough()}\n",
    "         |prompt\n",
    "         |llm\n",
    "         |StrOutputParser()\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adaptive-RAG는 다양한 복잡성을 가진 쿼리를 처리하기 위해 설계된 Retrieval-Augmented Generation 프레임워크입니다. 이 시스템은 쿼리의 복잡성에 따라 동적으로 쿼리 처리 전략을 조정하여, 가장 간단한 쿼리부터 단일 단계 접근 방식까지 다양한 쿼리를 효과적으로 처리할 수 있습니다.\n",
      "\n",
      "| Adaptive-RAG의 특징 | 설명 |\n",
      "|---------------------|------|\n",
      "| 동적 조정          | 쿼리의 복잡성에 따라 처리 전략을 조정 |\n",
      "| 다양한 쿼리 처리   | 간단한 쿼리부터 복잡한 쿼리까지 지원 |\n",
      "| 효율성              | 기존 방법보다 더 효과적이고 효율적임 |\n",
      "\n",
      "Adaptive-RAG는 쿼리 복잡성을 자동으로 분류하고, 모델 예측 결과에 기반하여 새로운 데이터를 생성하는 기능을 가지고 있습니다. 그러나 쿼리 복잡성을 잘못 라벨링할 가능성도 있으며, 향후 연구에서는 다양한 쿼리 복잡성을 주석 처리한 새로운 데이터셋을 생성할 필요가 있습니다.\n",
      "\n",
      "**출처**\n",
      "- Adaptive_RAG.pdf, 페이지 8, 13\n"
     ]
    }
   ],
   "source": [
    "print(chain.invoke(\"AdaptiveRAG에 대해서 설명해 주세요\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SelfRAG는 대형 언어 모델(LLM)의 품질과 사실성을 향상시키기 위해 설계된 새로운 프레임워크입니다. 이 프레임워크는 Self-Reflective Retrieval-Augmented Generation의 약자로, LLM이 관련 지식을 검색하고 생성하며 비판하는 과정을 통해 학습하도록 돕습니다. 기존의 Retrieval-Augmented Generation(RAG) 접근 방식은 무작위로 고정된 수의 검색된 구문을 통합하는 방식으로, 필요하지 않거나 관련성이 없는 구문을 포함할 수 있어 LLM의 다양성을 저하시킬 수 있습니다. SelfRAG는 이러한 문제를 해결하기 위해 LLM의 응답 품질을 높이고 사실성을 강화하는 데 중점을 둡니다.\n",
      "\n",
      "| 항목               | 내용                                                                 |\n",
      "|------------------|----------------------------------------------------------------------|\n",
      "| 프레임워크 이름      | Self-Reflective Retrieval-Augmented Generation (SelfRAG)            |\n",
      "| 목적               | LLM의 품질과 사실성 향상                                             |\n",
      "| 문제점            | 기존 RAG의 무작위 검색 및 통합 방식으로 인한 LLM의 다양성 저하       |\n",
      "| 해결책            | LLM이 검색, 생성, 비판하는 과정을 통해 학습하도록 지원              |\n",
      "\n",
      "SelfRAG는 LLM이 더 정확하고 유용한 응답을 생성할 수 있도록 돕는 혁신적인 접근 방식입니다.\n",
      "\n",
      "**출처**\n",
      "- Self_RAG.pdf, 페이지 0\n"
     ]
    }
   ],
   "source": [
    "print(chain.invoke(\"SelfRAG에 대해서 설명해 주세요\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SelfRAG와 AdaptiveRAG는 각각의 방식으로 정보 검색 및 생성 작업을 개선하는 기술입니다.\n",
      "\n",
      "| 기술         | 기능 설명                                                                                     |\n",
      "|--------------|----------------------------------------------------------------------------------------------|\n",
      "| SelfRAG      | 모델이 생성한 각 세그먼트 후 자신의 예측을 평가하고, 사용자 선호에 맞춰 검색 빈도를 조정할 수 있는 커스터마이징 가능한 디코딩 알고리즘을 제공합니다. |\n",
      "| AdaptiveRAG  | 다양한 사용자 쿼리에 대한 실용적인 시나리오에서의 적용 가능성을 검증하며, 비 offensive한 입력을 고려하여 적절한 응답을 생성하는 데 중점을 둡니다. |\n",
      "\n",
      "SelfRAG는 각 생성 세그먼트 후 모델이 자신의 예측을 평가하는 기능을 포함하고 있으며, 이를 통해 사용자 선호에 맞춰 모델의 행동을 조정할 수 있습니다. AdaptiveRAG는 다양한 사용자 쿼리에 대한 실험 결과를 통해 실제 시나리오에서의 적용 가능성을 입증하고, 비 offensive한 입력을 처리하는 데 중점을 두고 있습니다.\n",
      "\n",
      "**출처**\n",
      "- Self_RAG.pdf, 페이지 1\n",
      "- Adaptive_RAG.pdf, 페이지 9\n"
     ]
    }
   ],
   "source": [
    "print(chain.invoke(\"SelfRAG와 AdaptiveRAG로 무엇을 할 수 있는지 설명해 주세요\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-kr-gLkynrUQ-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
