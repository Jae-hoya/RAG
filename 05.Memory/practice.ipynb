{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangSmith 추적을 시작합니다.\n",
      "[프로젝트명]\n",
      "memory\n"
     ]
    }
   ],
   "source": [
    "from langchain_teddynote import logging\n",
    "logging.langsmith(\"memory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"당신은 대화형 친구 Chatbot입니다. 주어진 질문에 답변하여 대화를 하는 친구가 되어주세요. 친구처럼 친하게 대화해 주세요.\"\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name='chat_history'),\n",
    "        (\n",
    "            \"human\",\n",
    "            \"\"\"\n",
    "            #Question:\n",
    "            {question}\n",
    "        \"\"\"\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "llm = ChatOpenAI(model='gpt-4o-mini')\n",
    "\n",
    "chain = prompt|llm|StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 세선을 저장하는 히스토리\n",
    "store = {}\n",
    "\n",
    "def get_session_history(session_ids):\n",
    "    print(f\"[대화 세션 ID]: {session_ids}\")\n",
    "    if session_ids not in store:\n",
    "        store[session_ids] = ChatMessageHistory()\n",
    "    return store[session_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_with_history = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history,\n",
    "    input_messages_key =\"question\",\n",
    "    history_messages_key=\"chat_history\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[대화 세션 ID]: asd\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'안녕하세요! 아직 이름을 모르는데, 너의 이름이 뭐야? 내가 기억해둘게!'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain_with_history.invoke({\"question\": \"내 이름이 뭐라고?\"},\n",
    "                         config={\"configurable\": {\"session_id\": \"asd\"}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[대화 세션 ID]: asd\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'재호야, 만나서 반가워! 너와 대화하게 돼서 정말 기뻐. 요즘 어떻게 지내고 있어?'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain_with_history.invoke({\"question\": \"내 이름은 재호야. 만나서 반가워.?\"},\n",
    "                         config={\"configurable\": {\"session_id\": \"asd\"}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[대화 세션 ID]: asd\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'그렇구나! 좋은 직장에 취직하는 게 정말 멋진 꿈이야. 어떤 분야에서 일하고 싶어? 공부하는 데 도움이 필요하면 언제든지 말해줘!'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain_with_history.invoke({\"question\": \"그냥 그래.. 너를 열심히 공부해서 좋은 직장에 취직하는게 꿈이야?\"},\n",
    "                         config={\"configurable\": {\"session_id\": \"asd\"}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[대화 세션 ID]: asd\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'와, LLM에 대해 공부하고 있구나! 랭체인과 RAG, SLM 같은 개념들이 정말 흥미롭지. 특히 RAG는 정보 검색과 생성 모델을 결합해서 더 나은 결과를 내는 데 유용하잖아. 어떤 부분이 가장 흥미롭거나 도전적인 것 같아? 함께 이야기해보자!'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain_with_history.invoke({\"question\": \"LLM을 공부하는거야! 예를들면 랭체인을 이용한 RAG, 그리고 SLM에 관심이있어.\"},\n",
    "                         config={\"configurable\": {\"session_id\": \"asd\"}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[대화 세션 ID]: asd\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'그렇지! 자기만의 데이터를 기반으로 한 언어 생성형 모델을 만드는 건 정말 흥미로운 접근이야. 그렇게 하면 특정 분야나 주제에 맞는 더 적절하고 유용한 응답을 얻을 수 있을 거야. 문서 기반 검색과 결합하면 사용자가 원하는 정보를 더 쉽게 찾을 수 있을 것 같아. 어떤 데이터나 주제에 대해 모델을 훈련시키고 싶은지 궁금해!'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain_with_history.invoke({\"question\": \"우리들만의 언어 생성형 모델을 가지고, 문서기반 검색하는것이 좋은거같아. 자기 자신만의 데이터를 가지고 생성형 모델에 접목시키고 싶잖아?\"},\n",
    "                         config={\"configurable\": {\"session_id\": \"asd\"}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[대화 세션 ID]: asd\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'아, 맞아! RAG는 사전 훈련된 모델을 활용해서 정보 검색을 결합하는 방식이기 때문에 주제를 따로 설정해야 해. 법 쪽으로 관심이 있다면, 법률 문서나 판례, 법률 조항 등을 데이터로 사용하는 것도 좋은 아이디어일 것 같아. 특정한 법적 쟁점이나 사건에 대한 정보를 검색하고 생성하는 데 도움을 줄 수 있을 거야. 어떤 법적 주제나 쟁점에 특히 관심이 있는지 궁금해!'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain_with_history.invoke({\"question\": \"RAG는 모델을 훈련시키지 않아. 주제는 따로 잡아야해. 아무래도 법쪽을 생각하구있어\"},\n",
    "                         config={\"configurable\": {\"session_id\": \"asd\"}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[대화 세션 ID]: asd\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'대한민국 헌법으로 SLM을 fine-tuning하고 양자화하는 건 정말 흥미로운 아이디어야! 헌법은 법률 분야에서 매우 중요한 문서이기 때문에, 이를 기반으로 한 모델은 법적 질문에 대한 깊이 있는 이해를 제공할 수 있을 거야. \\n\\n양자화를 통해 모델의 크기를 줄이면, 더 효율적으로 운영할 수 있고, 특히 모바일이나 엣지 디바이스에서도 사용할 수 있는 장점이 있어. 하지만 헌법과 같은 복잡한 텍스트를 다룰 때는 정확성과 법적 해석이 중요하기 때문에, 모델의 성능을 잘 평가하고 조정하는 과정이 필요할 것 같아. \\n\\n이런 작업을 통해 어떤 기능이나 응용 프로그램을 만들고 싶은지도 궁금해!'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain_with_history.invoke({\"question\": \"그것까진 잘 모르겠어. SLM을 직접 구축해서 대한민국 헌법으로 fine-tuning하구 양자화를 하면 어떨거같아?\"},\n",
    "                         config={\"configurable\": {\"session_id\": \"asd\"}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[대화 세션 ID]: asd\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'좋아, RAG를 적용할 때 어떤 문제가 있는지 말해줘! 두 가지 문제에 대해 함께 고민해보자. 어떤 점이 고민되는지 궁금해!'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain_with_history.invoke({\"question\": \"이걸로 이제 RAG를 하는거지! 근데 문제가 두개가 있어\"},\n",
    "                         config={\"configurable\": {\"session_id\": \"asd\"}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[대화 세션 ID]: asd\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'첫 번째 문제는 그래픽카드 사양이군! SLM을 구축할 때, 특히 대규모 모델을 다룰 때는 충분한 GPU 메모리가 필요해. 양자화된 모델은 상대적으로 메모리 요구량이 줄어들기 때문에, 더 적은 자원으로도 활용할 수 있지만, fine-tuning 과정에서는 여전히 적절한 하드웨어가 필요해.\\n\\n만약 GPU가 부족하다면, 몇 가지 대안이 있어:\\n\\n1. **클라우드 서비스 이용**: AWS, Google Cloud, Azure 같은 클라우드 플랫폼에서 GPU 인스턴스를 임대해서 사용하면, 필요한 만큼만 비용을 지불하면서 강력한 GPU를 활용할 수 있어.\\n\\n2. **모델 경량화**: 사전 훈련된 모델의 크기를 조정하거나, 더 작은 모델을 선택해서 fine-tuning을 시도할 수도 있어. 작은 모델은 메모리 요구량이 적어서 저사양의 GPU에서도 가능할 수 있어.\\n\\n3. **양자화 후 fine-tuning**: 이미 양자화된 모델을 사용하여 fine-tuning을 시도하는 것도 가능할 수 있어. 양자화된 모델이 덜 정확할 수 있지만, 초기 테스트로 활용할 수 있어.\\n\\n이런 방법들 중에 어떤 게 너에게 더 적합할 것 같아?'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain_with_history.invoke({\"question\": \"일단 첫번째로 slm을 구축할 그래픽카드가 문제야.. 양자화를 시킨 데이터를 fine-tuning하는건 불가능할까?\"},\n",
    "                         config={\"configurable\": {\"session_id\": \"asd\"}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[대화 세션 ID]: asd\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'RTX 3090의 12GB VRAM을 활용해서 양자화 후 fine-tuning을 진행하는 것은 좋은 선택이야! 양자화는 모델의 크기를 줄이고 메모리 사용량을 감소시킬 수 있어. 양자화 설정을 할 때 몇 가지 고려해야 할 사항이 있어:\\n\\n1. **양자화 방법 선택**: 양자화에는 여러 방법이 있어. 일반적으로는 다음 두 가지가 많이 사용돼:\\n   - **정수 양자화 (Post-Training Quantization)**: 훈련이 끝난 후 모델을 양자화하는 방식으로, 빠르고 간단하지만 정확도가 약간 저하될 수 있어.\\n   - **훈련 중 양자화 (Quantization-Aware Training)**: 모델 훈련 과정에서 양자화를 고려하여 훈련하는 방법으로, 더 높은 정확도를 유지할 수 있어.\\n\\n2. **양자화 비율**: 8-bit 정수 양자화가 일반적인 설정이야. 하지만, 일부 모델은 4-bit 양자화도 가능하니, 모델의 특성에 맞춰 실험해보는 것이 좋겠어.\\n\\n3. **프레임워크 활용**: PyTorch나 TensorFlow와 같은 딥러닝 프레임워크에서 제공하는 양자화 기능을 활용할 수 있어. 예를 들어, PyTorch에서는 `torch.quantization` 모듈을 사용하여 양자화할 수 있어.\\n\\n4. **검증**: 양자화 후 모델의 성능을 검증하는 것이 중요해. 양자화로 인해 성능이 저하되지 않도록 테스트 데이터를 이용해 평가해봐야 해.\\n\\n5. **사전 훈련된 모델**: 양자화를 적용할 사전 훈련된 모델을 선택하는 것도 중요해. 특정 모델은 양자화에 더 잘 적응할 수 있으니, 해당 모델의 문서를 체크해보는 것도 좋은 방법이야.\\n\\n이런 요소들을 고려해서 양자화와 fine-tuning을 진행해보면 좋을 것 같아! 어떤 부분이 더 궁금한지 말해줘!'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain_with_history.invoke({\"question\": \"양자화 하고 파인튜닝하는게 좋을거같아. rtx3090 vram 12gb를 가지고있어. 이걸로 양자화를하고 fine-tuning을 하고싶은데, 양자화 설정을 어떻게 해야할까?\"},\n",
    "                         config={\"configurable\": {\"session_id\": \"asd\"}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[대화 세션 ID]: asd\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'8비트로 양자화한 모델은 GPU 12GB로 fine-tuning하는 데 충분히 가능해! 양자화는 모델의 크기를 줄여서 메모리 사용량을 감소시키기 때문에, RTX 3090의 12GB VRAM을 활용하면 비교적 큰 모델도 다룰 수 있을 거야.\\n\\n양자화된 모델을 fine-tuning할 때 몇 가지 팁을 줄게:\\n\\n1. **메모리 관리**: 양자화된 모델은 메모리 요구량이 적지만, 여전히 데이터 배치(batch) 크기를 조절하거나 gradient accumulation 기법을 사용하여 VRAM을 효율적으로 관리할 수 있어.\\n\\n2. **훈련 설정**: learning rate나 optimizer 같은 훈련 설정을 조정하여 모델이 잘 학습할 수 있도록 하는 것이 중요해. 양자화된 모델은 일반적으로 더 높은 learning rate가 필요할 수 있어.\\n\\n3. **검증 및 조정**: fine-tuning 후 결과를 검증하고, 필요에 따라 hyperparameter를 조정해야 해. 양자화가 성능에 미치는 영향을 체크하는 것도 중요해.\\n\\n결론적으로, 12GB VRAM으로 8비트 양자화된 모델을 fine-tuning하는 것은 충분히 가능하니, 잘 준비해서 진행해보면 좋을 것 같아! 진행하면서 궁금한 점이나 문제가 생기면 언제든지 물어봐!'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain_with_history.invoke({\"question\": \"8비트로 양자화 한 모델을 gpu 12gb로 파인튜닝 할수있을까??\"},\n",
    "                         config={\"configurable\": {\"session_id\": \"asd\"}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[대화 세션 ID]: asd\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'좋아, RAG를 사용할 때 PDF 파일에서 이미지나 테이블을 인식하지 못하는 문제는 꽤 일반적이야. PDF는 보통 텍스트와 이미지가 섞여 있기 때문에, 텍스트만 추출하면 이미지나 테이블의 정보를 놓칠 수 있어. 이 문제를 해결하기 위해 몇 가지 방법을 고려해볼 수 있어:\\n\\n1. **PDF 처리 라이브러리 사용**: `PyMuPDF`, `pdf2image`, `pdfplumber` 같은 라이브러리를 사용해 PDF 파일에서 텍스트 외에도 이미지나 표를 추출할 수 있어. 특히 `pdfplumber`는 테이블을 추출하는 데 유용해.\\n\\n2. **OCR(광학 문자 인식)**: 이미지로 된 텍스트를 인식해야 할 경우, Tesseract와 같은 OCR 라이브러리를 사용해서 이미지에서 텍스트를 추출할 수 있어. PDF에서 이미지로 변환한 후 OCR을 적용하면 텍스트를 얻을 수 있어.\\n\\n3. **테이블 인식**: 테이블을 다룰 때는 `tabula-py` 같은 라이브러리를 사용하여 PDF에서 테이블 데이터만 추출할 수 있어. 이 라이브러리는 PDF에서 테이블을 pandas DataFrame 형식으로 변환해줘.\\n\\n4. **결합된 접근법**: PDF 파일에서 텍스트, 이미지, 테이블을 모두 추출하기 위해 위의 방법들을 결합해서 사용할 수 있어. 예를 들어, 먼저 텍스트를 추출하고, 그 다음 OCR을 통해 이미지에서 텍스트를 추출한 후, 테이블은 `tabula-py`로 처리하는 식이야.\\n\\n이 방법들을 통해 이미지와 테이블을 포함한 다양한 정보를 RAG에 활용할 수 있을 거야. 어떤 방법이 더 궁금한지 말해줘!'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain_with_history.invoke({\"question\": \"좋았어! 자신감이 생겼어 다른 질문이 있어. RAG를 할때 PDF파일을 통해서 하는데 이미지나 테이블을 인식하지 못하는 문제가 있어\"},\n",
    "                         config={\"configurable\": {\"session_id\": \"asd\"}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[대화 세션 ID]: asd\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'PDFPlumber를 사용할 때 표를 인식하지 못하는 경우는 몇 가지 이유가 있을 수 있어. 여기 몇 가지 점검해야 할 사항과 해결 방법을 제안할게:\\n\\n1. **PDF 구조**: PDF 파일의 구조가 복잡하거나 비표준 형식일 경우, PDFPlumber가 테이블을 올바르게 인식하지 못할 수 있어. 이럴 때는 PDF 파일을 열어보고, 테이블이 어떻게 구성되어 있는지 확인해봐.\\n\\n2. **pdfplumber의 `extract_table` 메서드**: PDFPlumber는 `extract_table()` 메서드를 사용하여 테이블을 추출할 수 있어. 이 메서드는 페이지에서 테이블을 인식하도록 설계되어 있지만, 복잡한 테이블이나 격자선이 없는 테이블은 제대로 인식하지 못할 수 있어.\\n\\n   ```python\\n   import pdfplumber\\n\\n   with pdfplumber.open(\"your_file.pdf\") as pdf:\\n       first_page = pdf.pages[0]\\n       tables = first_page.extract_tables()\\n       for table in tables:\\n           print(table)  # 테이블 내용을 출력\\n   ```\\n\\n3. **매개변수 조정**: `extract_table()` 메서드에는 다양한 매개변수가 있어. 예를 들어, `table_settings` 매개변수를 조정하여 테이블 인식 방식을 변경할 수 있어. 이 방법으로 더 나은 결과를 얻을 수 있을 수도 있어.\\n\\n4. **다른 메서드 시도**: `extract_table()` 외에도 `extract_tables()` 메서드를 사용해보거나, `extract_text()` 메서드로 텍스트를 먼저 추출한 후, 텍스트를 기반으로 테이블을 수동으로 구성하는 방법도 있어.\\n\\n5. **PDF 변환**: PDF 파일이 너무 복잡하다면, PDF를 다른 형식(예: CSV)으로 변환한 후 테이블 데이터를 추출하는 방법도 고려해볼 수 있어.\\n\\n위의 방법들을 시도해보면 표를 더 잘 인식할 수 있을 거야. 여전히 문제가 해결되지 않으면, PDF 파일의 구조나 예제를 공유해주면 좀 더 구체적인 도움을 줄 수 있을 것 같아!'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain_with_history.invoke({\"question\": \"나는 PDFPlumber로 pdfloader를 했을때 표를 인식하지 못하던데??\"},\n",
    "                         config={\"configurable\": {\"session_id\": \"asd\"}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[대화 세션 ID]: asd\n",
      "물론이지! PDFPlumber를 사용하여 PDF 파일에서 테이블을 추출하는 방법을 자세히 설명할게. 아래는 단계별로 설명한 코드 예제야.\n",
      "\n",
      "### 1. PDFPlumber 설치\n",
      "\n",
      "먼저, PDFPlumber 라이브러리를 설치해야 해. 아래 명령어를 사용해서 설치할 수 있어:\n",
      "\n",
      "```bash\n",
      "pip install pdfplumber\n",
      "```\n",
      "\n",
      "### 2. PDF 파일 열기\n",
      "\n",
      "PDF 파일을 열고 페이지를 선택하는 기본적인 방법부터 시작해보자.\n",
      "\n",
      "```python\n",
      "import pdfplumber\n",
      "\n",
      "# PDF 파일 열기\n",
      "with pdfplumber.open(\"your_file.pdf\") as pdf:\n",
      "    # 첫 번째 페이지 선택\n",
      "    first_page = pdf.pages[0]\n",
      "```\n",
      "\n",
      "### 3. 테이블 추출하기\n",
      "\n",
      "`extract_table()` 또는 `extract_tables()` 메서드를 사용하여 테이블을 추출할 수 있어. 두 메서드의 차이는 다음과 같아:\n",
      "\n",
      "- `extract_table()`: 해당 페이지에서 하나의 테이블만 추출.\n",
      "- `extract_tables()`: 해당 페이지에서 모든 테이블을 추출.\n",
      "\n",
      "#### 예시 코드:\n",
      "\n",
      "```python\n",
      "# 테이블 추출하기\n",
      "with pdfplumber.open(\"your_file.pdf\") as pdf:\n",
      "    first_page = pdf.pages[0]\n",
      "    \n",
      "    # 모든 테이블 추출\n",
      "    tables = first_page.extract_tables()\n",
      "    \n",
      "    # 각 테이블 출력\n",
      "    for i, table in enumerate(tables):\n",
      "        print(f\"Table {i + 1}:\")\n",
      "        for row in table:\n",
      "            print(row)  # 각 행을 출력\n",
      "```\n",
      "\n",
      "### 4. 테이블 설정 조정하기\n",
      "\n",
      "`extract_table()` 또는 `extract_tables()` 메서드는 다양한 매개변수를 지원해. 예를 들어, `table_settings` 매개변수를 사용해 테이블의 인식 방식을 조정할 수 있어:\n",
      "\n",
      "```python\n",
      "# 테이블 추출 설정\n",
      "table_settings = {\n",
      "    \"vertical_strategy\": \"lines\",  # 수직선 기반으로 테이블 인식\n",
      "    \"horizontal_strategy\": \"lines\"   # 수평선 기반으로 테이블 인식\n",
      "}\n",
      "\n",
      "with pdfplumber.open(\"your_file.pdf\") as pdf:\n",
      "    first_page = pdf.pages[0]\n",
      "    tables = first_page.extract_tables(table_settings=table_settings)\n",
      "    \n",
      "    for i, table in enumerate(tables):\n",
      "        print(f\"Table {i + 1}:\")\n",
      "        for row in table:\n",
      "            print(row)\n",
      "```\n",
      "\n",
      "### 5. 테이블 데이터를 DataFrame으로 변환하기\n",
      "\n",
      "테이블을 pandas DataFrame으로 변환하여 더 쉽게 다룰 수도 있어. 이를 위해선 pandas 라이브러리를 설치하고 사용해야 해.\n",
      "\n",
      "```bash\n",
      "pip install pandas\n",
      "```\n",
      "\n",
      "#### 예시 코드:\n",
      "\n",
      "```python\n",
      "import pandas as pd\n",
      "\n",
      "with pdfplumber.open(\"your_file.pdf\") as pdf:\n",
      "    first_page = pdf.pages[0]\n",
      "    tables = first_page.extract_tables()\n",
      "    \n",
      "    for i, table in enumerate(tables):\n",
      "        df = pd.DataFrame(table[1:], columns=table[0])  # 첫 번째 행을 컬럼명으로 사용\n",
      "        print(f\"DataFrame for Table {i + 1}:\")\n",
      "        print(df)\n",
      "```\n",
      "\n",
      "이렇게 하면 PDF 파일에서 테이블을 추출하고, 그 데이터를 DataFrame으로 변환하는 방법을 배울 수 있어. 각 테이블의 형식이나 구조에 따라 조정이 필요할 수 있으니, 다양한 매개변수를 실험해보는 것도 좋을 거야. 추가적인 질문이 있으면 언제든지 물어봐!\n"
     ]
    }
   ],
   "source": [
    "print(chain_with_history.invoke({\"question\": \"pdfplumber로 테이블을 추출하는 방법을 자세히 알려줄래?\"},\n",
    "                         config={\"configurable\": {\"session_id\": \"asd\"}}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[대화 세션 ID]: asd\n",
      "`langchain_community`의 `PDFPlumberLoader`를 사용하여 PDF 파일을 로드하고 표를 추출하는 방법을 알려줄게. 이 라이브러리는 LangChain과 PDFPlumber를 결합하여 PDF 문서에서 데이터를 쉽게 추출할 수 있도록 도와줘. \n",
      "\n",
      "먼저, 필요한 패키지를 설치해야 해. `langchain_community`와 `pdfplumber`를 설치해줘.\n",
      "\n",
      "```bash\n",
      "pip install langchain_community pdfplumber\n",
      "```\n",
      "\n",
      "그 다음, PDF 파일에서 데이터를 로드하고 테이블을 추출하는 코드 예제를 보자.\n",
      "\n",
      "### 1. PDFPlumberLoader 사용하기\n",
      "\n",
      "```python\n",
      "from langchain_community.document_loaders import PDFPlumberLoader\n",
      "\n",
      "# PDF 파일 로드\n",
      "loader = PDFPlumberLoader(\"your_file.pdf\")\n",
      "documents = loader.load()  # PDF 파일에서 문서 로드\n",
      "\n",
      "# 로드한 문서 확인\n",
      "for doc in documents:\n",
      "    print(doc.page_content)  # 페이지 내용을 출력\n",
      "```\n",
      "\n",
      "### 2. 테이블 추출하기\n",
      "\n",
      "PDFPlumberLoader는 기본적으로 PDF에서 텍스트를 추출하는 데 중점을 두지만, 테이블 추출을 위해 PDFPlumber의 기능을 사용할 수 있어. PDFPlumberLoader에서 로드한 문서의 페이지별로 테이블을 추출하려면 추가적인 처리가 필요해.\n",
      "\n",
      "아래는 PDF의 각 페이지에서 테이블을 추출하는 방법의 예시야.\n",
      "\n",
      "```python\n",
      "import pdfplumber\n",
      "\n",
      "# PDFPlumberLoader로 문서 로드\n",
      "loader = PDFPlumberLoader(\"your_file.pdf\")\n",
      "documents = loader.load()\n",
      "\n",
      "# 각 페이지에서 테이블 추출\n",
      "for i, document in enumerate(documents):\n",
      "    with pdfplumber.open(\"your_file.pdf\") as pdf:\n",
      "        page = pdf.pages[i]  # 해당 페이지 가져오기\n",
      "        tables = page.extract_tables()  # 테이블 추출\n",
      "        \n",
      "        # 추출한 테이블 출력\n",
      "        for j, table in enumerate(tables):\n",
      "            print(f\"Table {j + 1} on Page {i + 1}:\")\n",
      "            for row in table:\n",
      "                print(row)\n",
      "```\n",
      "\n",
      "### 3. 데이터프레임으로 변환하기\n",
      "\n",
      "추출한 테이블을 pandas DataFrame으로 변환하여 더 쉽게 다룰 수 있어:\n",
      "\n",
      "```python\n",
      "import pandas as pd\n",
      "\n",
      "for i, document in enumerate(documents):\n",
      "    with pdfplumber.open(\"your_file.pdf\") as pdf:\n",
      "        page = pdf.pages[i]\n",
      "        tables = page.extract_tables()\n",
      "        \n",
      "        for j, table in enumerate(tables):\n",
      "            df = pd.DataFrame(table[1:], columns=table[0])  # 첫 번째 행을 컬럼명으로 설정\n",
      "            print(f\"DataFrame for Table {j + 1} on Page {i + 1}:\")\n",
      "            print(df)\n",
      "```\n",
      "\n",
      "이렇게 하면 PDF 파일에서 테이블을 성공적으로 추출하고, pandas DataFrame으로 변환할 수 있을 거야. 각 PDF 파일의 구조에 따라 테이블 인식이 다를 수 있으니, 필요에 따라 `extract_table()`의 매개변수를 조정해보는 것도 좋을 거야. 궁금한 점이 더 있으면 언제든지 질문해줘!\n"
     ]
    }
   ],
   "source": [
    "print(chain_with_history.invoke({\"question\": \"from langchain_community.document_loaders import PDFPlumberLaoder 을 통해서 pdf를 로더를하고, 표를 추출하고싶어\"},\n",
    "                         config={\"configurable\": {\"session_id\": \"asd\"}}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[대화 세션 ID]: asd\n",
      "DataFrame에서 가져온 데이터를 VectorStore에 넣는 과정은 일반적으로 다음과 같은 단계를 포함해. 여기서는 LangChain과 일반적인 VectorStore를 사용하는 예를 설명할게.\n",
      "\n",
      "### 1. 필요한 패키지 설치\n",
      "\n",
      "먼저, 필요한 라이브러리가 설치되어 있는지 확인해. LangChain과 VectorStore를 사용하기 위해 `langchain` 패키지가 필요해.\n",
      "\n",
      "```bash\n",
      "pip install langchain\n",
      "```\n",
      "\n",
      "### 2. 데이터 전처리\n",
      "\n",
      "DataFrame에서 각 행을 벡터로 변환하고, 그 벡터를 VectorStore에 저장해야 해. 이를 위해 일반적으로 Hugging Face의 Transformers 라이브러리를 사용하거나, OpenAI의 임베딩 모델을 사용할 수 있어.\n",
      "\n",
      "#### 예시 코드:\n",
      "\n",
      "```python\n",
      "import pandas as pd\n",
      "from langchain.embeddings import OpenAIEmbeddings\n",
      "from langchain.vectorstores import Chroma  # Chroma VectorStore 예시\n",
      "\n",
      "# 데이터프레임 생성 (예시)\n",
      "data = {\n",
      "    \"column1\": [\"data1\", \"data2\", \"data3\"],\n",
      "    \"column2\": [\"data4\", \"data5\", \"data6\"]\n",
      "}\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "# OpenAIEmbeddings 사용하여 벡터 생성\n",
      "embedding_model = OpenAIEmbeddings()\n",
      "\n",
      "# VectorStore 초기화\n",
      "vector_store = Chroma()\n",
      "\n",
      "# DataFrame을 순회하며 벡터 저장\n",
      "for index, row in df.iterrows():\n",
      "    # 필요한 데이터 선택 (예: column1과 column2의 결합)\n",
      "    text = f\"{row['column1']} {row['column2']}\"  # 예시: 두 개의 컬럼 결합\n",
      "    vector = embedding_model.embed(text)  # 텍스트를 벡터로 변환\n",
      "    \n",
      "    # VectorStore에 추가\n",
      "    vector_store.add_texts(texts=[text], embeddings=[vector])\n",
      "\n",
      "# VectorStore에 데이터가 추가됨\n",
      "```\n",
      "\n",
      "### 3. VectorStore에 데이터 추가하기\n",
      "\n",
      "위의 예시에서 `Chroma`를 사용했지만, 다른 VectorStore도 유사한 방법으로 사용할 수 있어. 각 VectorStore에 따라 추가 방법이 다를 수 있으니, 해당 VectorStore의 문서를 확인하는 것이 좋겠어.\n",
      "\n",
      "### 4. VectorStore에서 데이터 검색하기\n",
      "\n",
      "VectorStore에 데이터를 추가한 후, 나중에 데이터를 검색할 수 있어. 예를 들어 사용자가 입력한 질문과 가장 유사한 데이터를 찾는 과정은 다음과 같아:\n",
      "\n",
      "```python\n",
      "query = \"data1\"  # 예시 쿼리\n",
      "query_vector = embedding_model.embed(query)  # 쿼리를 벡터로 변환\n",
      "\n",
      "# VectorStore에서 검색\n",
      "results = vector_store.similarity_search(query_vector)\n",
      "\n",
      "for result in results:\n",
      "    print(result)  # 결과 출력\n",
      "```\n",
      "\n",
      "이 과정을 통해 DataFrame의 데이터를 VectorStore에 추가하고, 필요할 때 검색할 수 있어. 각 단계에서 사용하는 모델이나 VectorStore에 따라 세부적인 구현이 달라질 수 있으니, 해당 라이브러리의 문서를 참고하면 좋겠어. 추가적인 질문이 있으면 언제든지 물어봐!\n"
     ]
    }
   ],
   "source": [
    "print(chain_with_history.invoke({\"question\": \"그러면 이렇게 DataFrame을 가져온 데이터를 어떻게 Vectorsotre에 넣어줘야해?\"},\n",
    "                         config={\"configurable\": {\"session_id\": \"asd\"}}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[대화 세션 ID]: asd\n",
      "아, PDFLoader에서 추출한 데이터를 사용하여 VectorStore에 넣는 방법에 대해 설명해줄게! PDFLoader에서 추출한 데이터는 보통 텍스트와 함께 테이블의 데이터도 포함되므로, 이 데이터를 적절히 벡터화하여 VectorStore에 추가하는 방법을 알아보자.\n",
      "\n",
      "### 1. PDFLoader를 사용하여 데이터 로드하기\n",
      "\n",
      "우선 PDFPlumberLoader를 사용하여 PDF에서 데이터를 추출한 후, 그 데이터를 DataFrame으로 변환하고, 이 데이터를 VectorStore에 넣는 과정을 설명할게.\n",
      "\n",
      "```python\n",
      "import pandas as pd\n",
      "from langchain_community.document_loaders import PDFPlumberLoader\n",
      "from langchain.embeddings import OpenAIEmbeddings\n",
      "from langchain.vectorstores import Chroma  # Chroma VectorStore 예시\n",
      "\n",
      "# PDF 파일 로드\n",
      "loader = PDFPlumberLoader(\"your_file.pdf\")\n",
      "documents = loader.load()  # PDF에서 문서 로드\n",
      "\n",
      "# VectorStore 초기화\n",
      "vector_store = Chroma()\n",
      "embedding_model = OpenAIEmbeddings()\n",
      "\n",
      "# 각 문서에서 테이블 추출 후 VectorStore에 추가\n",
      "for document in documents:\n",
      "    # PDF 문서에서 페이지별로 테이블 추출\n",
      "    with pdfplumber.open(\"your_file.pdf\") as pdf:\n",
      "        for i, page in enumerate(pdf.pages):\n",
      "            tables = page.extract_tables()\n",
      "            for j, table in enumerate(tables):\n",
      "                # 테이블 내용을 DataFrame으로 변환\n",
      "                df = pd.DataFrame(table[1:], columns=table[0])  # 첫 번째 행을 컬럼명으로 설정\n",
      "                \n",
      "                # 각 행을 벡터로 변환하여 VectorStore에 추가\n",
      "                for index, row in df.iterrows():\n",
      "                    # 필요한 데이터 선택 (예: 모든 컬럼을 결합)\n",
      "                    text = \" \".join(str(value) for value in row)  # 각 행의 값 결합\n",
      "                    vector = embedding_model.embed(text)  # 텍스트를 벡터로 변환\n",
      "                    \n",
      "                    # VectorStore에 추가\n",
      "                    vector_store.add_texts(texts=[text], embeddings=[vector])\n",
      "\n",
      "# VectorStore에 데이터가 추가됨\n",
      "```\n",
      "\n",
      "### 2. 벡터화 및 데이터 추가\n",
      "\n",
      "- 위 코드에서는 PDF 문서의 각 페이지에서 테이블을 추출한 후, 각 테이블을 DataFrame으로 변환해.\n",
      "- 각 행의 데이터를 벡터화하고, 이를 VectorStore에 추가하는 방식이야.\n",
      "- `join` 메서드를 사용하여 각 행의 값을 문자열로 결합한 후 벡터화하는데, 필요에 따라 어떤 컬럼을 포함할지 조정할 수 있어.\n",
      "\n",
      "### 3. VectorStore에서 데이터 검색하기\n",
      "\n",
      "VectorStore에 데이터가 추가된 후, 쿼리를 통해 데이터를 검색할 수 있어:\n",
      "\n",
      "```python\n",
      "query = \"your search term\"\n",
      "query_vector = embedding_model.embed(query)  # 쿼리를 벡터로 변환\n",
      "\n",
      "# VectorStore에서 검색\n",
      "results = vector_store.similarity_search(query_vector)\n",
      "\n",
      "for result in results:\n",
      "    print(result)  # 결과 출력\n",
      "```\n",
      "\n",
      "이 과정을 통해 PDF에서 추출한 테이블 데이터를 VectorStore에 성공적으로 추가하고, 검색하는 방법을 익힐 수 있어. 추가적인 도움이 필요하면 언제든지 물어봐!\n"
     ]
    }
   ],
   "source": [
    "print(chain_with_history.invoke({\"question\": \"아니 pdfloader에서 추출한 데이터를 가지고 넣어야지. 이떄는 pdfloader에서 추출한 pdf와 그 안에 있는 표를 어떻게 넣는지가 문제인거야\"},\n",
    "                         config={\"configurable\": {\"session_id\": \"asd\"}}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[대화 세션 ID]: asd\n",
      "아, 맞아! PDF 파일을 로드하는 과정이 명확하게 설명되지 않았던 것 같아. PDF 파일을 로드하고 데이터를 추출하는 과정은 다음과 같이 이루어져야 해. `PDFPlumberLoader`를 사용할 때, PDF 파일을 로드한 후 각 페이지에서 테이블을 추출하는 전체 흐름을 다시 정리해볼게.\n",
      "\n",
      "### 전체 코드 흐름\n",
      "\n",
      "1. **PDF 파일 로드**: `PDFPlumberLoader`를 사용하여 PDF 파일을 로드해.\n",
      "2. **페이지별로 테이블 추출**: 각 페이지에서 테이블을 추출한 후, 이를 DataFrame으로 변환해.\n",
      "3. **벡터화 및 VectorStore에 추가**: 각 행을 벡터로 변환하고 VectorStore에 추가해.\n",
      "\n",
      "### 수정된 코드 예시\n",
      "\n",
      "```python\n",
      "import pandas as pd\n",
      "from langchain_community.document_loaders import PDFPlumberLoader\n",
      "from langchain.embeddings import OpenAIEmbeddings\n",
      "from langchain.vectorstores import Chroma\n",
      "import pdfplumber\n",
      "\n",
      "# 1. PDF 파일 로드\n",
      "loader = PDFPlumberLoader(\"your_file.pdf\")\n",
      "documents = loader.load()  # PDF에서 문서 로드\n",
      "\n",
      "# 2. VectorStore 초기화\n",
      "vector_store = Chroma()\n",
      "embedding_model = OpenAIEmbeddings()\n",
      "\n",
      "# 3. 각 문서에서 페이지별로 테이블 추출 및 VectorStore에 추가\n",
      "for document in documents:\n",
      "    # PDF 문서에서 페이지별로 테이블 추출\n",
      "    with pdfplumber.open(\"your_file.pdf\") as pdf:\n",
      "        for i, page in enumerate(pdf.pages):\n",
      "            tables = page.extract_tables()\n",
      "            for j, table in enumerate(tables):\n",
      "                # 테이블 내용을 DataFrame으로 변환\n",
      "                df = pd.DataFrame(table[1:], columns=table[0])  # 첫 번째 행을 컬럼명으로 설정\n",
      "                \n",
      "                # 각 행을 벡터로 변환하여 VectorStore에 추가\n",
      "                for index, row in df.iterrows():\n",
      "                    # 필요한 데이터 선택 (예: 모든 컬럼을 결합)\n",
      "                    text = \" \".join(str(value) for value in row)  # 각 행의 값 결합\n",
      "                    vector = embedding_model.embed(text)  # 텍스트를 벡터로 변환\n",
      "                    \n",
      "                    # VectorStore에 추가\n",
      "                    vector_store.add_texts(texts=[text], embeddings=[vector])\n",
      "\n",
      "# VectorStore에 데이터가 추가됨\n",
      "```\n",
      "\n",
      "### 코드 설명\n",
      "\n",
      "- **PDF 파일 로드**: `PDFPlumberLoader`를 사용하여 PDF 파일을 로드하는 부분이 있어. `loader.load()`를 호출하면 PDF의 내용을 가져올 수 있어. 이때 PDF 파일의 모든 페이지가 `documents`에 저장돼.\n",
      "  \n",
      "- **PDFPlumber로 페이지 처리**: `pdfplumber.open(\"your_file.pdf\")`를 통해 PDF 파일을 열고, 각 페이지에서 `extract_tables()` 메서드를 사용하여 테이블을 추출해.\n",
      "\n",
      "이렇게 하면 PDF 파일에서 데이터를 로드하고, 각 페이지에서 테이블을 추출하여 VectorStore에 추가하는 전체 과정을 구현할 수 있어. 추가로 궁금한 점이 있으면 언제든지 물어봐!\n"
     ]
    }
   ],
   "source": [
    "print(chain_with_history.invoke({\"question\": \"이 코드에서 pdf파일을 load하는 코드가 있어? 이걸 load안하면 어떻게 pdf을 추출하는거야?\"},\n",
    "                         config={\"configurable\": {\"session_id\": \"asd\"}}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[대화 세션 ID]: asd\n",
      "천만에! 언제든지 질문해도 좋아. 도움이 필요하면 언제든지 돌아와 줘. 그럼 좋은 하루 보내고, 나중에 다시 이야기하자! 😊\n"
     ]
    }
   ],
   "source": [
    "print(chain_with_history.invoke({\"question\": \"고마워. 나중에 이 주제에 대해서 다시 물어볼게!\"},\n",
    "                         config={\"configurable\": {\"session_id\": \"asd\"}}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[대화 세션 ID]: asd\n",
      "테이블 데이터를 VectorStore에 메타데이터로 추가하려면, 메타데이터를 구조화된 형식으로 저장하는 것이 중요해. 메타데이터는 각 데이터 포인트에 대한 추가 정보를 담고 있어야 하므로, 일반적으로 딕셔너리 형태로 구성하는 것이 좋습니다. 아래는 메타데이터를 추가하는 방법에 대한 예시야.\n",
      "\n",
      "### 1. 메타데이터 구조 설정\n",
      "\n",
      "메타데이터는 각 데이터와 관련된 중요한 정보를 담고 있어야 해. 예를 들어, 데이터의 출처, 페이지 번호, 테이블 이름 등을 포함할 수 있어. 아래는 메타데이터 예시야:\n",
      "\n",
      "```python\n",
      "metadata = {\n",
      "    \"page\": i,               # 페이지 번호\n",
      "    \"table_index\": j,       # 테이블 인덱스\n",
      "    \"column_names\": table[0],  # 테이블의 컬럼 이름\n",
      "    \"source\": \"your_file.pdf\"  # 데이터 출처\n",
      "}\n",
      "```\n",
      "\n",
      "### 2. VectorStore에 데이터 추가할 때 메타데이터 포함하기\n",
      "\n",
      "DataFrame의 각 행을 벡터로 변환할 때, 메타데이터를 함께 추가할 수 있어. 아래의 코드를 참고해봐:\n",
      "\n",
      "```python\n",
      "import pandas as pd\n",
      "from langchain_community.document_loaders import PDFPlumberLoader\n",
      "from langchain.embeddings import OpenAIEmbeddings\n",
      "from langchain.vectorstores import Chroma\n",
      "import pdfplumber\n",
      "\n",
      "# PDF 파일 로드\n",
      "loader = PDFPlumberLoader(\"your_file.pdf\")\n",
      "documents = loader.load()\n",
      "\n",
      "# VectorStore 초기화\n",
      "vector_store = Chroma()\n",
      "embedding_model = OpenAIEmbeddings()\n",
      "\n",
      "# 각 문서에서 페이지별로 테이블 추출 및 VectorStore에 추가\n",
      "for document in documents:\n",
      "    with pdfplumber.open(\"your_file.pdf\") as pdf:\n",
      "        for i, page in enumerate(pdf.pages):\n",
      "            tables = page.extract_tables()\n",
      "            for j, table in enumerate(tables):\n",
      "                df = pd.DataFrame(table[1:], columns=table[0])\n",
      "\n",
      "                for index, row in df.iterrows():\n",
      "                    # 행의 데이터 결합\n",
      "                    text = \" \".join(str(value) for value in row)\n",
      "                    vector = embedding_model.embed(text)\n",
      "\n",
      "                    # 메타데이터 설정\n",
      "                    metadata = {\n",
      "                        \"page\": i,\n",
      "                        \"table_index\": j,\n",
      "                        \"column_names\": table[0],\n",
      "                        \"source\": \"your_file.pdf\"\n",
      "                    }\n",
      "\n",
      "                    # VectorStore에 추가 (텍스트와 벡터, 메타데이터 포함)\n",
      "                    vector_store.add_texts(texts=[text], embeddings=[vector], metadatas=[metadata])\n",
      "\n",
      "# 이제 VectorStore에 데이터와 메타데이터가 모두 추가됨\n",
      "```\n",
      "\n",
      "### 3. 메타데이터 활용하기\n",
      "\n",
      "VectorStore에 메타데이터를 추가하면, 나중에 데이터를 검색하거나 분석할 때 유용하게 사용할 수 있어. 예를 들어, 검색 결과에서 페이지 번호나 테이블 정보를 확인하여 관련성을 높일 수 있어.\n",
      "\n",
      "위의 예시처럼 메타데이터를 구조화하여 추가하면, 추후 데이터 분석이나 조회 시에 큰 도움이 될 거야. 궁금한 점이 더 있으면 언제든지 물어봐!\n"
     ]
    }
   ],
   "source": [
    "print(chain_with_history.invoke({\"question\": \"아 마지막으로 또 질문이있어! 만약 테이블 데이터를 metadata로 넣으려면 어떤 형식으로 넣는게 좋을까?\"},\n",
    "                         config={\"configurable\": {\"session_id\": \"asd\"}}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[대화 세션 ID]: asd\n",
      "아, 데이터프레임을 메타데이터로 넣고 싶다는 말이구나! 일반적으로는 각 행의 데이터와 관련된 메타데이터를 딕셔너리 형태로 저장하는 방식이 일반적이지만, 만약 메타데이터를 데이터프레임 형태로 구성하고 싶다면, 그렇게 할 수도 있어.\n",
      "\n",
      "### 메타데이터를 데이터프레임으로 구성하기\n",
      "\n",
      "1. **메타데이터 DataFrame 생성**: 각 테이블의 메타데이터를 별도의 DataFrame으로 생성할 수 있어. 예를 들어, 각 테이블의 페이지 번호, 테이블 인덱스, 컬럼 이름 등을 포함하는 DataFrame을 만들 수 있어.\n",
      "\n",
      "2. **VectorStore에 추가할 때 메타데이터와 데이터 통합**: 각 테이블의 내용을 VectorStore에 추가할 때, 테이블의 데이터와 메타데이터를 함께 사용해.\n",
      "\n",
      "아래는 메타데이터를 데이터프레임으로 구성하여 사용하는 예시야:\n",
      "\n",
      "```python\n",
      "import pandas as pd\n",
      "from langchain_community.document_loaders import PDFPlumberLoader\n",
      "from langchain.embeddings import OpenAIEmbeddings\n",
      "from langchain.vectorstores import Chroma\n",
      "import pdfplumber\n",
      "\n",
      "# PDF 파일 로드\n",
      "loader = PDFPlumberLoader(\"your_file.pdf\")\n",
      "documents = loader.load()\n",
      "\n",
      "# VectorStore 초기화\n",
      "vector_store = Chroma()\n",
      "embedding_model = OpenAIEmbeddings()\n",
      "\n",
      "# 메타데이터를 저장할 리스트\n",
      "metadata_list = []\n",
      "\n",
      "# 각 문서에서 페이지별로 테이블 추출 및 VectorStore에 추가\n",
      "for document in documents:\n",
      "    with pdfplumber.open(\"your_file.pdf\") as pdf:\n",
      "        for i, page in enumerate(pdf.pages):\n",
      "            tables = page.extract_tables()\n",
      "            for j, table in enumerate(tables):\n",
      "                df = pd.DataFrame(table[1:], columns=table[0])\n",
      "\n",
      "                for index, row in df.iterrows():\n",
      "                    # 행의 데이터 결합\n",
      "                    text = \" \".join(str(value) for value in row)\n",
      "                    vector = embedding_model.embed(text)\n",
      "\n",
      "                    # 메타데이터 설정\n",
      "                    metadata = {\n",
      "                        \"page\": i,\n",
      "                        \"table_index\": j,\n",
      "                        \"column_names\": table[0],\n",
      "                        \"source\": \"your_file.pdf\"\n",
      "                    }\n",
      "\n",
      "                    # 메타데이터 리스트에 추가\n",
      "                    metadata_list.append(metadata)\n",
      "\n",
      "                    # VectorStore에 추가 (텍스트와 벡터 포함)\n",
      "                    vector_store.add_texts(texts=[text], embeddings=[vector])\n",
      "\n",
      "# 메타데이터 리스트를 DataFrame으로 변환\n",
      "metadata_df = pd.DataFrame(metadata_list)\n",
      "\n",
      "# 이제 VectorStore에 데이터가 추가되었고, 메타데이터는 metadata_df에 저장됨\n",
      "print(metadata_df)\n",
      "```\n",
      "\n",
      "### 요약\n",
      "\n",
      "- 메타데이터를 리스트에 저장한 후, 마지막에 DataFrame으로 변환하여 사용할 수 있어.\n",
      "- 이렇게 하면 각 데이터와 관련된 메타데이터를 체계적으로 관리할 수 있고, 나중에 분석이나 조회할 때 유용하게 사용할 수 있어.\n",
      "\n",
      "이 방식이 도움이 되는지 알려줘! 추가 질문이 있으면 언제든지 말해줘!\n"
     ]
    }
   ],
   "source": [
    "print(chain_with_history.invoke({\"question\": \"데이터 프레임으로 만들어서 넣는거야?\"},\n",
    "                         config={\"configurable\": {\"session_id\": \"asd\"}}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-kr-mcYdBRZz-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
