{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_teddynote.document_loaders import HWPLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = HWPLoader(\"./data/RAG draft.hwp\")\n",
    "\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(len(docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': './data/RAG draft.hwp'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "한국정보통신학회논문지 Vol. 23, No. 1: 399~406, Mar. 2019제목김재호1 · 김장영2*영문제목Jae-Ho Kim1 ·  Jang-Young Kim2*1Graduate Student, Department of Computer Science, The University of Suwon, Hwaseong, 18323 Korea 2*Associate Professor, Department of Computer Science, The University of Suwon, Hwaseong, 18323 Korea요  약 JKIICEJournal of the Korea Institute of Information andCommunication EngineeringReceived 29 January 2019,          Revised 29 March 2019,       Accepted 21 April 2019(출판사에서작성)* Corresponding Author Jang-Young Kim(E-mail:jykim77@suwon.ac.kr, Tel:+82-31-229-8345)Associate Professor, Department of Computer Science , The University of Suwon, Hwasung, 18323 Korea http://doi.org/10.6109/jkiice.2019.23.1.399       ⴅȀpISSN:2234-4772Open AccessThis is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License(http://creativecommons.org/licenses/ by-nc/3.0/) which permits unrestricted  non-commercial use, distribution, and reproduction in any medium, provided   the original work is properly cited.Copyright Ⓒ The Korea Institute of Information and Communication Engineering.ABSTRACT키워드 : Keywords : \n",
      "Ⅰ. 서  론LLM이 많이 발전하면서, GPT, Claude, Llama모델등을 효과적으로 사용하기 위한 방법들이 각광을 받고 있다. 일반적인 연구자들은 비용과 시간적인 측면에서 산업계에 비해 한계를 가지고 있다. 따라서 일반 연구자들은 새로운 대형 LLM을 개발하는 혁신전인 주제에 대한 연구는 포기하고, 기술적인 세부사항이나 작은 문제에 집중하는게 효과적이기 때문이다. 예를들면, 특정 domain을 위한 모델에이 Task를 해결하기 위해 집중하면, 모델의 scale은 작아질 수 있는 부분들을 다룰수 있다. 또는 산업계에서 모델들을 공개하면, 연구자들은 공개된 사전학습을 모델하는 방향으로, 우리에게 맞는 무기를 찾아나가고 있는 추세다. [1Survival Startegies for Depressed AI Academics]. 이중에서 기존 모델에서 특정 도메인에 맞게 훈련시키는 Fine-Tuning, 모델의 크기를 줄이기 위한 방법인 Quantization, Automatic Mixed Precision (AMP)기법들이 부상함과 동시에, LLM의 할루시네이션, 최신 데이터 갱신, 보안성, 설명력 부족등을 보완하기 위한 RAG(Retrieval Augmendted Genaration)기술 또한 부상하고 있다. RAG란  데이터가 적을 때 RAG를 고려할 수 있는데, NLP분야에서, 정보검색을 통해 언어 생성 과정을 향상시키는 방법이다. 일반적으로 정보검색 단계와 언어생성 단계로 나누며, 가지고 있는 데이터, 정보를 검색하고, 관련성 높은 데이터를 데이터베이스나 인터넷에서 검색한다. 보통 데이터베이스에서 찾으며, 이때 유사도 검색 같은 기술이 사용된다. 핵심은 일반적인 데이터가 아니라 내가 가지고 있는 데이터를 기반으로 답변하는 것이다. 이것을 정보 검색 단계라고 한다.ȫĀ언어 생성 단계에서는 검색된 정보를 바탕으로 생성모델(LLM)이 답변을 생성한다. 생성모델은 검색된 Query에 대해는 컨텐츠 필터링을 할 수 있고, query를 검색엔진에 검색을 하는데 유사도 기반으로 검색한다. 이때 query도 embedding처리 되기 때문에, vector로 변환된다.  검색은 retrieval data에 하고, 검색엔진은 retrieval된 data(인덱싱된 데이터)를 기반으로 관련 답변을 LLM을 통해 답변을 생성성하고  제대로 된 답변을 하는지 모니터링을 하기 위해선 저장이 필요하다. 하지만 많은 사용자들은 RAG가 생각보다 잘 작동하지 않는다는 말들을 한다. 본 연구에서는 왜 이 RAG가 잘 되지 않는지, 효과적으로 RAG를 잘 하기 위해서 고려해야할 사항들, 방법들에 대해서 이야기한다.Ⅱ. 기존연구2.1 Retrieval-Augmented Generation for Knowledge-Intensive NLP TasksⅢ. 분석 알고리즘3.1 Ⅳ.실험결과 및 분석 4.1 데이터가 적을 때 RAG를 고려할 수 있다.RAG는 NLP분야에서, 정보검색을 통해 언어 생성 과정을 향상시키는 방법. 내가 가지고 있는 데이터, 정보를 검색한다. 관련성 높은 데이터를 데이터베이스나 인터넷에서 찾는다. 보통 데이터베이스에서 찾는다. 이때 유사도 검색 같은 기술이 사용된다. 핵심은 일반적인 데이터가 아니라 내가 가지고 있는 데이터를 기반으로 답변하는 것이다.1.ݶĀ정보 검색 단계: 가장 관련성이 높은 정보를 데이터 베이스나 인터넷에서 찾는다. 검색에는 유사도 검색 같은 기술이 사용되며 핵심은 내가 가지고 있는 데이터가 있다는 것이다.2.ݶĀ언어 생성 단계: 검색된 정보를 바탕으로 생성모델(LLM)이 답변을 생성한다. 생성모델은 검색된 우리의 RAG는 왜 잘되지 않을까?많은 사람들은 RAG를 처음 접하고 이런 생각을 할것입니다. 생각보다 원하는만큼 잘 되지않네?저도 제가 원했던것 많큼 결과를 도출하지 못했던 경험이 있습니다. 이것을 토대로 포트폴리오를 작성했습니다. 이 모든것들은 1. Embedding- Embedding model의 선택을 무시하는 경우가 많습니다. 생각했던것 보다 Embedding model은 중요합니다. Embedding모델은 표현의 다양성을 늘려준다고 보시면 됩니다.RAG를 했을 때, 원하는 답변을 받지 못하는경우, 고려사항입니다. 가장 중요한것은 지원하는 언어를 확인하는 것 입니다. 모델의 document를 확인하면 어떤 모델을 지원하는지 확인할 수 있습니다.높은 차원은 더 풍부한 정보 표현이 가능하지만, 계산 비용이 증가합니다. 이 높은 차원은 RAG의 출력에 영향을 줍니다.예시는 다음과 같습니다.(1) OepnAIEmbedding: text-embedding-3-small model.https://platform.openai.com/docs/guides/embeddings/embedding-modelsGPT-4로 학습된 최신 모델, 다국어 지원 강화차원: 1536최대 토큰: 8191 (0까지의 인덱싱을 포함할 경우 총 8092개.)--------------------https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models?hl=kohttps://ai.google.dev/gemini-api/docs/models/gemini?hl=ko(2) Google embedding-001차원: 768최대 토큰: ৄĀ2,048특징:PaLM 모델을 기반으로 함다양한 작업에 범용적으로 사용 가능(3) Google: embedding-004차원: 768최대 토큰: ৄĀ2,048차이는 없다. 모두 retriever에 제한적이다. 가장 큰 단점은 최대 토큰 길이도 작지만, 한국어를 지원하지 않는다.-------------------(4) HuggingFace: BAAI/bge-m3차원: 1024최대 토큰:           8192 RAG의 검색 파이프라인에 대한 몇 가지 제안다음 파이프라인을 사용하는 것이 좋습니다: 하이브리드 검색 + 재순위 지정.하이브리드 검색은 다양한 방법의 강점을 활용하여 더 높은 정확도와 더 강력한 일반화 기능을 제공합니다. 고전적인 예: 임베딩 검색과 BM25 알고리즘을 모두 사용하는 것입니다. 이제 임베딩과 스파스 검색을 모두 지원하는 BGE-M3를 사용해 볼 수 있습니다. 이를 통해 밀집 임베딩을 생성할 때 추가 비용 없이 토큰 가중치(BM25와 유사)를 얻을 수 있습니다. 하이브리드 검색을 사용하려면 Vespa 와 Milvus를 참조할 수 있다.교차 인코더 모델로서, re-ranker는 bi-encoder 임베딩 모델보다 더 높은 정확도를 보여줍니다. 검색 후 re-ranking 모델(예: bge-reranker , bge-reranker-v2 )을 활용하면 선택된 텍스트를 추가로 필터링할 수 있습니다.이것은 Semantic Chunker에도 영향을 미친다.2. TextSplitter텍스트 분할의 필요성은, RAG를 할 때, 문서를 세분화 함으로써 질문에 대한 연관성이 있는 정보만 가져오는데 도움이 된다. 각각의 특정 주제나, 내용에 초점을 맞추므로, 관련성이 높은 정보를 제공한다.또한 chunk를 잘 쪼개면, 연관된것을 골라서 가져오는데, 이것은 retriever할때 유사도가 높아진다는 뜻이다.또한 전체문서를 llm으로 입력을 하면, 토큰비용이 많이 발생하고, 효율적인 답변을 많은 정보속에서 발췌하여 답변하지 못한다. 이것은 할루시네이션 문제로 연결된다. 할루시네이션을 해결하기 위한 방법으로 제시되는 방법중 하나다.즉, 필요한 정보면 발췌하기 위한 목적도 있다.이것은 정답이 없다. 많은 경험을 해보고 적합한 전략을 짜야한다. 일반적으로  RecursiveCharacterTextSplitter를 사용한다. chunk가 충분히 작아질 떄 까지 주어진 목록의 순서대로 텍스트를 분할한다.이때 우리는 chunk_size와 chunk_overlap을 정한다. 문장의 길이를 정하는것인데,chunk_size가 커질수록 맥락을 더 잘 이해하지만 정확도가 떨어진다.chunk_size가 커질수록 맥락의 이해도가 떨어지지만 정확도가 높아진다.즉 trade-off관계가 있다.이것은 우리가 직접 문서를 보고 정하는게 대략 정하는게 가장 좋다.이 자르는 과정은 retriever하는 과정에서 중요하다. chunk_overlap은 상당히 중요한 역할을한다.우리는 맥락적으로 자르고 싶을때가 있다. 이때는 SemanticChunk를 사용한다.보통 PDF파일과 같이 text문서들은 글의 맥락들, 논리성이 중요하기 때문에, RecursiveCharacterTextSplitter를 사용한다.CSV 파일의 경우에는 CharacterTextSplitter를 사용하여 각 행을 개별적인 텍스트 블록으로 분할한다.3. VectorDB정보를 벡터로 저장하는 데이터베이스.벡터데이터: 다차원 벡터 공간에 표현되는 데이터를 의미. 일반적으로 기계학습에 사용되는 임베딩 알고리즘에서 파생.1. 효과적인 데이터 관리가 가능하다. 방대한 양의 비정형 데이터를 처리하기 위해 사용. 2. 검색기능에 효과적이다. 추천시스템, 자연어처리등 응용 프로그램에 필수적인 검색기능에 탁월하다. - RAG에선 유사성3. 확장성. 동적으로 셋팅하기편함4. ai와의 호환성.어떤 DB를 써야할까? - 사용자가 많은, reference들이 많은걸 이용하면 좋다. 즉 많이 사용하는 것들을 찾아서, 어떤것에 유용한지 찾아보고 사용하는것이 좋다.특성을 많이 타지않고, 보편적으로 많이 사용되는 CHROMADB와 FASS1. ChromaDB의 경우에는, 이미지 처리를 위해 특별히 설계. 대규모 색상 데이터를 관리하고 검색. 색상 히스토그램 및 기타 색상 표현 작업에 최적화Color-specific indexing, Querying bt color similarity2. FAISS 유사성 검색을 위해 설계된 범용적 라이브러리. vectordb의 초석이라고 볼수도 있다.특정 유형에 국한되지 않고, 다양한 애플리 케이션에 적용 가능하다.versatility, efficiency, integration with deep learning freameworks----4. prompt : HuggingFace에서 local model을 이용할 경우, 답변이 이상하게 나오는 경우가 있다 이것은, 모델마다 훈련되는 데이터의 template가 다르기 때문이다.만약 답변이 이상하게 된다면, prompt를 모델이 제공하는 chat format에 맞추어 작성하면 된다.여기서 prompt는 output parser와 큰 연관이 있는데, 이 둘을 잘 결합하여 원하는 출력 형태를 얻을 수 있다.모델의 Inteligence가 높다면, prompt로부터 유연해진다.5. llm 답변을 생성할때 사용된다. 문서 검색과 prompt와결합이 되어 LLM을 통해 문장을 생성한다.llm에서 중요한것은 첫번째로, 임베딩때와 마찬가지로 원하는 언어를 즉, 한국어를 잘 하는지 확인하는 것 입다. 만약, 한국어를 지원하지 않는다면 한국어를 가지고 파인튜닝을 고려할 수 있다.하지만 이것은 많은 비용이 발생할 수 있다. 처음부터 한국어를 잘 하는지, 또한 retriever에 대한 답변을 잘 하는지 고려해야 한다. 그것이 아니라면, huggigng face에서 한국어로 파인튜닝한 모델을 고려하는것도 좋은 방법이다.실제로, 한국어를 잘 못한다면, 답변에 있어서 맥락이 맞지 않거나, 오로지 영어로만 답변하는 상황이 초래된다.추가적으로, prompt를 입력할때, llm의 inteligence에 따라서 prompt를 이행하는지, 못하는지도 결정이 됩니다. 낮은 inteligence모델은 prompt를 이행하지 못하는 결과를 초래하게 됩니다.마지막으로, local model을 이용할 때, 모델의 파라미터가 크다면, 시간이 매우 오래걸릴수 있다는 점입니다. 이럴때도 파인튜닝 또는 양자화를 고려할 수 있다.따라서 llm은 매우 중요하다. 모델의 크기가 크면 Inteligence가 높은게 일반적이지만, RAG에 있어서 무조건 그렇지 않기도 하다. 예를들어 EEVE010.8B model은 chat form에 따라prompt를 맞춰야 잘 출력하는 반면, Llama3.1 8b모델은 chat form에 따르지 않아도 올바른 출력을 하였다.무료모델들은 생각보다 쉽지않다. 모델을 다운을 받고, 쥬피터들이 적용되지 않을수도 있다. 예를들어, 윈도우 환경에서 AttributeError: module 'signal' has no attribute 'SIGALRM'오류가 발생한다면, WSL환경, 리눅스 환경에서만 가능한 모델들이 있다. 이것은 Ollama로 해결할수도 있지만, 모델이 올라가있지 않거나, gguf파일이 없다면 리눅스 환경을 고려할수밖에 없다.또한 입출력 토큰의 개수가 상대적으로 제한적이다. 가장 큰 장점은 보안적인 장점과, 과금이 들지 않는다는 것이다.Ollama를 통해 모델을 불러올수도 있는데, 올라마 깃허브에 따르면, 7B 파라미터 모델에는 적어도 8기가의 램의, 13B 모델에는 16기가의 램이, 33B모델에는 32기가 램이, 70B 모델에는 64기가의 램이 필요하다고 한다.https://github.com/ollama/ollamahttps://ollama.com/library/llama2:70b또한 보안성 문제에도 훌륭하다. 약 10B 정도 크기의 모델을 쓰는데 적합하다.모델을 고려할 때, bnksysyanolja-eeve-korean-instruct-10.8b모델이나gemma2를 고려할 수 있다.# 시간적 비용문제.시간적 비용문제의 대표적인 문제로는 크게 3개를 뽑았다.1. 임베딩 모델입력 데이터를 벡터로 변환하는 과정에서 시간이 많이 소요될 수 있다. 특히, 임베딩 모델을 사용할 때 local모델을 이용하는경우 시간이 오래걸리는 문제를 확인할 수 있다. 대량의 데이터를 처리할 경우, 임베딩 모델의 속도가 전체 시스템의 성능에 큰 영향을 미칠 수 있다특히 이 local model을 2. 벡터데이터베이스대표적으로 ChromaDB나, FAISS를 가지고 vector store를 만들 때, 시간이 오래 걸리는 경우를 확인할 수 있다.벡터 데이터베이스에서 유사한 벡터를 검색하는 과정도 시간이 걸릴 수 있다. 데이터베이스의 크기와 사용되는 인덱싱 기술, 검색 알고리즘의 효율성에 따라 검색 시간이 달라질 수 있다. 대규모 데이터셋에서는 특히나 검색 시간이 증가할 수 있다.시간을 줄이는 방법에는 과금이 들긴하지만 파인콘이 효율적이다.3. llm 모델임베딩 모델과 local모델을 이용할 경우, API를 이용했을때 보다 RAG를 출력하는데 시간이 더욱 오래걸리는것을 확인할 수 있다.특히 모델의 크기(inteligence, parameter등)와 복잡성이 커질경우 문장생성에 더많은 시간이 소요된다.4. retriever, 검색알고리즘모델에서 정보를 검색하기 위해 사용하는 알고리즘의 효율성에 따라 시간이 달라질 수 있다. 비효율적인 검색 방법이나 복잡한 쿼리는 결과를 얻는 데 더 많은 시간을 필요로 할 수 있다.5. 데이터의크기사용하고 있는 데이터베이스가 매우 크면, 적절한 정보를 검색하는 데 시간이 더 걸릴 수 있다. 데이터베이스의 크기가 클수록 검색 알고리즘이 더 많은 데이터를 처리해야 하므로 시간이 소요된다.해결방법은 API를 이용하거나, 하드웨어를 업그레이드 하는것 이다.------------이 외에도 조금 더 나은 성능이나, 원하는 형태를 원한다면 outputparser, retriever, chunk_size들을 고려할 수 있다. Ⅴ. 결론 및 향후연구REFERENCES[1] Gojka Roglic, “WHO Global report in diabetes: A summary”, International Journal of Noncommunicable Diseases, vol.1, pp. 3-8,  Jun. 2016. DOI: 10.4103/2468-8827.184853[2] S. G. Kim, D. S. Choi, “The present State of Diabetes Mellitus in Korea”, Journal of the Korean Medical Association, vol.51, pp. 791-798, Sep. 2008. DOI: https://doi.org/10.5124/jkma.2008.51.9.791[3] Kaggle Data set, Diabetes Prediction Dataset, 2022.  [Internet].Available:https://www.kaggle.com/datasets/alexteboul/diabetes-health-indicators-dataset?resource=download[4] J. W. Smith, J. E. Everhart, W. C. Dickson, W. C. Knowler , R. S. Johannes, “Using the ADAP Learning Algorithm to Forecast the Onset of Diabetes Mellitus,” Proceedings Annual Symposium on Computer Application in Medical Care(Proc Annu Symp Comput Appl Med Care), pp. 261-265, Nov. 1988.[5] D. Bhatt, C. Patel, H. Talsania, J. Patel, R. Vaghela, S. Pandya, K. Modi, H. Ghayvat, “CNN Variants for Computer Vision: History, Architecture, Application, Challenges and Future Scope,” Electronics, vol.10, Oct, 2021. DOI:10.3390/electronics10202470[6] Y. Wang, J. Liu, J. Mišić, V. B. Mišić, S. Lv, X. Chang, “Assessing Optimizer Impact on DNN Model Sensitivity to Adversarial Examples,” IEEE Access, vol.7, pp. 152766-152776, 2019. DOI:10.1109/access.2019.2948658 [7] X. Li, G. A. Ng, F. S. Schlindwein, \"Convolutional and recurrent neural networks for early detection of sepsis using hourly physiological data from patients in intensive care unit\", 2019 Computing in Cardiology (CinC), IEEE, 2019. DOI: 10.22489/CinC.2019.054[8] S. Streukens, S. Leroi Werelds, \"Multicollinearity: an overview and introduction of ridge PLS-SEM estimation\", Partial Least Squares Path Modeling: Basic Concepts, Methodological Issues and Applications, pp.183-207, 2023 DOI:10.1007/978-3-031-37772-3_7[9] N. Shrestha, “Detecting multicollinearity in regression analysis”, American Journal of Applied Mathematics and Statistics, vol.8, no.2, pp.39-42, 2020. DOI:10.12691/ajams-8-2-1[10] A. Ogunpola, F. Saeed, S. Basurra, A. M. Albarrak, S. N. Qasem, \"Machine learning-based predictive models for detection of cardiovascular diseases\", Diagnostics, vol.14, 2024. DOI:10.3390/diagnostics14020144김재호(Jae-Ho Kim)수원대학교 컴퓨터학부 학사 수원대학교 컴퓨터학부 석사과정 ※관심분야 : 인공지능김장영(Jang-Young Kim)연세대학교 컴퓨터과학 공학사Pennsylvania State Univ. 공학석사State University of New York 공학박사University of South Carolina 교수수원대학교 컴퓨터학부 교수※관심분야 : Big data, AI, Cloud computing, Networks\n"
     ]
    }
   ],
   "source": [
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-kr-gLkynrUQ-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
