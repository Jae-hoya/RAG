{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 디렉토리 로더\n",
    "\n",
    "- 와일드카드 패턴을 포함하여 파일 시스템에서 로드하는 방법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install python-magic-bin\n",
    "#pip install limagic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The MIME type of '..\\\\data\\\\SPRI_AI_Brief_2023년12월호_F.pdf' is \"cannot open `..\\\\data\\\\SPRI_AI_Brief_2023\\\\353\\\\205\\\\20412\\\\354\\\\233\\\\224\\\\355\\\\230\\\\270_F.pdf' (Illegal byte sequence)\". This file type is not currently supported in unstructured.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "\n",
    "# 디렉토리 로더 초기화\n",
    "loader = DirectoryLoader(\"../data\", glob=\"**/*.pdf\")\n",
    "# 문서 로드\n",
    "docs = loader.load()\n",
    "# 문서 개수 계산\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# loader_cls 변경\n",
    "\n",
    "loader_cls 는 기본 값으로 UnstructuredLoader 클래스를 사용합니다.\n",
    "\n",
    "로더를 사용자 정의하려면 loader_cls kwarg에 로더 클래스를 지정하세요.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "# loader_cls 를 TextLoader 로 변경합니다.\n",
    "loader = DirectoryLoader(\"../\", glob=\"**/*.md\", loader_cls=TextLoader)\n",
    "\n",
    "# 문서 로드\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': '..\\\\5.LlamaParseLoader.py'}, page_content='import os\\nfrom typing import Iterator\\nfrom typing import Any, Dict, List, Optional\\nfrom langchain_core.document_loaders import BaseLoader\\nfrom langchain_core.documents import Document\\nfrom llama_parse import LlamaParse\\nfrom llama_index.core import SimpleDirectoryReader\\nimport nest_asyncio\\n\\nnest_asyncio.apply()\\n\\n\\nclass LlamaParseLoader(BaseLoader):\\n    \"\"\"파일을 한 줄씩 읽어오는 문서 로더의 예시입니다.\"\"\"\\n\\n    def __init__(self, file_paths: List[str], parsing_instructions=\"\") -> None:\\n        \"\"\"로더를 파일 경로와 함께 초기화합니다.\\n        Args:\\n            file_paths: 로드할 파일의 경로입니다.\\n        \"\"\"\\n        # LlamaParse 설정\\n        parser = LlamaParse(\\n            # api_key=\"llx-...\",  # API 키 (환경 변수 LLAMA_CLOUD_API_KEY에 저장 가능)\\n            result_type=\"markdown\",  # 결과 타입: \"markdown\" 또는 \"text\"\\n            num_workers=4,  # 여러 파일 처리 시 API 호출 분할 수\\n            verbose=True,\\n            language=\"ko\",  # 언어 설정 (기본값: \\'en\\')\\n            invalidate_cache=True,\\n            skip_diagonal_text=True,\\n            use_vendor_multimodal_model=True,\\n            vendor_multimodal_model_name=\"openai-gpt4o\",\\n            vendor_multimodal_api_key=os.environ.get(\"OPENAI_API_KEY\"),\\n            parsing_instruction=parsing_instructions,\\n        )\\n\\n        file_extractor = {\".pdf\": parser}\\n\\n        self.document_reader = SimpleDirectoryReader(\\n            input_files=file_paths,\\n            file_extractor=file_extractor,\\n        )\\n\\n    def lazy_load(self) -> Iterator[Document]:  # <-- 인자를 받지 않습니다\\n        \"\"\"파일을 한 줄씩 읽어오는 지연 로더입니다.\\n\\n        지연 로드 메소드를 구현할 때는, 문서를 하나씩 생성하여 반환하는 제너레이터를 사용해야 합니다.\\n        \"\"\"\\n        documents = self.document_reader.load_data()\\n        langchain_documents = [doc.to_langchain_format() for doc in documents]\\n        return langchain_documents\\n'),\n",
       " Document(metadata={'source': '..\\\\streamlit_practice.py'}, page_content='import streamlit as st\\n\\nfrom langchain_core.messages.chat import ChatMessage\\nfrom langchain_openai import ChatOpenAI\\nfrom langchain_core.prompts import ChatPromptTemplate\\nfrom langchain_core.output_parsers import StrOutputParser\\nfrom langchain_teddynote.prompts import load_prompt\\nfrom langchain import hub\\n\\nfrom dotenv import load_dotenv\\n\\nload_dotenv()\\n\\nst.title(\"방이의 chat gpt test\")\\n\\n# message가 없으면 빈 list 생성\\nif \"messages\" not in st.sesstion_state:\\n    st.session_state[\"messages\"] = []\\n\\nwith st.sidebar:\\n    clear_btn = st.button(\"대화초기화\")\\n'),\n",
       " Document(metadata={'source': '..\\\\19.Streamlit\\\\01-ChaptGPT\\\\main.py'}, page_content='import streamlit as st\\nimport os\\nfrom dotenv import load_dotenv\\nfrom langchain_community.document_loaders import PDFPlumberLoader\\nfrom langchain_core.messages import ChatMessage\\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\\nfrom langchain_openai import OpenAIEmbeddings, ChatOpenAI\\nfrom langchain_community.vectorstores import FAISS\\nfrom langchain_teddynote.prompts import load_prompt\\nfrom langchain_core.runnables import RunnablePassthrough\\nfrom langchain_core.output_parsers import StrOutputParser\\n\\n\\nload_dotenv()\\n\\nif not os.path.exists(\"cache\"):\\n    os.mkdir(\"cache\")\\n\\nif not os.path.exists(\"cache/files\"):\\n    os.mkdir(\"cache/fiels\")\\n\\nif not os.path.exists(\"cache/embeddings\"):\\n    os.mkdir(\"cache/embeddings\")\\n\\n\\n# 대화 기록 저장을 위한 session_state\\ndef print_messages():\\n    for chat_message in st.session_state[\"messages\"]:\\n        st.chat_message(chat_message.role).write(chat_message.content)\\n\\n\\ndef add_messages(role, content):\\n    st.session_state[\"messages\"].append(ChatMessage(role=role, content=content))\\n\\n\\nst.title(\"pdf기반 Q&A💬\")\\n\\nwith st.sidebar:\\n    # 초기화 버튼\\n    clear_btn = st.button(\"대화 초기화\")\\n\\n    # 파일 업로드\\n    uploaded_file = st.file_uploader(\"파일 업로드\", tpye=\"pdf\")\\n\\n    # 모델 메뉴 선택\\n    selected_model = st.selectbox(\\n        \"모델선택\", [\"gpt-4o-mini\", \"gpt-4o\", \"gpt-3.5-turbo\"]\\n    )\\n\\n\\n# 파일 업로드 및 인덱싱\\n@st.cache_resource(show_spinner=\"파일이 업로드중입니다\")\\ndef embed_file(file):\\n    file_content = file.read()\\n    file_path = f\"cache/files/{file.name}\"\\n    with open(file_path, \"wb\") as f:\\n        f.write(file_content)\\n\\n    loader = PDFPlumberLoader(file_path)\\n    docs = loader.load()\\n\\n    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\\n    documents = text_splitter.split_documents(docs)\\n\\n    embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\\n\\n    vector_store = FAISS.from_documents(documents, embeddings)\\n    retriever = vector_store.as_retriever()\\n    return retriever\\n\\n\\n# Create Chain\\ndef create_chain(retriever, model_name=\"gpt-4o-mini\"):\\n    prompt = load_prompt(\"prompts/pdf-rag.yaml\", encoding=\"utf-8\")\\n\\n    llm = ChatOpenAI(model=model_name)\\n    chain = (\\n        {\"context\": retriever, \"question\": RunnablePassthrough}\\n        | prompt\\n        | llm\\n        | StrOutputParser()\\n    )\\n    return chain\\n\\n\\n#######---------------------\\n\\n# 파일이 업로드 되면\\nif uploaded_file:\\n    retriever = embed_file(uploaded_file)\\n    chain = create_chain(retriever, model_name=selected_model)\\n    # chain 객체를 st.session_state[\"chain\"]에 저장\\n    st.session_state[\"chain\"] = chain\\n\\n# 대화 초기화 버튼을 누르면\\nif clear_btn:\\n    st.session_state[\"messages\"] = []\\n\\nprint_messages()\\n\\n# 사용자의 입력\\nuser_input = st.chat_input(\"궁금한 내용을 물어보세요.\")\\n\\n# 경고메세지를 위함\\nwarning_msg = st.empty()\\n\\nif user_input:\\n    # 체인생성\\n    chain = st.session_state[\"chain\"]\\n\\n    if chain is not None:\\n        # 사용자의 입력\\n        st.chat_message(\"user\").write(user_input)\\n\\n        # 스트리밍 호출\\n        response = chain.stream(user_input)\\n\\n        with st.chat_message(\"assistant\"):\\n            container = st.empty()\\n\\n            ai_answer = \"\"\\n            for token in response:\\n                ai_answer += token\\n                container.markdown(ai_answer)\\n\\n        add_messages(\"user\", user_input)\\n        add_messages(\"assistant\", ai_answer)\\n\\n    else:\\n        warning_msg.error(\"파일을 업로드 해주세요\")\\n\\n\\n# ##############\\n# if uploaded_file:\\n#     retriever = embed_file(uploaded_file)\\n#     chain = create_chain(retriever, model=selected_model)\\n#     st.session_state[\"messages\"] = chain\\n\\n# if clear_btn:\\n#     st.session_state[\"message\"] = []\\n\\n# print_messages()\\n\\n# user_input = st.chat_input(\"궁금한 내용을 물어보세요.\")\\n\\n# warning_msg = st.emplty()\\n\\n# if user_input:\\n#     chain = st.session_state[\"chain\"]\\n\\n#     if chain is not None:\\n#         # 사용자 입력\\n#         st.chat_message(\"user\").write(user_input)\\n#         # 스트리밍 호출\\n#         response = chain.stream(user_input)\\n\\n#         with st.chat_message(\"assistant\"):\\n#             container = st.empty()\\n\\n#             ai_answer = \"\"\\n#             for token in response:\\n#                 ai_answer += token\\n#                 container.markdown(ai_answer)\\n\\n#         add_messages(\"user\", user_input)\\n#         add_messages(\"assistant\", ai_answer)\\n#     else:\\n#         warning_msg.error(\"파일을 업로드 해주세요.\")\\n# ##############\\n\\n# if uploaded_file:\\n#     retriever = embed_file(uploaded_file)\\n#     chain = create_chain(retriever, model=selected_model)\\n#     st.session_state[\"messages\"] = chain\\n\\n# if clear_btn:\\n#     st.session_state[\"meesages\"] = []\\n\\n# print_messages()\\n\\n# user_input = st.chat_input(\"궁금하 내용\")\\n# warning_msg = st.empty()\\n\\n# if user_input:\\n#     chain = st.session_state(\"chain\")\\n#     if chain is not None:\\n#         st.chat_message(\"user\").write(user_input)\\n\\n#         response = chain.stream(\"user_input\")\\n\\n#         with st.chat_message(\"assistant\"):\\n#             container = st.empty()\\n\\n#             ai_answer = \"\"\\n#             for token in response:\\n#                 ai_answer += token\\n#                 container.markdown(ai_answer)\\n\\n#         add_messages(\"user\", user_input)\\n#         add_messages(\"assistant\", ai_answer)\\n'),\n",
       " Document(metadata={'source': '..\\\\19.Streamlit\\\\01-ChaptGPT\\\\new_main.py'}, page_content='import streamlit as st\\nfrom langchain_core.output_parsers import StrOutputParser\\nfrom langchain_core.messages import ChatMessage\\nfrom langchain_core.prompts import PromptTemplate\\nfrom langchain_openai import ChatOpenAI\\nfrom langchain_core.prompts import load_prompt\\n\\n\\nst.set_page_config(page_title=\"나만의 ChatGPT 💬\", page_icon=\"💬\")\\nst.title(\"나만의 ChatGPT 💬\")\\n\\n# st.session_state는 Streamlit에서 세션별로 상태를 관리할 수 있도록 하는 객체입니다.  session_state는 세션별로 데이터를 유지한다.\\nif \"messages\" not in st.session_state:\\n    st.session_state[\"messages\"] = []\\n\\n\\n# 모든 채팅의 히스토리를 화면에 출력. st.chat_message는 Streamlit에서 채팅 형식의 메시지를 화면에 출력하는 데 사용\\ndef print_history():\\n    for msg in st.session_state[\"messages\"]:\\n        st.chat_message(msg.role).write(msg.content)\\n\\n\\n# 새로운 채팅 메세지를 st.session_state[\\'messages]에 추가\\ndef add_history(role, content):\\n    st.session_state[\"messages\"].append(ChatMessage(role=role, content=content))\\n\\n\\n# 체인을 생성합니다.\\ndef create_chain(prompt, model):\\n    chain = prompt | ChatOpenAI(model_name=model) | StrOutputParser()\\n    return chain\\n\\n\\nwith st.sidebar:\\n    clear_btn = st.button(\"대화내용 초기화\")\\n    tab1, tab2 = st.tabs([\"프롬프트\", \"프리셋\"])\\n    prompt = \"\"\"당신은 친절한 AI 어시스턴트 입니다. 사용자의 질문에 간결하게 답변해 주세요.\"\"\"\\n    user_text_prompt = tab1.text_area(\"프롬프트\", value=prompt)\\n    user_text_apply_btn = tab1.button(\"프롬프트 적용\", key=\"apply1\")\\n    if user_text_apply_btn:\\n        tab1.markdown(f\"✅ 프롬프트가 적용되었습니다\")\\n        prompt_template = user_text_prompt + \"\\\\n\\\\n#Question:\\\\n{question}\\\\n\\\\n#Answer:\"\\n        prompt = PromptTemplate.from_template(prompt_template)\\n        st.session_state[\"chain\"] = create_chain(prompt, \"gpt-4o-mini\")\\n\\n    user_selected_prompt = tab2.selectbox(\"프리셋 선택\", [\"sns\", \"번역\", \"요약\"])\\n    user_selected_apply_btn = tab2.button(\"프롬프트 적용\", key=\"apply2\")\\n    if user_selected_apply_btn:\\n        tab2.markdown(f\"✅ 프롬프트가 적용되었습니다\")\\n        prompt = load_prompt(f\"prompts/{user_selected_prompt}.yaml\", encoding=\"utf8\")\\n        st.session_state[\"chain\"] = create_chain(prompt, \"gpt-4o-mini\")\\n\\nif clear_btn:\\n    retriever = st.session_state[\"messages\"].clear()\\n\\nprint_history()\\n\\n\\nif \"chain\" not in st.session_state:\\n    # user_prompt\\n    prompt_template = user_text_prompt + \"\\\\n\\\\n#Question:\\\\n{question}\\\\n\\\\n#Answer:\"\\n    prompt = PromptTemplate.from_template(prompt_template)\\n    st.session_state[\"chain\"] = create_chain(prompt, \"gpt-4o-mini\")\\n\\nif user_input := st.chat_input():\\n    add_history(\"user\", user_input)\\n    st.chat_message(\"user\").write(user_input)\\n    with st.chat_message(\"assistant\"):\\n        chat_container = st.empty()\\n\\n        stream_response = st.session_state[\"chain\"].stream(\\n            {\"question\": user_input}\\n        )  # 문서에 대한 질의\\n        ai_answer = \"\"\\n        for chunk in stream_response:\\n            ai_answer += chunk\\n            chat_container.markdown(ai_answer)\\n        add_history(\"ai\", ai_answer)\\n'),\n",
       " Document(metadata={'source': '..\\\\19.Streamlit\\\\02-이메일 프로젝트-PudanticOuptputparser♥\\\\main.py'}, page_content='import os\\nfrom dotenv import load_dotenv\\nimport streamlit as st\\nfrom langchain_core.messages.chat import ChatMessage\\nfrom langchain_core.prompts import ChatPromptTemplate, PromptTemplate\\nfrom langchain_openai import ChatOpenAI\\nfrom langchain_core.output_parsers import StrOutputParser\\nfrom langchain_teddynote.prompts import load_prompt\\nfrom langchain_core.pydantic_v1 import BaseModel, Field\\nfrom langchain_core.output_parsers import PydanticOutputParser\\nfrom langchain_community.utilities import SerpAPIWrapper\\n\\n# API KEY 정보로드\\nload_dotenv()\\n# 검색을 위한 api_key\\nos.environ[\"SERPAPI_API_KEY\"] = (\\n    \"9020cdf9235a65b087f324a802b283ff67ac56cf8ddf0ab5bc8522c284d4a573\"\\n)\\n\\nst.title(\"Email 요약기💬\")\\n\\n\\n# 처음 1번만 실행하기 위한 코드\\nif \"messages\" not in st.session_state:\\n    # 대화기록을 저장하기 위한 용도로 생성한다.\\n    st.session_state[\"messages\"] = []\\n\\n# 사이드바 생성\\nwith st.sidebar:\\n    # 초기화 버튼 생성\\n    clear_btn = st.button(\"대화 초기화\")\\n\\n\\n# 이전 대화를 출력\\ndef print_messages():\\n    for chat_message in st.session_state[\"messages\"]:\\n        st.chat_message(chat_message.role).write(chat_message.content)\\n\\n\\n# 새로운 메시지를 추가\\ndef add_message(role, message):\\n    st.session_state[\"messages\"].append(ChatMessage(role=role, content=message))\\n\\n\\n# pydantic. 이메일 본문에서 주요 엔티티 추출\\nclass EmailSummary(BaseModel):\\n    person: str = Field(description=\"메일을 보낸 사람\")\\n    email: str = Field(description=\"메일을 보낸 사람의 이메일 주소\")\\n    subject: str = Field(description=\"메일 제목\")\\n    summary: str = Field(description=\"메일 본문을 요약한 텍스트\")\\n    date: str = Field(description=\"메일 본문에 언급된 미팅 날짜와 시간\")\\n    company: str = Field(description=\"메일을 보낸 사람의 회사정보\")\\n\\n\\n# pydantic 체인 생성\\ndef create_email_parsing_chain():\\n\\n    llm = ChatOpenAI(model=\"gpt-4o-mini\")\\n\\n    # PydanticOutputParser 생성 - prompt에 get_format_instruction을 넣어주기 위해서.\\n    output_parser = PydanticOutputParser(pydantic_object=EmailSummary)\\n\\n    # PROMPT 생성\\n    template = \"\"\"\\n    You are a helpful assistant. Please answer the following questions in KOREAN.\\n\\n    #Question:\\n    다음의 이메일 내용 중에서 주요 내용을 추출해 주세요.\\n\\n    #Email Converation:\\n    {email_conversation}\\n\\n    #Format:\\n    {format}\\n    \"\"\"\\n\\n    prompt = PromptTemplate.from_template(template)\\n    prompt = prompt.partial(format=output_parser.get_format_instructions())\\n\\n    chain = prompt | llm | output_parser\\n\\n    return chain\\n\\n\\n# 두번째 체인\\ndef create_report_chain():\\n\\n    llm = ChatOpenAI(model=\"gpt-4o-mini\")\\n\\n    prompt = load_prompt(\"prompts/email.yaml\", encoding=\"utf-8\")\\n\\n    # 출력 파서\\n    output_parser = StrOutputParser()\\n\\n    # 체인 생성\\n    chain = prompt | llm | output_parser\\n    return chain\\n\\n\\n# 초기화 버튼이 눌리면...\\nif clear_btn:\\n    st.session_state[\"messages\"] = []\\n\\n# 이전 대화 기록 출력\\nprint_messages()\\n\\n# 사용자의 입력\\nuser_input = st.chat_input(\"궁금한 내용을 물어보세요!\")\\n\\n# 만약에 사용자 입력이 들어오면...\\nif user_input:\\n    # 사용자의 입력\\n    st.chat_message(\"user\").write(user_input)\\n\\n    # 1)email을 파싱하는 chain 을 생성 및 실행\\n    email_chain = create_email_parsing_chain()\\n    # email에서 주요 정보를 추출하는 체인\\n    answer = email_chain.invoke({\"email_conversation\": user_input})\\n\\n    # 2) 보낸 사람의 추가 정보 수집(검색)\\n    params = {\\n        \"engine\": \"google\",\\n        \"gl\": \"kr\",\\n        \"hl\": \"ko\",\\n        \"num\": \"3\",\\n    }  # 검색 파라미터 hl은 google ui language, gl은 country\\n    search = SerpAPIWrapper(params=params)  # 검색 객체\\n    search_query = f\"{answer.person} {answer.company} {answer.email}\"  # 검색 query\\n    search_result = search.run(search_query)  # 검색 query실행\\n    search_result = eval(search_result)  # 리스트 형태로 반환\\n    search_result_string = \"\\\\n\".join(\\n        search_result\\n    )  # 검색 결과를 스트링으로 만들어준다.\\n\\n    # 3) 이메일 요약 리포트 생성. 검색한 결과를 가지고 정리를 해준다.\\n    report_chain = create_report_chain()\\n    report_chain_input = {\\n        \"sender\": answer.person,\\n        \"additional_information\": search_result_string,\\n        \"company\": answer.company,\\n        \"email\": answer.email,\\n        \"subject\": answer.subject,\\n        \"summary\": answer.summary,\\n        \"date\": answer.date,\\n    }\\n\\n    # 스트리밍 호출\\n    response = report_chain.stream(report_chain_input)\\n    with st.chat_message(\"assistant\"):\\n        # 빈 공간(컨테이너)을 만들어서, 여기에 토큰을 스트리밍 출력한다.\\n        container = st.empty()\\n\\n        ai_answer = \"\"\\n        for token in response:\\n            ai_answer += token\\n            container.markdown(ai_answer)\\n\\n    # 대화기록을 저장한다.\\n    add_message(\"user\", user_input)\\n    add_message(\"assistant\", ai_answer)\\n\\n# streamlit run .\\\\main.py\\n'),\n",
       " Document(metadata={'source': '..\\\\19.Streamlit\\\\03-모델활용 프로젝트♥\\\\main.py'}, page_content='import streamlit as st\\nfrom langchain_core.messages.chat import ChatMessage\\nfrom langchain_openai import ChatOpenAI\\nfrom langchain_core.output_parsers import StrOutputParser\\nfrom langchain_teddynote.prompts import load_prompt\\nfrom dotenv import load_dotenv\\nimport glob\\n\\n# API KEY 정보로드\\nload_dotenv()\\n\\nst.title(\"나만의 챗GPT💬\")\\n\\n\\n# 처음 1번만 실행하기 위한 코드\\nif \"messages\" not in st.session_state:\\n    # 대화기록을 저장하기 위한 용도로 생성한다.\\n    st.session_state[\"messages\"] = []\\n\\n# 사이드바 생성\\nwith st.sidebar:\\n    # 초기화 버튼 생성\\n    clear_btn = st.button(\"대화 초기화\")\\n\\n    prompt_files = glob.glob(\"prompts/*.yaml\")\\n    selected_prompt = st.selectbox(\"프롬프트를 선택해 주세요\", prompt_files, index=0)\\n    task_input = st.text_input(\"TASK 입력\", \"\")\\n\\n\\n# 이전 대화를 출력\\ndef print_messages():\\n    for chat_message in st.session_state[\"messages\"]:\\n        st.chat_message(chat_message.role).write(chat_message.content)\\n\\n\\n# 새로운 메시지를 추가\\ndef add_message(role, message):\\n    st.session_state[\"messages\"].append(ChatMessage(role=role, content=message))\\n\\n\\n# 체인 생성\\ndef create_chain(prompt_filepath, task=\"\"):\\n    # prompt 적용\\n    prompt = load_prompt(prompt_filepath, encoding=\"utf-8\")\\n\\n    # 추가 파라미터가 있으면 추가\\n    if task:\\n        prompt = prompt.partial(task=task)\\n\\n    # GPT\\n    llm = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)\\n\\n    # 출력 파서\\n    output_parser = StrOutputParser()\\n\\n    # 체인 생성\\n    chain = prompt | llm | output_parser\\n    return chain\\n\\n\\n# 초기화 버튼이 눌리면...\\nif clear_btn:\\n    st.session_state[\"messages\"] = []\\n\\n# 이전 대화 기록 출력\\nprint_messages()\\n\\n# 사용자의 입력\\nuser_input = st.chat_input(\"궁금한 내용을 물어보세요!\")\\n\\n# 만약에 사용자 입력이 들어오면...\\nif user_input:\\n    # 사용자의 입력\\n    st.chat_message(\"user\").write(user_input)\\n    # chain 을 생성\\n    chain = create_chain(selected_prompt, task=task_input)\\n\\n    # 스트리밍 호출\\n    response = chain.stream({\"question\": user_input})\\n    with st.chat_message(\"assistant\"):\\n        # 빈 공간(컨테이너)을 만들어서, 여기에 토큰을 스트리밍 출력한다.\\n        container = st.empty()\\n\\n        ai_answer = \"\"\\n        for token in response:\\n            ai_answer += token\\n            container.markdown(ai_answer)\\n\\n    # 대화기록을 저장한다.\\n    add_message(\"user\", user_input)\\n    add_message(\"assistant\", ai_answer)\\n'),\n",
       " Document(metadata={'source': '..\\\\19.Streamlit\\\\03-모델활용 프로젝트♥\\\\retriever.py'}, page_content='from langchain_community.document_loaders import PDFPlumberLoader\\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\\nfrom langchain_community.vectorstores import FAISS\\nfrom langchain_openai import ChatOpenAI, OpenAIEmbeddings\\n\\n\\n# Indexing\\ndef create_retriever(file_path):\\n    # 1단계: 문서 로드\\n    loader = PDFPlumberLoader(file_path)\\n    docs = loader.load()\\n\\n    # 2단계: 문서 분할\\n    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=50)\\n    documents = text_splitter.split_documents(docs)\\n\\n    # 3단계: 임베딩 생성\\n    embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\\n\\n    # 4단계: 벡터 스토어 생성\\n    vector_store = FAISS.from_documents(documents, embeddings)\\n\\n    # 5단계: 검색기(Retriever) 생성\\n    retriever = vector_store.as_retriever()\\n    return retriever\\n'),\n",
       " Document(metadata={'source': '..\\\\19.Streamlit\\\\03-모델활용 프로젝트♥\\\\pages\\\\01_Multi_PDF copy.py'}, page_content='import os\\nimport streamlit as st\\nfrom dotenv import load_dotenv\\nfrom langchain_teddynote import logging\\nfrom langchain_community.document_loaders import PDFPlumberLoader\\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\\nfrom langchain_community.vectorstores import FAISS\\nfrom langchain_openai import ChatOpenAI, OpenAIEmbeddings\\nfrom langchain_core.messages.chat import ChatMessage\\nfrom langchain_teddynote.prompts import load_prompt\\nfrom langchain_core.runnables import RunnablePassthrough\\nfrom langchain_core.output_parsers import StrOutputParser\\n\\n\\nload_dotenv()\\n\\nlogging.langsmith(\"[Project] PDF RAG\")\\n\\n# 캐시 디렉토리 생성\\nif not os.path.exists(\".cache\"):\\n    os.mkdir(\".cache\")\\n\\n# 파일 업로드 전용 폴더\\nif not os.path.exists(\".cache/files\"):\\n    os.mkdir(\".cache/files\")\\n\\nif not os.path.exists(\".cache/embeddings\"):\\n    os.mkdir(\".cache/embeddings\")\\n\\nst.title(\"PDF기반 QA💬\")\\n\\n# 처음 1번만 실행하기 위한 코드. - 대화기록을 저장하기 위한 코드로 생성한다.\\nif \"messages\" not in st.session_state:\\n    st.session_state[\"messages\"] = []\\n\\n# 아무런 파일을 업로드 하지 않는경우\\nif \"chain\" not in st.session_state:\\n    st.session_state[\"chain\"] = None\\n\\n# 사이드바 생성\\nwith st.sidebar:\\n    # 초기화 버튼 생성\\n    clear_btn = st.button(\"대화 초기화\")\\n\\n    # 파일 업로드\\n    uploaded_file = st.file_uploader(\"PDF파일 업로드\", type=[\"pdf\"])\\n\\n    # 모델 메뉴 선택\\n    selected_model = st.selectbox(\\n        \"model선택\", [\"gpt-4o-mini\", \"gpt-4o\", \"gpt-3.5-turbo\"]\\n    )\\n\\n\\n# 이전 대화를 츌력 - chat_message에는 role과 content가 들어가 있다.\\ndef print_messages():\\n    for chat_message in st.session_state[\"messages\"]:\\n        st.chat_message(chat_message.role).write(chat_message.content)  \\n\\n\\n# 새로운 메세지를 추가 - ChatMessage은 langchain_core.messages.chat에 있다.\\ndef add_messages(role, message):\\n    st.session_state[\"messages\"].append(ChatMessage(role=role, content=message))\\n\\n\\n# 파일을 캐시 저장(시간이 오래 걸리는 작업을 처리할 예정)\\n@st.cache_resource(show_spinner=\"업로드한 파일을 처리중입니다.\")\\ndef embed_file(file):\\n    # 업로드한 파일을 캐시 디렉토리에 저장\\n    file_content = file.read()\\n    file_path = f\"./.cache/files/{file.name}\"\\n    with open(file_path, \"wb\") as f:\\n        f.write(file_content)\\n\\n    # ------------------------------ indexing\\n    # 1단계: 문서 로드\\n    loader = PDFPlumberLoader(file_path)\\n    docs = loader.load()\\n\\n    # 2단계: 문서 분할\\n    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=50)\\n    documents = text_splitter.split_documents(docs)\\n\\n    # 3단계: 임베딩 생성\\n    embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\\n\\n    # 4단계: 벡터 스토어 생성\\n    vector_store = FAISS.from_documents(documents, embeddings)\\n\\n    # 5단계: 검색기(Retriever) 생성\\n    retriever = vector_store.as_retriever()\\n    return retriever\\n\\n\\n# 체인 생성\\ndef create_chain(retriever, model_name=\"gpt-4o-mini\"):\\n    # 6단계: 프롬프트 생성\\n    prompt = load_prompt(\"prompts/pdf-rag.yaml\", encoding=\"utf-8\")\\n\\n    # 7단계 언어모델 생성\\n    llm = ChatOpenAI(model=model_name, temperature=0)\\n\\n    # 8단계 체인 생성\\n    chain = (\\n        {\"context\": retriever, \"question\": RunnablePassthrough()}\\n        | prompt\\n        | llm\\n        | StrOutputParser()\\n    )\\n    return chain\\n\\n\\n# 파일이 업로드 되었을 때\\nif uploaded_file:\\n    # 파일 업로드 후 retriever생성. 시간이 오래걸릴 예정\\n    retriever = embed_file(uploaded_file)\\n    chain = create_chain(retriever, model_name=selected_model)\\n    st.session_state[\"chain\"] = chain\\n\\n# 초기화 버튼이 눌렸을 때\\nif clear_btn:\\n    st.session_state[\"messages\"] = []\\n\\n# 이전 대화기록 출력\\nprint_messages()\\n\\n# 사용자의 입력\\nuser_input = st.chat_input(\"궁금한 내용을 물어보세요.\")\\n\\n# 경고메세지를 띄우기 위한 빈 영역\\nwarning_msg = st.empty()\\n\\n# 만약 사용자의 입력이 들어오면,\\nif user_input:\\n    # 체인 생성\\n    chain = st.session_state[\"chain\"]\\n\\n    if chain is not None:\\n        # 사용자의 입력\\n        st.chat_message(\"user\").write(user_input)\\n        # 스트리밍 호출\\n        response = chain.stream(user_input)\\n        with st.chat_message(\"assistant\"):\\n            # 빈 컨테이너를 만들어서, 여기에 토큰을 스트리밍\\n            container = st.empty()\\n\\n            ai_answer = \"\"\\n            for token in response:\\n                ai_answer += token\\n                container.markdown(ai_answer)\\n\\n        # 대화기록을 저장\\n        add_messages(\"user\", user_input)\\n        add_messages(\"assistant\", ai_answer)\\n\\n    else:\\n        # 파일을업로드 하라는 경고 메세지\\n        warning_msg.error(\"파일을 업로드 해주세요\")\\n\\n# poetry shell로 가상환경 실행\\n# streamlit run PDF.py\\n'),\n",
       " Document(metadata={'source': '..\\\\19.Streamlit\\\\03-모델활용 프로젝트♥\\\\pages\\\\01_Multi_PDF.py'}, page_content='import os\\nimport streamlit as st\\nfrom dotenv import load_dotenv\\nfrom langchain_teddynote import logging\\nfrom langchain_community.document_loaders import PDFPlumberLoader\\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\\nfrom langchain_community.vectorstores import FAISS\\nfrom langchain_openai import ChatOpenAI, OpenAIEmbeddings\\nfrom langchain_core.messages.chat import ChatMessage\\nfrom langchain_teddynote.prompts import load_prompt\\nfrom langchain_core.runnables import RunnablePassthrough\\nfrom langchain_core.output_parsers import StrOutputParser\\n\\n\\nload_dotenv()\\n\\nlogging.langsmith(\"[Project] PDF RAG\")\\n\\n# 캐시 디렉토리 생성\\nif not os.path.exists(\".cache\"):\\n    os.mkdir(\".cache\")\\n\\n# 파일 업로드 전용 폴더\\nif not os.path.exists(\".cache/files\"):\\n    os.mkdir(\".cache/files\")\\n\\nif not os.path.exists(\".cache/embeddings\"):\\n    os.mkdir(\".cache/embeddings\")\\n\\nst.title(\"PDF기반 QA💬\")\\n\\n# 처음 1번만 실행하기 위한 코드. - 대화기록을 저장하기 위한 코드로 생성한다.\\nif \"messages\" not in st.session_state:\\n    st.session_state[\"messages\"] = []\\n\\n# 아무런 파일을 업로드 하지 않는경우\\nif \"chain\" not in st.session_state:\\n    st.session_state[\"chain\"] = None\\n\\n# 사이드바 생성\\nwith st.sidebar:\\n    # 초기화 버튼 생성\\n    clear_btn = st.button(\"대화 초기화\")\\n\\n    # 파일 업로드\\n    uploaded_files = st.file_uploader(\\n        \"PDF파일 업로드\", type=[\"pdf\"], accept_multiple_files=True\\n    )\\n\\n    # 모델 메뉴 선택\\n    selected_model = st.selectbox(\\n        \"model선택\", [\"gpt-4o-mini\", \"gpt-4o\", \"gpt-3.5-turbo\"]\\n    )\\n\\n\\n# 이전 대화를 츌력 - chat_message에는 role과 content가 들어가 있다.\\ndef print_messages():\\n    for chat_message in st.session_state[\"messages\"]:\\n        st.chat_message(chat_message.role).write(chat_message.content)\\n\\n\\n# 새로운 메세지를 추가 - ChatMessage은 langchain_core.messages.chat에 있다.\\ndef add_messages(role, message):\\n    st.session_state[\"messages\"].append(ChatMessage(role=role, content=message))\\n\\n\\n# 파일을 캐시 저장(시간이 오래 걸리는 작업을 처리할 예정)\\n@st.cache_resource(show_spinner=\"업로드한 파일을 처리중입니다.\")\\ndef embed_files(files):\\n\\n    all_documents = []\\n\\n    for file in files:\\n        # 업로드한 파일을 캐시 디렉토리에 저장\\n        file_content = file.read()\\n        file_path = f\"./.cache/files/{file.name}\"\\n        with open(file_path, \"wb\") as f:\\n            f.write(file_content)\\n\\n        # ------------------------------ indexing\\n        # 1단계: 문서 로드\\n        loader = PDFPlumberLoader(file_path)\\n        docs = loader.load()\\n\\n        # 2단계: 문서 분할\\n        text_splitter = RecursiveCharacterTextSplitter(\\n            chunk_size=2000, chunk_overlap=100\\n        )\\n        documents = text_splitter.split_documents(docs)\\n\\n        all_documents.extend(documents)\\n\\n    if not all_documents:\\n        st.error(\"PDF 파일에서 문서를 추출하지 못했습니다.\")\\n        return None\\n\\n    # 3단계: 임베딩 생성\\n    embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\\n\\n    # 4단계: 벡터 스토어 생성\\n    vector_store = FAISS.from_documents(all_documents, embeddings)\\n\\n    # 5단계: 검색기(Retriever) 생성\\n    retriever = vector_store.as_retriever()\\n    return retriever\\n\\n\\n# 체인 생성\\ndef create_chain(retriever, model_name=\"gpt-4o-mini\"):\\n    # 6단계: 프롬프트 생성\\n    prompt = load_prompt(\"prompts/pdf-rag.yaml\", encoding=\"utf-8\")\\n\\n    # 7단계 언어모델 생성\\n    llm = ChatOpenAI(model=model_name, temperature=0)\\n\\n    # 8단계 체인 생성\\n    chain = (\\n        {\"context\": retriever, \"question\": RunnablePassthrough()}\\n        | prompt\\n        | llm\\n        | StrOutputParser()\\n    )\\n    return chain\\n\\n\\n# 파일이 업로드 되었을 때\\nif uploaded_files:\\n    # 파일 업로드 후 retriever생성. 시간이 오래걸릴 예정\\n    retriever = embed_files(uploaded_files)\\n    chain = create_chain(retriever, model_name=selected_model)\\n    st.session_state[\"chain\"] = chain\\n\\n# 초기화 버튼이 눌렸을 때\\nif clear_btn:\\n    st.session_state[\"messages\"] = []\\n\\n# 이전 대화기록 출력\\nprint_messages()\\n\\n# 사용자의 입력\\nuser_input = st.chat_input(\"궁금한 내용을 물어보세요.\")\\n\\n# 경고메세지를 띄우기 위한 빈 영역\\nwarning_msg = st.empty()\\n\\n# 만약 사용자의 입력이 들어오면,\\nif user_input:\\n    # 체인 생성\\n    chain = st.session_state[\"chain\"]\\n\\n    if chain is not None:\\n        # 사용자의 입력\\n        st.chat_message(\"user\").write(user_input)\\n        # 스트리밍 호출\\n        response = chain.stream(user_input)\\n        with st.chat_message(\"assistant\"):\\n            # 빈 컨테이너를 만들어서, 여기에 토큰을 스트리밍\\n            container = st.empty()\\n\\n            ai_answer = \"\"\\n            for token in response:\\n                ai_answer += token\\n                container.markdown(ai_answer)\\n\\n        # 대화기록을 저장\\n        add_messages(\"user\", user_input)\\n        add_messages(\"assistant\", ai_answer)\\n\\n    else:\\n        # 파일을업로드 하라는 경고 메세지\\n        warning_msg.error(\"파일을 업로드 해주세요\")\\n\\n# poetry shell로 가상환경 실행\\n# streamlit run PDF.py\\n'),\n",
       " Document(metadata={'source': '..\\\\19.Streamlit\\\\03-모델활용 프로젝트♥\\\\pages\\\\01_PDF pratice.py'}, page_content='import os\\nimport streamlit as st\\nfrom dotenv import load_dotenv\\nfrom langchain_core.messages import ChatMessage\\n\\n\\nload_dotenv()\\n\\nif not os.path.exists(\".cache\"):\\n    os.mkdir(\"./cache\")\\n\\nif not os.path.exists(\".cache/files\"):\\n    os.mkdir(\"./cache/files\")\\n\\nif not os.path.exists(\".embeddings\"):\\n    os.mkdir(\"./embeddings\")\\n\\nif \"messages\" not in st.session_state:\\n    st.session_state[\"messages\"] = []\\n\\nif \"chain\" not in st.session_state:\\n    st.session_state[\"chain\"] = None\\n\\n\\ndef print_messages():\\n    for chat_message in st.session_state[\"messages\"]:\\n        st.chat_message(chat_message.role).write(chat_message.content)\\n\\ndef add_message(role, message):\\n    st.session_state[\"message\"].append(ChatMessage(role=role, content=message))\\n\\n@st.cache_resource(show_spinner= \"업로드한 파일을 처리해 주세요\")\\ndef embed_file(file):\\n    file_content = file.read()\\n    file_path = f\".cache/files/file\"'),\n",
       " Document(metadata={'source': '..\\\\19.Streamlit\\\\03-모델활용 프로젝트♥\\\\pages\\\\01_PDF.py'}, page_content='import os\\nimport streamlit as st\\nfrom dotenv import load_dotenv\\nfrom langchain_teddynote import logging\\nfrom langchain_community.document_loaders import PDFPlumberLoader\\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\\nfrom langchain_community.vectorstores import FAISS\\nfrom langchain_openai import ChatOpenAI, OpenAIEmbeddings\\nfrom langchain_core.messages.chat import ChatMessage\\nfrom langchain_teddynote.prompts import load_prompt\\nfrom langchain_core.runnables import RunnablePassthrough\\nfrom langchain_core.output_parsers import StrOutputParser\\n\\n\\nload_dotenv()\\n\\nlogging.langsmith(\"[Project] PDF RAG\")\\n\\n# 캐시 디렉토리 생성\\nif not os.path.exists(\".cache\"):\\n    os.mkdir(\".cache\")\\n\\n# 파일 업로드 전용 폴더\\nif not os.path.exists(\".cache/files\"):\\n    os.mkdir(\".cache/files\")\\n\\nif not os.path.exists(\".cache/embeddings\"):\\n    os.mkdir(\".cache/embeddings\")\\n\\nst.title(\"PDF기반 QA💬\")\\n\\n# 처음 1번만 실행하기 위한 코드. - 대화기록을 저장하기 위한 코드로 생성한다.\\nif \"messages\" not in st.session_state:\\n    st.session_state[\"messages\"] = []\\n\\n# 아무런 파일을 업로드 하지 않는경우\\nif \"chain\" not in st.session_state:\\n    st.session_state[\"chain\"] = None\\n\\n# 사이드바 생성\\nwith st.sidebar:\\n    # 초기화 버튼 생성\\n    clear_btn = st.button(\"대화 초기화\")\\n\\n    # 파일 업로드\\n    uploaded_file = st.file_uploader(\"PDF파일 업로드\", type=[\"pdf\"])\\n\\n    # 모델 메뉴 선택\\n    selected_model = st.selectbox(\\n        \"model선택\", [\"gpt-4o-mini\", \"gpt-4o\", \"gpt-3.5-turbo\"]\\n    )\\n\\n\\n# 이전 대화를 츌력 - chat_message에는 role과 content가 들어가 있다.\\ndef print_messages():\\n    for chat_message in st.session_state[\"messages\"]:\\n        st.chat_message(chat_message.role).write(chat_message.content)\\n\\n\\n# 새로운 메세지를 추가 - ChatMessage은 langchain_core.messages.chat에 있다.\\ndef add_messages(role, message):\\n    st.session_state[\"messages\"].append(ChatMessage(role=role, content=message))\\n\\n\\n# 파일을 캐시 저장(시간이 오래 걸리는 작업을 처리할 예정)\\n@st.cache_resource(show_spinner=\"업로드한 파일을 처리중입니다.\")\\ndef embed_file(file):\\n    # 업로드한 파일을 캐시 디렉토리에 저장\\n    file_content = file.read()\\n    file_path = f\"./.cache/files/{file.name}\"\\n    with open(file_path, \"wb\") as f:\\n        f.write(file_content)\\n\\n    # ------------------------------ indexing\\n    # 1단계: 문서 로드\\n    loader = PDFPlumberLoader(file_path)\\n    docs = loader.load()\\n\\n    # 2단계: 문서 분할\\n    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=50)\\n    documents = text_splitter.split_documents(docs)\\n\\n    # 3단계: 임베딩 생성\\n    embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\\n\\n    # 4단계: 벡터 스토어 생성\\n    vector_store = FAISS.from_documents(documents, embeddings)\\n\\n    # 5단계: 검색기(Retriever) 생성\\n    retriever = vector_store.as_retriever()\\n    return retriever\\n\\n\\n# 체인 생성\\ndef create_chain(retriever, model_name=\"gpt-4o-mini\"):\\n    # 6단계: 프롬프트 생성\\n    prompt = load_prompt(\"prompts/pdf-rag.yaml\", encoding=\"utf-8\")\\n\\n    # 7단계 언어모델 생성\\n    llm = ChatOpenAI(model=model_name, temperature=0)\\n\\n    # 8단계 체인 생성\\n    chain = (\\n        {\"context\": retriever, \"question\": RunnablePassthrough()}\\n        | prompt\\n        | llm\\n        | StrOutputParser()\\n    )\\n    return chain\\n\\n\\n# 파일이 업로드 되었을 때\\nif uploaded_file:\\n    # 파일 업로드 후 retriever생성. 시간이 오래걸릴 예정\\n    retriever = embed_file(uploaded_file)\\n    chain = create_chain(retriever, model_name=selected_model)\\n    st.session_state[\"chain\"] = chain\\n\\n# 초기화 버튼이 눌렸을 때\\nif clear_btn:\\n    st.session_state[\"messages\"] = []\\n\\n# 이전 대화기록 출력\\nprint_messages()\\n\\n# 사용자의 입력\\nuser_input = st.chat_input(\"궁금한 내용을 물어보세요.\")\\n\\n# 경고메세지를 띄우기 위한 빈 영역\\nwarning_msg = st.empty()\\n\\n# 만약 사용자의 입력이 들어오면,\\nif user_input:\\n    # 체인 생성\\n    chain = st.session_state[\"chain\"]\\n\\n    if chain is not None:\\n        # 사용자의 입력\\n        st.chat_message(\"user\").write(user_input)\\n        # 스트리밍 호출\\n        response = chain.stream(user_input)\\n        with st.chat_message(\"assistant\"):\\n            # 빈 컨테이너를 만들어서, 여기에 토큰을 스트리밍\\n            container = st.empty()\\n\\n            ai_answer = \"\"\\n            for token in response:\\n                ai_answer += token\\n                container.markdown(ai_answer)\\n\\n        # 대화기록을 저장\\n        add_messages(\"user\", user_input)\\n        add_messages(\"assistant\", ai_answer)\\n\\n    else:\\n        # 파일을업로드 하라는 경고 메세지\\n        warning_msg.error(\"파일을 업로드 해주세요\")\\n\\n# poetry shell로 가상환경 실행\\n# streamlit run PDF.py\\n'),\n",
       " Document(metadata={'source': '..\\\\19.Streamlit\\\\03-모델활용 프로젝트♥\\\\pages\\\\01_PDF_extract table.py'}, page_content='import os\\nimport streamlit as st\\nfrom dotenv import load_dotenv\\nfrom langchain_teddynote import logging\\nfrom langchain_community.document_loaders import PDFPlumberLoader\\nfrom langchain_text_splitters import (\\n    RecursiveCharacterTextSplitter,\\n    MarkdownTextSplitter,\\n)\\nfrom langchain_community.vectorstores import FAISS\\nfrom langchain_openai import ChatOpenAI, OpenAIEmbeddings\\nfrom langchain_core.messages.chat import ChatMessage\\nfrom langchain_teddynote.prompts import load_prompt\\nfrom langchain_core.runnables import RunnablePassthrough\\nfrom langchain_core.output_parsers import StrOutputParser\\n\\n\\nload_dotenv()\\n\\nlogging.langsmith(\"[Project] PDF RAG\")\\n\\n# 캐시 디렉토리 생성\\nif not os.path.exists(\".cache\"):\\n    os.mkdir(\".cache\")\\n\\n# 파일 업로드 전용 폴더\\nif not os.path.exists(\".cache/files\"):\\n    os.mkdir(\".cache/files\")\\n\\nif not os.path.exists(\".cache/embeddings\"):\\n    os.mkdir(\".cache/embeddings\")\\n\\nst.title(\"PDF기반 QA💬\")\\n\\n# 처음 1번만 실행하기 위한 코드. - 대화기록을 저장하기 위한 코드로 생성한다.\\nif \"messages\" not in st.session_state:\\n    st.session_state[\"messages\"] = []\\n\\n# 아무런 파일을 업로드 하지 않는경우\\nif \"chain\" not in st.session_state:\\n    st.session_state[\"chain\"] = None\\n\\n# 사이드바 생성\\nwith st.sidebar:\\n    # 초기화 버튼 생성\\n    clear_btn = st.button(\"대화 초기화\")\\n\\n    # 파일 업로드\\n    uploaded_file = st.file_uploader(\"PDF파일 업로드\", type=[\"pdf\"])\\n\\n    # 모델 메뉴 선택\\n    selected_model = st.selectbox(\\n        \"model선택\", [\"gpt-4o-mini\", \"gpt-4o\", \"gpt-3.5-turbo\"]\\n    )\\n\\n\\n# 이전 대화를 츌력 - chat_message에는 role과 content가 들어가 있다.\\ndef print_messages():\\n    for chat_message in st.session_state[\"messages\"]:\\n        st.chat_message(chat_message.role).write(chat_message.content)\\n\\n\\n# 새로운 메세지를 추가 - ChatMessage은 langchain_core.messages.chat에 있다.\\ndef add_messages(role, message):\\n    st.session_state[\"messages\"].append(ChatMessage(role=role, content=message))\\n\\n\\n# 파일을 캐시 저장(시간이 오래 걸리는 작업을 처리할 예정)\\n@st.cache_resource(show_spinner=\"업로드한 파일을 처리중입니다.\")\\ndef embed_file(file):\\n    # 업로드한 파일을 캐시 디렉토리에 저장\\n    file_content = file.read()\\n    file_path = f\"./.cache/files/{file.name}\"\\n    with open(file_path, \"wb\") as f:\\n        f.write(file_content)\\n\\n    # ------------------------------ indexing\\n    # 1단계: 문서 로드\\n    loader = PDFPlumberLoader(file_path)\\n    docs = loader.load()\\n\\n    # 2단계: 문서 분할 - 21 Markdown\\n    markdown_splitter = MarkdownTextSplitter(chunk_size=1000, chunk_overlap=50)\\n    markdown_docs = markdown_splitter.split_documents(docs)\\n\\n    # 2단계: 문서 분할 - 2 Recursive\\n    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=50)\\n    documents = text_splitter.split_documents(markdown_docs)\\n\\n    # 3단계: 임베딩 생성\\n    embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\\n\\n    # 4단계: 벡터 스토어 생성\\n    vector_store = FAISS.from_documents(documents, embeddings)\\n\\n    # 5단계: 검색기(Retriever) 생성\\n    retriever = vector_store.as_retriever()\\n    return retriever\\n\\n\\n# 체인 생성\\ndef create_chain(retriever, model_name=\"gpt-4o-mini\"):\\n    # 6단계: 프롬프트 생성\\n    prompt = load_prompt(\"prompts/pdf-rag.yaml\", encoding=\"utf-8\")\\n\\n    # 7단계 언어모델 생성\\n    llm = ChatOpenAI(model=model_name, temperature=0)\\n\\n    # 8단계 체인 생성\\n    chain = (\\n        {\"context\": retriever, \"question\": RunnablePassthrough()}\\n        | prompt\\n        | llm\\n        | StrOutputParser()\\n    )\\n    return chain\\n\\n\\n# 파일이 업로드 되었을 때\\nif uploaded_file:\\n    # 파일 업로드 후 retriever생성. 시간이 오래걸릴 예정\\n    retriever = embed_file(uploaded_file)\\n    chain = create_chain(retriever, model_name=selected_model)\\n    st.session_state[\"chain\"] = chain\\n\\n# 초기화 버튼이 눌렸을 때\\nif clear_btn:\\n    st.session_state[\"messages\"] = []\\n\\n# 이전 대화기록 출력\\nprint_messages()\\n\\n# 사용자의 입력\\nuser_input = st.chat_input(\"궁금한 내용을 물어보세요.\")\\n\\n# 경고메세지를 띄우기 위한 빈 영역\\nwarning_msg = st.empty()\\n\\n# 만약 사용자의 입력이 들어오면,\\nif user_input:\\n    # 체인 생성\\n    chain = st.session_state[\"chain\"]\\n\\n    if chain is not None:\\n        # 사용자의 입력\\n        st.chat_message(\"user\").write(user_input)\\n        # 스트리밍 호출\\n        response = chain.stream(user_input)\\n        with st.chat_message(\"assistant\"):\\n            # 빈 컨테이너를 만들어서, 여기에 토큰을 스트리밍\\n            container = st.empty()\\n\\n            ai_answer = \"\"\\n            for token in response:\\n                ai_answer += token\\n                container.markdown(ai_answer)\\n\\n        # 대화기록을 저장\\n        add_messages(\"user\", user_input)\\n        add_messages(\"assistant\", ai_answer)\\n\\n    else:\\n        # 파일을업로드 하라는 경고 메세지\\n        warning_msg.error(\"파일을 업로드 해주세요\")\\n\\n# poetry shell로 가상환경 실행\\n# streamlit run PDF.py\\n'),\n",
       " Document(metadata={'source': '..\\\\19.Streamlit\\\\03-모델활용 프로젝트♥\\\\pages\\\\02_Local_RAG.py'}, page_content='import os\\nimport streamlit as st\\nfrom dotenv import load_dotenv\\nfrom langchain_teddynote import logging\\nfrom langchain_community.document_loaders import PDFPlumberLoader\\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\\nfrom langchain_community.vectorstores import FAISS\\nfrom langchain_openai import ChatOpenAI, OpenAIEmbeddings\\nfrom langchain_community.chat_models import ChatOllama\\nfrom langchain_core.messages.chat import ChatMessage\\nfrom langchain_teddynote.prompts import load_prompt\\nfrom langchain_core.runnables import RunnablePassthrough\\nfrom langchain_core.output_parsers import StrOutputParser\\n\\nfrom retriever import create_retriever\\n\\nload_dotenv()\\n\\nlogging.langsmith(\"[Project] PDF RAG\")\\n\\n# 캐시 디렉토리 생성\\nif not os.path.exists(\".cache\"):\\n    os.mkdir(\".cache\")\\n\\n# 파일 업로드 전용 폴더\\nif not os.path.exists(\".cache/files\"):\\n    os.mkdir(\".cache/files\")\\n\\nif not os.path.exists(\".cache/embeddings\"):\\n    os.mkdir(\".cache/embeddings\")\\n\\nst.title(\"Local model 기반 RAG💬\")\\n\\n# 처음 1번만 실행하기 위한 코드. - 대화기록을 저장하기 위한 코드로 생성한다.\\nif \"messages\" not in st.session_state:\\n    st.session_state[\"messages\"] = []\\n\\n# 아무런 파일을 업로드 하지 않는경우\\nif \"chain\" not in st.session_state:\\n    st.session_state[\"chain\"] = None\\n\\n# 사이드바 생성\\nwith st.sidebar:\\n    # 초기화 버튼 생성\\n    clear_btn = st.button(\"대화 초기화\")\\n\\n    # 파일 업로드\\n    uploaded_file = st.file_uploader(\"PDF파일 업로드\", type=[\"pdf\"])\\n\\n    # 모델 메뉴 선택\\n    selected_model = st.selectbox(\\n        \"model선택\", [\"gpt-4o-mini\", \"Ollama-EEVE\", \"Ollama-Llama3.1\"]\\n    )\\n\\n\\n# 이전 대화를 츌력 - chat_message에는 role과 content가 들어가 있다.\\ndef print_messages():\\n    for chat_message in st.session_state[\"messages\"]:\\n        st.chat_message(chat_message.role).write(chat_message.content)\\n\\n\\n# 새로운 메세지를 추가 - ChatMessage은 langchain_core.messages.chat에 있다.\\ndef add_message(role, message):\\n    st.session_state[\"messages\"].append(ChatMessage(role=role, content=message))\\n\\n\\n# 파일을 캐시 저장(시간이 오래 걸리는 작업을 처리할 예정)\\n@st.cache_resource(show_spinner=\"업로드한 파일을 처리중입니다.\")\\ndef embed_file(file):\\n    # 업로드한 파일을 캐시 디렉토리에 저장\\n    file_content = file.read()\\n    file_path = f\"./.cache/files/{file.name}\"\\n    with open(file_path, \"wb\") as f:\\n        f.write(file_content)\\n\\n    # create_retriever를 retriever로부터 가져온다.\\n    return create_retriever(file_path)\\n\\n\\n# retriever할때, 메타데이터를제외하고, page_content만 불러오기 위함이다. meta data를 이해 못하는 경우가 있기 때문이다.\\ndef format_doc(document_list):\\n    return \"\\\\n\\\\n\".join([doc.page_content for doc in document_list])\\n\\n\\n# 체인 생성\\ndef create_chain(retriever, model_name=\"gpt-4o-mini\"):\\n    # 모델 이름이 gpt-4o-mini면,\\n    if model_name == \"gpt-4o-mini\":\\n        # 6단계: 프롬프트 생성\\n        prompt = load_prompt(\"prompts/pdf-rag.yaml\", encoding=\"utf-8\")\\n\\n        # 7단계 언어모델 생성\\n        llm = ChatOpenAI(model=model_name, temperature=0)\\n\\n    # xionic = ChatOpenAI(\\n    #     model_name=\"xionic-1-72b-20240610\",\\n    #     base_url=\"https://sionic.chat/v1/\",\\n    #     api_key=\"934c4bbc-c384-4bea-af82-1450d7f8128d\",\\n    # )\\n    # pdf-rag로 써도잘되네?\\n    elif model_name == \"Ollama-EEVE\":\\n        prompt = load_prompt(\"prompts/pdf-rag-ollama-EEVE.yaml\", encoding=\"utf-8\")\\n        llm = ChatOllama(model=\"EEVE-Korean-10.8b:latest\", temperature=0)\\n\\n    elif model_name == \"Ollama-Llama3.1\":\\n        prompt = load_prompt(\"prompts/pdf-rag.yaml\", encoding=\"utf-8\")\\n        llm = ChatOllama(model=\"llama3.1\", temperature=0)\\n\\n    # 8단계 체인 생성\\n    chain = (\\n        {\"context\": retriever | format_doc, \"question\": RunnablePassthrough()}\\n        | prompt\\n        | llm\\n        | StrOutputParser()\\n    )\\n    return chain\\n\\n\\n# 파일이 업로드 되었을 때\\nif uploaded_file:\\n    # 파일 업로드 후 retriever생성. 시간이 오래걸릴 예정\\n    retriever = embed_file(uploaded_file)\\n    chain = create_chain(retriever, model_name=selected_model)\\n    st.session_state[\"chain\"] = chain\\n\\n# 초기화 버튼이 눌렸을 때\\nif clear_btn:\\n    st.session_state[\"messages\"] = []\\n\\n# 이전 대화기록 출력\\nprint_messages()\\n\\n# 사용자의 입력\\nuser_input = st.chat_input(\"궁금한 내용을 물어보세요.\")\\n\\n# 경고메세지를 뜨우기 위한 빈 영역\\nwarning_msg = st.empty()\\n\\n# 만약 사용자의 입력이 들어오면,\\nif user_input:\\n    # 체인 생성\\n    chain = st.session_state[\"chain\"]\\n\\n    if chain is not None:\\n        # 사용자의 입력\\n        st.chat_message(\"user\").write(user_input)\\n        # 스트리밍 호출\\n        response = chain.stream(user_input)\\n        with st.chat_message(\"assistant\"):\\n            # 빈 컨테이너를 만들어서, 여기에 토큰을 스트리밍\\n            container = st.empty()\\n\\n            ai_answer = \"\"\\n            for token in response:\\n                ai_answer += token\\n                container.markdown(ai_answer)\\n\\n        # 대화기록을 저장\\n        add_message(\"user\", user_input)\\n        add_message(\"assistant\", ai_answer)\\n\\n    else:\\n        # 파일을업로드 하라는 경고 메세지\\n        warning_msg.error(\"파일을 업로드 해주세요\")\\n\\n# poetry shell로 가상환경 실행\\n# streamlit run .\\\\02.Local_lag.py\\n'),\n",
       " Document(metadata={'source': '..\\\\19.Streamlit\\\\03-모델활용 프로젝트♥\\\\pages\\\\03_Multi_Modal.py'}, page_content='import os\\nimport streamlit as st\\nfrom dotenv import load_dotenv\\nfrom langchain_teddynote import logging\\nfrom langchain_community.vectorstores import FAISS\\nfrom langchain_openai import ChatOpenAI, OpenAIEmbeddings\\nfrom langchain_core.messages.chat import ChatMessage\\nfrom langchain_teddynote.prompts import load_prompt\\nfrom langchain_teddynote.models import MultiModal\\nfrom langchain_teddynote.messages import stream_response\\n\\nfrom retriever import create_retriever\\n\\nload_dotenv()\\n\\nlogging.langsmith(\"[Project] 이미지 인식\")\\n\\n# 캐시 디렉토리 생성\\nif not os.path.exists(\".cache\"):\\n    os.mkdir(\".cache\")\\n\\n# 파일 업로드 전용 폴더\\nif not os.path.exists(\".cache/files\"):\\n    os.mkdir(\".cache/files\")\\n\\nif not os.path.exists(\".cache/embeddings\"):\\n    os.mkdir(\".cache/embeddings\")\\n\\nst.title(\"이미지 인식 기반 챗봇💬\")\\n\\n# 처음 1번만 실행하기 위한 코드. - 대화기록을 저장하기 위한 코드로 생성한다.\\nif \"messages\" not in st.session_state:\\n    st.session_state[\"messages\"] = []\\n\\n# 탭을 생성\\nmain_tab1, main_tab2 = st.tabs([\"이미지\", \"대화내용\"])\\n\\n\\n# 사이드바 생성\\nwith st.sidebar:\\n    # 초기화 버튼 생성\\n    clear_btn = st.button(\"대화 초기화\")\\n\\n    # 파일 업로드\\n    uploaded_file = st.file_uploader(\"이미지파일 업로드\", type=[\"jpg\", \"jpeg\", \"png\"])\\n\\n    # 모델 메뉴 선택\\n    selected_model = st.selectbox(\"model선택\", [\"gpt-4o-mini\", \"gpt-4o\"])\\n\\n    # 시스템 프롬프트 추가\\n    system_prompt = st.text_area(\\n        \"시스템 프롬프트\",\\n        \"당신은 표(재무제표)를 해석하는 금융 AI어시스턴트 입니다. \\\\n당신의 임무는 주어진 테이블 형식의 재무제표를 바탕으로 흥미로운 사실을 정리하여 친절하게 답변하는것 입니다.\",\\n        height=200,\\n    )\\n\\n\\n# 이전 대화를 츌력 - chat_message에는 role과 content가 들어가 있다.\\ndef print_messages():\\n    for chat_message in st.session_state[\"messages\"]:\\n        main_tab2.chat_message(chat_message.role).write(chat_message.content)\\n        # st.chat_message(chat_message.role).write(chat_message.content)\\n\\n\\n# 새로운 메세지를 추가 - ChatMessage은 langchain_core.messages.chat에 있다.\\ndef add_message(role, message):\\n    st.session_state[\"messages\"].append(ChatMessage(role=role, content=message))\\n\\n\\n# 이미지를 캐시 저장(시간이 오래 걸리는 작업을 처리할 예정)\\n@st.cache_resource(show_spinner=\"업로드한 이미지를 처리중입니다.\")\\ndef process_imagefile(file):\\n    # 업로드한 파일을 캐시 디렉토리에 저장\\n    file_content = file.read()\\n    file_path = f\"./.cache/files/{file.name}\"\\n\\n    with open(file_path, \"wb\") as f:\\n        f.write(file_content)\\n\\n    # create_retriever를 retriever로부터 가져온다.\\n    return file_path\\n\\n\\n# 이미지에 대한 대답 생성\\ndef generate_answer(\\n    image_filepath, system_prompt, user_prompt, model_name=\"gpt-4o-mini\"\\n):\\n    # model_name으로 넣어줘야 동적으로 사용가능.\\n    llm = ChatOpenAI(temperature=0, model=model_name)\\n\\n    # 멀티모댈 객체 생성\\n    multimodal = MultiModal(llm, system_prompt=system_prompt, user_prompt=user_prompt)\\n\\n    # 이미지 파일로 부터 질의(스트림 방식)\\n    answer = multimodal.stream(image_filepath)\\n    return answer\\n\\n\\n# 초기화 버튼이 눌렸을 때\\nif clear_btn:\\n    st.session_state[\"messages\"] = []\\n\\n# 이전 대화기록 출력\\nprint_messages()\\n\\n# 사용자의 입력\\nuser_input = st.chat_input(\"궁금한 내용을 물어보세요.\")\\n\\n# 경고메세지를 뜨우기 위한 빈 영역\\nwarning_msg = main_tab2.empty()\\n\\n# 이미지가 업로드가 된다면\\nif uploaded_file:\\n    # 이미지 파일을 처리\\n    image_filepath = process_imagefile(uploaded_file)\\n    main_tab1.image(image_filepath)\\n    # st.image(image_filepath)\\n\\n# 만약 사용자의 입력이 들어오면,\\nif user_input:\\n    # 파일이 업로드 되었는지 확인\\n    if uploaded_file:\\n        # process_imagefile 함수를 통해 file uploaded : 이미지 파일 처리\\n        image_filepath = process_imagefile(uploaded_file)\\n        # # 이미지 파일을 업로드 했다는 메세지 출력\\n        # warning_msg.success(\"이미지 파일을 업로드했습니다.\")\\n        # generate_answer 함수사용 : 답변요청\\n        response = generate_answer(\\n            image_filepath, system_prompt, user_input, selected_model\\n        )\\n\\n        # 사용자의 입력\\n        main_tab2.chat_message(\"user\").write(user_input)\\n\\n        with main_tab2.chat_message(\"assistant\"):\\n            # 빈 컨테이너를 만들어서, 여기에 토큰을 스트리밍\\n            container = st.empty()\\n\\n            ai_answer = \"\"\\n            for token in response:\\n                ai_answer += token.content\\n                container.markdown(ai_answer)\\n\\n        # 대화기록을 저장\\n        add_message(\"user\", user_input)\\n        add_message(\"assistant\", ai_answer)\\n\\n    else:\\n        # 이미지를 업로드 하라는 경고 메세지\\n        warning_msg.error(\"이미지를 업로드 해주세요\")\\n\\n# poetry shell로 가상환경 실행\\n# streamlit run .\\\\02.Local_lag.py\\n'),\n",
       " Document(metadata={'source': '..\\\\19.Streamlit\\\\03-모델활용 프로젝트♥\\\\pages\\\\04_Multi_turn.py'}, page_content='from urllib import response\\nfrom requests import session\\nimport streamlit as st\\nfrom langchain_core.messages.chat import ChatMessage\\nfrom langchain_openai import ChatOpenAI\\nfrom langchain_teddynote import logging\\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\\nfrom langchain_community.chat_message_histories import ChatMessageHistory\\nfrom langchain_core.chat_history import BaseChatMessageHistory\\nfrom langchain_core.runnables.history import RunnableWithMessageHistory\\nfrom langchain_openai import ChatOpenAI\\nfrom langchain_core.output_parsers import StrOutputParser\\n\\nfrom dotenv import load_dotenv\\nimport os\\n\\n\\n# API KEY 정보로드\\nload_dotenv()\\n\\n# 프로젝트 이름을 입력합니다.\\nlogging.langsmith(\"[Project] Multi Tuen 챗봇\")\\n\\n# 캐시 디렉토리 생성\\nif not os.path.exists(\".cache\"):\\n    os.mkdir(\".cache\")\\n\\n# 파일 업로드 전용 폴더\\nif not os.path.exists(\".cache/files\"):\\n    os.mkdir(\".cache/files\")\\n\\nif not os.path.exists(\".cache/embeddings\"):\\n    os.mkdir(\".cache/embeddings\")\\n\\nst.title(\"대화내용을 기억하는 챗봇 💬\")\\n\\n# 처음 1번만 실행하기 위한 코드\\nif \"messages\" not in st.session_state:\\n    # 대화기록을 저장하기 위한 용도로 생성한다.\\n    st.session_state[\"messages\"] = []\\n\\nif \"store\" not in st.session_state:\\n    st.session_state[\"store\"] = {}\\n\\n\\n# 사이드바 생성\\nwith st.sidebar:\\n    # 초기화 버튼 생성\\n    clear_btn = st.button(\"대화 초기화\")\\n\\n    # 모델 선택 메뉴\\n    selected_model = st.selectbox(\"LLM 선택\", [\"gpt-4o-mini\", \"gpt-4o\"], index=0)\\n\\n    # 세션 ID 를 지정하는 메뉴\\n    session_id = st.text_input(\"세션 ID를 입력하세요.\", \"abc123\")\\n\\n\\n# 이전 대화를 출력\\ndef print_messages():\\n    for chat_message in st.session_state[\"messages\"]:\\n        st.chat_message(chat_message.role).write(chat_message.content)\\n\\n\\n# 새로운 메시지를 추가\\ndef add_message(role, message):\\n    st.session_state[\"messages\"].append(ChatMessage(role=role, content=message))\\n\\n\\n# 세션 ID를 기반으로 세션 기록을 가져오는 함수\\ndef get_session_history(session_ids):\\n    if session_ids not in st.session_state[\"store\"]:  # 세션 ID가 store에 없는 경우\\n        # 새로운 ChatMessageHistory 객체를 생성하여 store에 저장\\n        st.session_state[\"store\"][session_ids] = ChatMessageHistory()\\n    return st.session_state[\"store\"][session_ids]  # 해당 세션 ID에 대한 세션 기록 반환\\n\\n\\n# 체인 생성\\ndef create_chain(model_name=\"gpt-4o\"):\\n\\n    # 프롬프트 정의\\n    prompt = ChatPromptTemplate.from_messages(\\n        [\\n            (\\n                \"system\",\\n                \"당신은 Question-Answering 챗봇입니다. 주어진 질문에 대한 답변을 제공해주세요.\",\\n            ),\\n            # 대화기록용 key 인 chat_history 는 가급적 변경 없이 사용하세요!\\n            MessagesPlaceholder(variable_name=\"chat_history\"),\\n            (\"human\", \"#Question:\\\\n{question}\"),  # 사용자 입력을 변수로 사용\\n        ]\\n    )\\n\\n    # llm 생성\\n    llm = ChatOpenAI(model_name=\"gpt-4o\")\\n\\n    # 일반 Chain 생성\\n    chain = prompt | llm | StrOutputParser()\\n\\n    chain_with_history = RunnableWithMessageHistory(\\n        chain,\\n        get_session_history,  # 세션 기록을 가져오는 함수\\n        input_messages_key=\"question\",  # 사용자의 질문이 템플릿 변수에 들어갈 key\\n        history_messages_key=\"chat_history\",  # 기록 메시지의 키\\n    )\\n    return chain_with_history\\n\\n\\n# 초기화 버튼이 눌리면...\\nif clear_btn:\\n    st.session_state[\"messages\"] = []\\n\\n# 이전 대화 기록 출력\\nprint_messages()\\n\\n# 사용자의 입력\\nuser_input = st.chat_input(\"궁금한 내용을 물어보세요!\")\\n\\n# 경고 메시지를 띄우기 위한 빈 영역\\nwarning_msg = st.empty()\\n\\nif \"chain\" not in st.session_state:\\n    st.session_state[\"chain\"] = create_chain(model_name=selected_model)\\n\\n\\n# 만약에 사용자 입력이 들어오면...\\nif user_input:\\n    chain = st.session_state[\"chain\"]\\n    if chain is not None:\\n        response = chain.stream(\\n            # 질문 입력\\n            {\"question\": user_input},\\n            # 세션 ID 기준으로 대화를 기록합니다.\\n            config={\"configurable\": {\"session_id\": session_id}},\\n        )\\n\\n        # 사용자의 입력\\n        st.chat_message(\"user\").write(user_input)\\n\\n        with st.chat_message(\"assistant\"):\\n            # 빈 공간(컨테이너)을 만들어서, 여기에 토큰을 스트리밍 출력한다.\\n            container = st.empty()\\n\\n            ai_answer = \"\"\\n            for token in response:\\n                ai_answer += token\\n                container.markdown(ai_answer)\\n\\n            # 대화기록을 저장한다.\\n            add_message(\"user\", user_input)\\n            add_message(\"assistant\", ai_answer)\\n    else:\\n        # 이미지를 업로드 하라는 경고 메시지 출력\\n        warning_msg.error(\"이미지를 업로드 해주세요.\")\\n'),\n",
       " Document(metadata={'source': '..\\\\19.Streamlit\\\\03-모델활용 프로젝트♥\\\\pages\\\\04_Multi_turn_practice copy 2.py'}, page_content='import os\\nimport streamlit as st\\nfrom dotenv import load_dotenv\\nfrom langchain_teddynote import logging\\nfrom langchain_teddynote.prompts import load_prompt\\nfrom langchain_core.messages import ChatMessage  # 메세지 추가\\nfrom langchain_community.chat_message_histories import (\\n    ChatMessageHistory,\\n)  # 대화기록 저장\\nfrom langchain_community.document_loaders import PDFPlumberLoader\\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\\nfrom langchain_openai import OpenAIEmbeddings, ChatOpenAI\\nfrom langchain_community.vectorstores import FAISS\\nfrom langchain_core.prompts import (\\n    ChatPromptTemplate,\\n    MessagesPlaceholder,\\n)  # 대화기록 저장\\nfrom langchain_core.output_parsers import StrOutputParser\\nfrom langchain_core.runnables import RunnablePassthrough\\nfrom langchain_core.runnables.history import RunnableWithMessageHistory  # 대화기록 저장\\n\\nload_dotenv()\\n\\nlogging.langsmith(\"[Project]Multi-turn\")\\n\\n# 캐시 디렉토리 생성 .을 찍는건 숨김표시 의미\\nif not os.path.exists(\"./.cache\"):\\n    os.mkdir(\"./.cache\")\\n\\nif not os.path.exists(\"./.cache/files\"):\\n    os.mkdir(\"./.cache/files\")\\n\\nif not os.path.exists(\"./.cache/embeddings\"):\\n    os.mkdir(\"./.cache.embeddings\")\\n\\nst.title(\"대화 내용을 기억하는 챗봇\")\\n\\n\\nif \"messages\" not in st.session_state:\\n    st.session_state[\"messages\"] = []\\n\\nif \"chain\" not in st.session_state:\\n    st.session_state[\"chain\"] = {}\\n\\n# ★★★ store를캐싱하는 코드. 세션 기록을 저장하는 딕셔너리\\nif \"store\" not in st.session_state:\\n    st.session_state[\"store\"] = {}\\n\\nwith st.sidebar:\\n    clear_btn = st.button(\"대화 초기화\")\\n\\n    uploaded_file = st.file_uploader(\"pdf파일 업로드\", type=[\"pdf\"])\\n\\n    selected_model = st.selectbox(\"모델선택\", [\"gpt-4o-mini\", \"gpt-4o\"], index=0)\\n\\n    system_prompt = st.text_area(\\n        \"시스템 프롬프트\", \"프롬프트를 입력해주세요.\", height=200\\n    )\\n    session_id = st.text_input(\"세션아이디를 입력하세요\", \"abc123\")\\n\\n# from langchain_core.messages import ChatMessage\\n\\n\\n# 이전대화 출력\\ndef print_messages():\\n    for chat_message in st.session_state[\"messages\"]:\\n        st.chat_message(chat_message.role).write(chat_message.content)\\n\\n\\n# 새로운 메세지 추가\\ndef add_message(role, message):\\n    st.session_state[\"messages\"].append(ChatMessage(role=role, content=message))\\n\\n\\n@st.cache_resource(show_spinner=\"파일을 업로드 중 입니다.\")\\ndef embed_file(file):\\n\\n    file_content = file.read()\\n    file_path = f\"./.cache/files/{file.name}\"\\n\\n    with open(file_path, \"wb\") as f:\\n        f.write(file_content)\\n\\n    ##### indexing\\n    loader = PDFPlumberLoader(file_path)\\n    docs = loader.load()\\n    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=50)\\n    documents = text_splitter.split_documents(docs)\\n\\n    embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\\n    vector_store = FAISS.from_documents(documents, embeddings)\\n    retriever = vector_store.as_retriever()\\n    return retriever\\n\\n\\n# 세션 기록을 저장하는 딕셔너리\\n# store = {}\\n# from langchain_community.chat_message_histories import ChatMessageHistory\\n\\n\\n# ★★★세션 id를 기반으로 세션 기록을 가져오는 함수 store를 st.session_state[\"store\"]로 바꿔줌\\ndef get_session_history(session_ids):\\n    if session_ids not in st.session_state[\"store\"]:\\n        st.session_state[\"store\"][session_ids] = ChatMessageHistory()\\n    return st.session_state[\"store\"][session_ids]\\n\\n\\n# from langchain_core.runnables.history import RunnableWithMessageHistory\\ndef create_chain(retriever, model_name=\"gpt-4o-mini\"):\\n\\n    # prompt = load_prompt(\"prompts/pdf-rag.yaml\", encoding=\"utf-8\")\\n    prompt = ChatPromptTemplate.from_messages(\\n        [\\n            (\\n                \"system\",\\n                \"당신은 Question-Answering 챗봇입니다. 주어진 문서를 통해서 질문에 대한 답변을 제공해 주세요. 모르면 모른다고 답변해 주세요. 한국어로 대답해 주세요.\",\\n            ),\\n            MessagesPlaceholder(variable_name=\"chat_history\"),\\n            (\"human\", \"#Question:\\\\n{question} #Context\\\\n{context}\"),\\n        ]\\n    )\\n\\n    llm = ChatOpenAI(model=model_name)\\n\\n    chain = (\\n        {\"context\": retriever, \"question\": RunnablePassthrough()}\\n        | prompt\\n        | llm\\n        | StrOutputParser()\\n    )\\n    # ★★★\\n    chain_with_history = RunnableWithMessageHistory(\\n        chain,\\n        get_session_history,\\n        input_messages_key=\"question\",\\n        history_messages_key=\"chat_history\",\\n    )\\n    return chain_with_history\\n\\n\\nif uploaded_file:\\n    retriever = embed_file(uploaded_file)\\n    chain = create_chain(retriever, model_name=selected_model)\\n    st.session_state[\"chain\"] = chain\\n\\n\\nif clear_btn:\\n    st.session_state[\"messages\"] = []\\n\\n\\nprint_messages()\\n\\n\\n# 유저 질문\\nuser_input = st.chat_input(\"궁금한 내용을 물어보세요\")\\n\\n# 경고메세지를 위함\\nwarning_msg = st.empty()\\n\\n# if \"chain\" not in st.session_state:\\n#     st.session_state[\"chain\"] = create_chain(retriever, model_name=selected_model)\\n\\nif user_input:\\n    # 체인생성\\n    chain = st.session_state[\"chain\"]\\n\\n    if chain:\\n        response = chain.stream(\\n            {\"question\": user_input},\\n            config={\"configurable\": {\"session_id\": session_id}},\\n        )\\n\\n        st.chat_message(\"user\").write(user_input)\\n\\n        with st.chat_message(\"assistant\"):\\n            container = st.empty()\\n\\n            ai_answer = \"\"\\n            for token in response:\\n                ai_answer += token\\n                container.markdown(ai_answer)\\n\\n        add_message(\"user\", user_input)\\n        add_message(\"assistant\", ai_answer)\\n\\n    else:\\n        warning_msg.error(\"파일을 업로드 해 주세요\")\\n'),\n",
       " Document(metadata={'source': '..\\\\19.Streamlit\\\\03-모델활용 프로젝트♥\\\\pages\\\\04_Multi_turn_practice.py'}, page_content='from urllib import response\\nfrom requests import session\\nimport os\\nimport streamlit as st\\nfrom dotenv import load_dotenv\\nfrom langchain_teddynote import logging\\nfrom langchain_community.document_loaders import PDFPlumberLoader\\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\\nfrom langchain_community.vectorstores import FAISS\\nfrom langchain_openai import ChatOpenAI, OpenAIEmbeddings\\nfrom langchain_core.messages.chat import ChatMessage\\nfrom langchain_teddynote.prompts import load_prompt\\nfrom langchain_core.runnables import RunnablePassthrough\\nfrom langchain_core.output_parsers import StrOutputParser\\n\\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\\nfrom langchain_community.chat_message_histories import ChatMessageHistory\\nfrom langchain_core.runnables.history import RunnableWithMessageHistory\\n\\nload_dotenv()\\n\\nlogging.langsmith(\"[Project] PDF Multi-turn RAG\")\\n\\n# 캐시 디렉토리 생성\\nif not os.path.exists(\".cache\"):\\n    os.mkdir(\".cache\")\\n\\n# 파일 업로드 전용 폴더\\nif not os.path.exists(\".cache/files\"):\\n    os.mkdir(\".cache/files\")\\n\\nif not os.path.exists(\".cache/embeddings\"):\\n    os.mkdir(\".cache/embeddings\")\\n\\nst.title(\"PDF기반 멀티턴 QA💬\")\\n\\n# 처음 1번만 실행하기 위한 코드. - 대화기록을 저장하기 위한 코드로 생성한다.\\nif \"messages\" not in st.session_state:\\n    st.session_state[\"messages\"] = []\\n\\n# 아무런 파일을 업로드 하지 않는경우\\nif \"chain\" not in st.session_state:\\n    st.session_state[\"chain\"] = None\\n\\nif \"store\" not in st.session_state:\\n    st.session_state[\"store\"] = {}\\n\\n# 사이드바 생성\\nwith st.sidebar:\\n    # 초기화 버튼 생성\\n    clear_btn = st.button(\"대화 초기화\")\\n\\n    # 파일 업로드\\n    uploaded_file = st.file_uploader(\"PDF파일 업로드\", type=[\"pdf\"])\\n\\n    # 모델 메뉴 선택\\n    selected_model = st.selectbox(\\n        \"model선택\", [\"gpt-4o-mini\", \"gpt-4o\", \"gpt-3.5-turbo\"]\\n    )\\n\\n    # 세션 ID 를 지정하는 메뉴\\n    session_id = st.text_input(\"세션 ID를 입력하세요.\", \"abc123\")\\n\\n\\n# 이전 대화를 츌력 - chat_message에는 role과 content가 들어가 있다.\\ndef print_messages():\\n    for chat_message in st.session_state[\"messages\"]:\\n        st.chat_message(chat_message.role).write(chat_message.content)\\n\\n\\n# 새로운 메세지를 추가 - ChatMessage은 langchain_core.messages.chat에 있다.\\ndef add_messages(role, message):\\n    # message가 문자열인지 확인\\n    if isinstance(message, str):\\n        st.session_state[\"messages\"].append(ChatMessage(role=role, content=message))\\n    else:\\n        st.session_state[\"messages\"].append(\\n            ChatMessage(role=role, content=str(message))\\n        )\\n\\n\\n# 세션 ID를 기반으로 세션 기록을 가져오는 함수\\ndef get_session_history(session_ids):\\n    if session_ids not in st.session_state[\"store\"]:  # 세션 ID가 store에 없는 경우\\n        # 새로운 ChatMessageHistory 객체를 생성하여 store에 저장\\n        st.session_state[\"store\"][session_ids] = ChatMessageHistory()\\n    return st.session_state[\"store\"][session_ids]  # 해당 세션 ID에 대한 세션 기록 반환\\n\\n\\n# 파일을 캐시 저장(시간이 오래 걸리는 작업을 처리할 예정)\\n@st.cache_resource(show_spinner=\"업로드한 파일을 처리중입니다.\")\\ndef embed_file(file):\\n    # 업로드한 파일을 캐시 디렉토리에 저장\\n    file_content = file.read()\\n    file_path = f\"./.cache/files/{file.name}\"\\n    with open(file_path, \"wb\") as f:\\n        f.write(file_content)\\n\\n    # ------------------------------ indexing\\n    # 1단계: 문서 로드\\n    loader = PDFPlumberLoader(file_path)\\n    docs = loader.load()\\n\\n    # 2단계: 문서 분할\\n    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=50)\\n    documents = text_splitter.split_documents(docs)\\n\\n    # 3단계: 임베딩 생성\\n    embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\\n\\n    # 4단계: 벡터 스토어 생성\\n    vector_store = FAISS.from_documents(documents, embeddings)\\n\\n    # 5단계: 검색기(Retriever) 생성\\n    retriever = vector_store.as_retriever()\\n    return retriever\\n\\n\\n# 체인 생성\\ndef create_chain(retriever, model_name=\"gpt-4o-mini\"):\\n    # 6단계: 프롬프트 생성\\n    # prompt = load_prompt(\"prompts/pdf-rag.yaml\", encoding=\"utf-8\")\\n    prompt = ChatPromptTemplate.from_messages(\\n        [\\n            (\\n                \"system\",\\n                \"당신은 Question-Answering 챗봇입니다. 주어진 문서를 통해서 질문에 대한 답변을 제공해 주세요. 모르면 모른다고 답변해 주세요. 한국어로 대답해 주세요.\",\\n            ),\\n            MessagesPlaceholder(variable_name=\"chat_history\"),\\n            (\"human\", \"#Question:\\\\n{question} #Context\\\\n{context}\"),\\n        ]\\n    )\\n\\n    # 7단계 언어모델 생성\\n    llm = ChatOpenAI(model=model_name, temperature=0)\\n\\n    # 8단계 체인 생성\\n    chain = (\\n        {\"context\": retriever, \"question\": RunnablePassthrough()}\\n        | prompt\\n        | llm\\n        | StrOutputParser()\\n    )\\n    chain_with_history = RunnableWithMessageHistory(\\n        chain,\\n        get_session_history,  # 세션 기록을 가져오는 함수\\n        input_messages_key=\"question\",  # 사용자의 질문이 템플릿 변수에 들어갈 key\\n        history_messages_key=\"chat_history\",  # 기록 메시지의 키\\n    )\\n    return chain_with_history\\n\\n\\n# 파일이 업로드 되었을 때\\nif uploaded_file:\\n    # 파일 업로드 후 retriever생성. 시간이 오래걸릴 예정\\n    retriever = embed_file(uploaded_file)\\n    chain = create_chain(retriever, model_name=selected_model)\\n    st.session_state[\"chain\"] = chain\\nelse:\\n    retriever = None\\n# 초기화 버튼이 눌렸을 때\\nif clear_btn:\\n    st.session_state[\"messages\"] = []\\n\\n# 이전 대화기록 출력\\nprint_messages()\\n\\n# 사용자의 입력\\nuser_input = st.chat_input(\"궁금한 내용을 물어보세요.\")\\n\\n# 경고메세지를 띄우기 위한 빈 영역\\nwarning_msg = st.empty()\\n\\n\\nif st.session_state[\"chain\"] is None and retriever is not None:\\n    st.session_state[\"chain\"] = create_chain(retriever, model_name=selected_model)\\n\\n\\n# if \"chain\" not in st.session_state:\\n#     st.session_state[\"chain\"] = create_chain(retriever, model_name=selected_model)\\n\\n# 만약 사용자의 입력이 들어오면,\\nif user_input:\\n    chain = st.session_state[\"chain\"]\\n    if chain is not None:\\n        response = chain.stream(\\n            # 질문 입력\\n            {\"question\": user_input},\\n            # 세션 ID 기준으로 대화를 기록합니다.\\n            config={\"configurable\": {\"session_id\": session_id}},\\n        )\\n\\n        # 사용자의 입력\\n        st.chat_message(\"user\").write(user_input)\\n\\n        with st.chat_message(\"assistant\"):\\n            # 빈 공간(컨테이너)을 만들어서, 여기에 토큰을 스트리밍 출력한다.\\n            container = st.empty()\\n\\n            ai_answer = \"\"\\n            for token in response:\\n                ai_answer += token.content\\n                container.markdown(ai_answer)\\n\\n            # 대화기록을 저장한다.\\n            add_messages(\"user\", user_input)\\n            add_messages(\"assistant\", ai_answer)\\n    else:\\n        # 이미지를 업로드 하라는 경고 메시지 출력\\n        warning_msg.error(\"pdf파일을 업로드 해주세요.\")\\n\\n\\n# poetry shell로 가상환경 실행\\n# streamlit run PDF.py\\n\\n\\n# # 세션 ID를 기반으로 세션 기록을 가져오는 함수\\n# def get_session_history(session_ids):\\n#     if session_ids not in st.session_state[\"store\"]:  # 세션 ID가 store에 없는 경우\\n#         # 새로운 ChatMessageHistory 객체를 생성하여 store에 저장\\n#         st.session_state[\"store\"][session_ids] = ChatMessageHistory()\\n#     return st.session_state[\"store\"][session_ids]  # 해당 세션 ID에 대한 세션 기록 반환\\n\\n\\n# # 파일을 캐시 저장(시간이 오래 걸리는 작업을 처리할 예정)\\n# @st.cache_resource(show_spinner=\"업로드한 파일을 처리중입니다.\")\\n# def embed_file(file):\\n#     # 업로드한 파일을 캐시 디렉토리에 저장\\n#     file_content = file.read()\\n#     file_path = f\"./.cache/files/{file.name}\"\\n#     with open(file_path, \"wb\") as f:\\n#         f.write(file_content)\\n\\n#     # ------------------------------ indexing\\n#     # 1단계: 문서 로드\\n#     loader = PDFPlumberLoader(file_path)\\n#     docs = loader.load()\\n\\n#     # 2단계: 문서 분할\\n#     text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=50)\\n#     documents = text_splitter.split_documents(docs)\\n\\n#     # 3단계: 임베딩 생성\\n#     embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\\n\\n#     # 4단계: 벡터 스토어 생성\\n#     vector_store = FAISS.from_documents(documents, embeddings)\\n\\n#     # 5단계: 검색기(Retriever) 생성\\n#     retriever = vector_store.as_retriever()\\n#     return retriever\\n\\n\\n# # 체인 생성\\n# def create_chain(retriever, model_name=\"gpt-4o-mini\"):\\n#     # 6단계: 프롬프트 생성\\n#     # prompt = load_prompt(\"prompts/pdf-rag.yaml\", encoding=\"utf-8\")\\n#     prompt = ChatPromptTemplate.from_messages(\\n#         [\\n#             (\\n#                 \"system\",\\n#                 \"당신은 Question-Answering 챗봇입니다. 주어진 문서를 통해서 질문에 대한 답변을 제공해 주세요. 모르면 모른다고 답변해 주세요. 한국어로 대답해 주세요.\",\\n#             ),\\n#             MessagesPlaceholder(variable_name=\"chat_history\"),\\n#             (\"human\", \"#Question:\\\\n{question} #Context\\\\n{context}\"),\\n#         ]\\n#     )\\n\\n#     # 7단계 언어모델 생성\\n#     llm = ChatOpenAI(model=model_name, temperature=0)\\n\\n#     # 8단계 체인 생성\\n#     chain = (\\n#         {\"context\": retriever, \"question\": RunnablePassthrough()}\\n#         | prompt\\n#         | llm\\n#         | StrOutputParser()\\n#     )\\n#     chain_with_history = RunnableWithMessageHistory(\\n#         chain,\\n#         get_session_history,  # 세션 기록을 가져오는 함수\\n#         input_messages_key=\"question\",  # 사용자의 질문이 템플릿 변수에 들어갈 key\\n#         history_messages_key=\"chat_history\",  # 기록 메시지의 키\\n#     )\\n#     return chain_with_history\\n\\n\\n# # 파일이 업로드 되었을 때\\n# if uploaded_file:\\n#     # 파일 업로드 후 retriever생성. 시간이 오래걸릴 예정\\n#     retriever = embed_file(uploaded_file)\\n#     chain = create_chain(retriever, model_name=selected_model)\\n#     st.session_state[\"chain\"] = chain\\n\\n# # 사용자의 입력\\n# user_input = st.chat_input(\"궁금한 내용을 물어보세요.\")\\n\\n# if \"chain\" not in st.session_state:\\n#     st.session_state[\"chain\"] = create_chain(retriever, model_name=selected_model)\\n'),\n",
       " Document(metadata={'source': '..\\\\19.Streamlit\\\\03-모델활용 프로젝트♥\\\\pages\\\\05_pdf-Multi_turn copy.py'}, page_content='from dotenv import load_dotenv\\nimport os\\nimport streamlit as st\\nfrom operator import itemgetter\\n\\nfrom langchain_teddynote import logging\\nfrom langchain_community.document_loaders import PDFPlumberLoader\\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\\nfrom langchain_openai import ChatOpenAI, OpenAIEmbeddings\\nfrom langchain_community.vectorstores import Chroma\\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\\nfrom langchain_core.output_parsers import StrOutputParser\\n\\n# kiwi\\nfrom kiwipiepy import Kiwi\\n\\nkiwi = Kiwi()\\n# Ensemble Retriever\\nfrom langchain.retrievers import BM25Retriever, EnsembleRetriever\\n\\n# reranker\\nfrom langchain.retrievers import ContextualCompressionRetriever\\nfrom langchain.retrievers.document_compressors import CrossEncoderReranker\\nfrom langchain_community.cross_encoders import HuggingFaceCrossEncoder\\n\\n# reorder\\nfrom langchain_community.document_transformers import LongContextReorder\\nfrom langchain_core.runnables import RunnableLambda\\n\\nlogging.langsmith(\"[Project] PDF Multu-turn RAG\")\\n\\nload_dotenv()\\n\\nif not os.path.exists(\".cache\"):\\n    os.mkdir(\"cache\")\\n\\nif not os.path.exists(\".cache/embeddings\"):\\n    os.mkdir(\"cache/embeddings\")\\n\\nif not os.path.exists(\".cache/files\"):\\n    os.mkdir(\"cache/files\")\\n\\nst.title(\"PDF기반 멀티턴 QA\")\\n\\n# 처음 한번만 실행하기 위한 코드 - 리스트로 저장하는것은 순서대로 저장하는 특징이 있다\\nif \"messages\" not in st.session_state:\\n    st.session_state[\"messages\"] = []\\n\\n# 딕셔너리로 저장하는것은 키벨류 쌍으로 저장하는 특징이 있다.\\nif \"store\" not in st.session_state:\\n    st.session_state[\"store\"] = {}\\n\\nwith st.sidebar:\\n    clear_btn = st.button(\"대화초기화\")\\n\\n    uploaded_file = st.file_uploader(\"PDF파일 업로드\", type=[\"PDF\"])\\n\\n    selected_model = st.selectbox(\\n        \"model선택\", [\"gpt-4o-mini\", \"gpt-4o\", \"gpt-3.5-turbo\"]\\n    )\\n\\n    # 세션 ID를 지정하는 메뉴\\n    session_id = st.text_input(\"세션 ID를 입력해 주세요\", \"abc123\")\\n\\n\\n# 이전 대화를 출력 - chat_message에는 role과 content가 들어가 있다.\\ndef print_message():\\n    for chat_message in st.session_state[\"messages\"]:\\n        st.chat_message(chat_message.role).write(chat_message.content)\\n\\n\\nfrom langchain_core.messages import ChatMessage\\n\\n\\n# 새로운 메세지를 추가 - ChatMessage은 langchain_core.messages.chat에 있다.\\ndef add_message(role, message):\\n    st.session_state[\"messages\"].append(ChatMessage(role=role, content=message))\\n\\n\\nfrom langchain_community.chat_message_histories import ChatMessageHistory\\n\\n\\n# 세션 ID를 기반으로 세션 기록을 가져오는 함수\\ndef get_session_history(session_ids):\\n    if session_ids not in st.session_state[\"store\"]:\\n        st.session_state[\"store\"][session_ids] = ChatMessageHistory()\\n    return st.session_state[\"store\"][session_ids]\\n\\n\\n# Kiwi 함수 : kiwi tokenizer\\ndef kiwi_tokenize(docs):\\n    return [token.form for token in kiwi.tokenize(docs)]\\n\\n\\n# 파일을 캐시 저장\\n@st.cache_resource(show_spinner=\"업로드한 파일을 처리중입니다.\")\\ndef embed_file(file):\\n    file_content = file.read()\\n    file_path = f\"./.cache/files/{file.name}\"\\n    with open(file_path, \"wb\") as f:\\n        f.write(file_content)\\n\\n    # indexing\\n    loader = PDFPlumberLoader(file_path)\\n    docs = loader.load()\\n\\n    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=50)\\n    documents = text_splitter.split_documents(docs)\\n\\n    embedding = OpenAIEmbeddings(model=\"text-embedding-3-small\")\\n    # chroma\\n    chroma_vector_store = Chroma.from_documents(documents, embedding)\\n    chroma_retriever = chroma_vector_store.as_retriever()\\n    # kiwi + bm25\\n    kiwi_vector_store = BM25Retriever.from_documents(\\n        documents, preprocess_func=kiwi_tokenize\\n    )\\n    # ensemble\\n    ensemble_retriever = EnsembleRetriever(\\n        retrievers=[chroma_retriever, kiwi_vector_store],\\n        weights=[0.5, 0.5],\\n        search_kwargs={\"k\": 10},\\n    )\\n    # reranker\\n    reranker_model = HuggingFaceCrossEncoder(model_name=\"BAAI/bge-reranker-v2-m3\")\\n    # 상위 3개 모델 선택\\n    compressor = CrossEncoderReranker(model=reranker_model, top_n=3)\\n    compression_retriever = ContextualCompressionRetriever(\\n        base_compressor=compressor, base_retriever=ensemble_retriever\\n    )\\n    return compression_retriever\\n\\n\\n# Reorder\\ndef reorder_documents(compression_retriever):\\n    # 재정렬\\n    reordering = LongContextReorder()\\n    reordered_docs = reordering.transform_documents(compression_retriever)\\n    return reordered_docs\\n\\n\\ndef create_chain(compression_retriever, model_name=\"gpt-4o-mini\"):\\n\\n    prompt = ChatPromptTemplate.from_messages(\\n        [\\n            (\\n                \"system\",\\n                \"\"\"\\n            당신은 Question-Answering 챗봇입니다. 주어진 문서를 통해서 질문에 대한 답변을 제공해 주세요. 모르면 모른다고 답변해 주세요. 한국어로 대답해 주세요\\n            \"\"\",\\n            ),\\n            MessagesPlaceholder(variable_name=\"chat_history\"),\\n            (\\n                \"user\",\\n                \"\"\"\\n            #Question\\n            {question}\\n            #Context\\n            {context}\\n            \"\"\",\\n            ),\\n        ]\\n    )\\n    llm = ChatOpenAI(model=model_name, temperature=0)\\n    chain = (\\n        {\\n            \"context\": itemgetter(\"question\")\\n            | compression_retriever\\n            | reorder_documents,\\n            \"question\": itemgetter(\"question\"),\\n            \"chat_history\": itemgetter(\"chat_history\"),\\n        }\\n        | prompt\\n        | llm\\n        | StrOutputParser()\\n    )\\n    from langchain_core.runnables import RunnableWithMessageHistory\\n\\n    chain_with_history = RunnableWithMessageHistory(\\n        chain,\\n        get_session_history,\\n        input_messages_key=\"question\",\\n        history_messages_key=\"chat_history\",\\n    )\\n    return chain_with_history\\n\\n\\nif uploaded_file:\\n    compression_retriever = embed_file(uploaded_file)\\n    chain = create_chain(compression_retriever, model_name=selected_model)\\n    st.session_state[\"chain\"] = chain\\n\\nif clear_btn:\\n    st.session_state[\"messages\"] = []\\n\\n# 이전 대화기록 출력\\nprint_message()\\n\\nuser_input = st.chat_input(\"궁금한 내용을 물어보세요\")\\nwarning_msg = st.empty()\\n\\nif user_input:\\n    chain = st.session_state[\"chain\"]\\n\\n    if chain is not None:\\n        response = chain.stream(\\n            {\"question\": user_input},\\n            config={\"configurable\": {\"session_id\": session_id}},\\n        )\\n        # 사용자의 입력\\n        st.chat_message(\"user\").write(user_input)\\n        # 빈 컨테이너를 만들어서 여기에 토큰을 스트리밍 출력한다.\\n        with st.chat_message(\"assistant\"):\\n            container = st.empty()\\n\\n            ai_answer = \"\"\\n            for token in response:\\n                ai_answer += token\\n                container.markdown(ai_answer)\\n\\n            # 대화기록 저장\\n            add_message(\"user\", user_input)\\n            add_message(\"assistant\", ai_answer)\\n\\n    else:\\n        warning_msg.error(\"pdf 파일을 업로드 해주세요.\")\\n'),\n",
       " Document(metadata={'source': '..\\\\19.Streamlit\\\\03-모델활용 프로젝트♥\\\\pages\\\\05_pdf-Multi_turn.py'}, page_content='from operator import itemgetter\\nimport os\\nimport streamlit as st\\nfrom dotenv import load_dotenv\\nfrom langchain_teddynote import logging\\nfrom langchain_community.document_loaders import PDFPlumberLoader\\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\\nfrom langchain_community.vectorstores import FAISS\\nfrom langchain_openai import ChatOpenAI, OpenAIEmbeddings\\nfrom langchain_core.messages.chat import ChatMessage\\nfrom langchain_teddynote.prompts import load_prompt\\nfrom langchain_core.runnables import RunnablePassthrough\\nfrom langchain_core.output_parsers import StrOutputParser\\n\\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\\nfrom langchain_community.chat_message_histories import ChatMessageHistory\\nfrom langchain_core.runnables.history import RunnableWithMessageHistory\\n\\nload_dotenv()\\n\\nlogging.langsmith(\"[Project] PDF Multi-turn RAG\")\\n\\n# 캐시 디렉토리 생성\\nif not os.path.exists(\".cache\"):\\n    os.mkdir(\".cache\")\\n\\n# 파일 업로드 전용 폴더\\nif not os.path.exists(\".cache/files\"):\\n    os.mkdir(\".cache/files\")\\n\\nif not os.path.exists(\".cache/embeddings\"):\\n    os.mkdir(\".cache/embeddings\")\\n\\nst.title(\"PDF기반 멀티턴 QA💬\")\\n\\n# 처음 1번만 실행하기 위한 코드. - 대화기록을 저장하기 위한 코드로 생성한다.\\nif \"messages\" not in st.session_state:\\n    st.session_state[\"messages\"] = []\\n\\n# 아무런 파일을 업로드 하지 않는경우: store안에 chain이 들어가 있으니까!\\n# if \"chain\" not in st.session_state:\\n#     st.session_state[\"chain\"] = None\\n\\nif \"store\" not in st.session_state:\\n    st.session_state[\"store\"] = {}\\n\\n# 사이드바 생성\\nwith st.sidebar:\\n    # 초기화 버튼 생성\\n    clear_btn = st.button(\"대화 초기화\")\\n\\n    # 파일 업로드\\n    uploaded_file = st.file_uploader(\"PDF파일 업로드\", type=[\"pdf\"])\\n\\n    # 모델 메뉴 선택\\n    selected_model = st.selectbox(\\n        \"model선택\", [\"gpt-4o-mini\", \"gpt-4o\", \"gpt-3.5-turbo\"]\\n    )\\n\\n    # 세션 ID 를 지정하는 메뉴\\n    session_id = st.text_input(\"세션 ID를 입력하세요.\", \"abc123\")\\n\\n\\n# 이전 대화를 츌력 - chat_message에는 role과 content가 들어가 있다.\\ndef print_messages():\\n    for chat_message in st.session_state[\"messages\"]:\\n        st.chat_message(chat_message.role).write(chat_message.content)\\n\\n\\n# 새로운 메세지를 추가 - ChatMessage은 langchain_core.messages.chat에 있다.\\ndef add_messages(role, message):\\n    st.session_state[\"messages\"].append(ChatMessage(role=role, content=message))\\n\\n\\n# 세션 ID를 기반으로 세션 기록을 가져오는 함수\\ndef get_session_history(session_ids):\\n    if session_ids not in st.session_state[\"store\"]:  # 세션 ID가 store에 없는 경우\\n        # 새로운 ChatMessageHistory 객체를 생성하여 store에 저장\\n        st.session_state[\"store\"][session_ids] = ChatMessageHistory()\\n    return st.session_state[\"store\"][session_ids]  # 해당 세션 ID에 대한 세션 기록 반환\\n\\n\\n# 파일을 캐시 저장(시간이 오래 걸리는 작업을 처리할 예정)\\n@st.cache_resource(show_spinner=\"업로드한 파일을 처리중입니다.\")\\ndef embed_file(file):\\n    # 업로드한 파일을 캐시 디렉토리에 저장\\n    file_content = file.read()\\n    file_path = f\"./.cache/files/{file.name}\"\\n    with open(file_path, \"wb\") as f:\\n        f.write(file_content)\\n\\n    # ------------------------------ indexing\\n    # 1단계: 문서 로드\\n    loader = PDFPlumberLoader(file_path)\\n    docs = loader.load()\\n\\n    # 2단계: 문서 분할\\n    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=50)\\n    documents = text_splitter.split_documents(docs)\\n\\n    # 3단계: 임베딩 생성\\n    embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\\n\\n    # 4단계: 벡터 스토어 생성\\n    vector_store = FAISS.from_documents(documents, embeddings)\\n\\n    # 5단계: 검색기(Retriever) 생성\\n    retriever = vector_store.as_retriever()\\n    return retriever\\n\\n\\n# 체인 생성\\ndef create_chain(retriever, model_name=\"gpt-4o-mini\"):\\n    # 6단계: 프롬프트 생성\\n    # prompt = load_prompt(\"prompts/pdf-rag.yaml\", encoding=\"utf-8\")\\n    prompt = ChatPromptTemplate.from_messages(\\n        [\\n            (\\n                \"system\",\\n                \"당신은 Question-Answering 챗봇입니다. 주어진 문서를 통해서 질문에 대한 답변을 제공해 주세요. 모르면 모른다고 답변해 주세요. 한국어로 대답해 주세요.\",\\n            ),\\n            MessagesPlaceholder(variable_name=\"chat_history\"),\\n            (\"human\", \"#Question:\\\\n{question} #Context\\\\n{context}\"),\\n        ]\\n    )\\n\\n    # 7단계 언어모델 생성\\n    llm = ChatOpenAI(model=model_name, temperature=0)\\n\\n    # 8단계 체인 생성\\n    chain = (\\n        {\\n            \"context\": itemgetter(\"question\") | retriever,  # 수정\\n            \"question\": itemgetter(\"question\"),  # 수정\\n            \"chat_history\": itemgetter(\"chat_history\"),  # 수정c\\n        }\\n        | prompt\\n        | llm\\n        | StrOutputParser()\\n    )\\n    chain_with_history = RunnableWithMessageHistory(\\n        chain,\\n        get_session_history,  # 세션 기록을 가져오는 함수\\n        input_messages_key=\"question\",  # 사용자의 질문이 템플릿 변수에 들어갈 key\\n        history_messages_key=\"chat_history\",  # 기록 메시지의 키\\n    )\\n    return chain_with_history\\n\\n\\n# 파일이 업로드 되었을 때\\nif uploaded_file:\\n    # 파일 업로드 후 retriever생성. 시간이 오래걸릴 예정\\n    retriever = embed_file(uploaded_file)\\n    chain = create_chain(retriever, model_name=selected_model)\\n    st.session_state[\"chain\"] = chain\\n\\n# 초기화 버튼이 눌렸을 때\\nif clear_btn:\\n    st.session_state[\"messages\"] = []\\n\\n# 이전 대화기록 출력\\nprint_messages()\\n\\n# 사용자의 입력\\nuser_input = st.chat_input(\"궁금한 내용을 물어보세요.\")\\n\\n# 경고메세지를 띄우기 위한 빈 영역\\nwarning_msg = st.empty()\\n\\n# 수정\\n# if \"chain\" not in st.session_state:\\n#     st.session_state[\"chain\"] = create_chain(retriever, model_name=selected_model)\\n\\n# 만약 사용자의 입력이 들어오면,\\nif user_input:\\n    chain = st.session_state[\"chain\"]\\n    if chain is not None:\\n        response = chain.stream(\\n            # 질문 입력\\n            {\"question\": user_input},\\n            # 세션 ID 기준으로 대화를 기록합니다.\\n            config={\"configurable\": {\"session_id\": session_id}},\\n        )\\n\\n        # 사용자의 입력\\n        st.chat_message(\"user\").write(user_input)\\n\\n        with st.chat_message(\"assistant\"):\\n            # 빈 공간(컨테이너)을 만들어서, 여기에 토큰을 스트리밍 출력한다.\\n            container = st.empty()\\n\\n            ai_answer = \"\"\\n            for token in response:\\n                ai_answer += token\\n                container.markdown(ai_answer)\\n\\n            # 대화기록을 저장한다.\\n            add_messages(\"user\", user_input)\\n            add_messages(\"assistant\", ai_answer)\\n    else:\\n        # 이미지를 업로드 하라는 경고 메시지 출력\\n        warning_msg.error(\"pdf파일을 업로드 해주세요.\")\\n'),\n",
       " Document(metadata={'source': '..\\\\19.Streamlit\\\\MyProject\\\\1.main.py'}, page_content='import streamlit as st\\nfrom langchain_core.messages.chat import ChatMessage\\n\\nst.title(\"방이 Chat GPT Test\")\\n\\n\\n# 처음 1번만 실행하기 위한 코드\\nif \"messages\" not in st.session_state:\\n    # 대화내용을 저장하는 기능\\n    st.session_state[\"messages\"] = []\\n\\n\\n# 이전 대화내용을 출력, 프린트 함수.\\n# for role, message in st.session_state[\"messages\"]:\\n#     st.chat_message(role).write(message)\\ndef print_messages():\\n    for chat_message in st.session_state[\"messages\"]:\\n        st.chat_message(chat_message.role).write(chat_message.content)\\n        # st.write(f\"{chat_message.role}: {chat_message.comtent}\")\\n\\n\\n# 새로운 메세지를 추가하는 함수를 만듬.\\ndef add_message(role, message):\\n    st.session_state[\"messages\"].append(ChatMessage(role=role, content=message))\\n\\n\\nprint_messages()\\n\\n\\n# 사용자 입력창\\nuser_input = st.chat_input(\"궁금한 것을 물어보세요\")\\n\\n# 만약 사용자 입력이 들어오면..\\n# 저장, message의 경우 contrainer안에 담아주는 역할.\\nif user_input:\\n    # st.write(f\"사용자 입력: {user_input}\")\\n    st.chat_message(\"user\").write(user_input)  # 입력\\n    st.chat_message(\"assistant\").write(user_input)  # ai도 그대로 입력\\n\\n    # 대화내용을 저장. 위의 st.session_state[\"messages\"] = []를 받는다.\\n    add_message(\"user\", user_input)\\n    add_message(\"assistant\", user_input)\\n    # ChatMessage(role=\"user\", content=user_input)\\n    # ChatMessage(role=\"assistant\", content=user_input)\\n    # st.session_state[\"messages\"].append((\"user\", user_input))\\n    # st.session_state[\"messages\"].append((\"assistant\", user_input))\\n\\n## Terminal창에서 streamlit을 켜준다.\\n# streamlit run .\\\\19-Streamlit\\\\MyProject\\\\main.py\\n'),\n",
       " Document(metadata={'source': '..\\\\19.Streamlit\\\\MyProject\\\\2.main.py'}, page_content='import streamlit as st\\n\\nfrom langchain_core.messages.chat import ChatMessage\\nfrom langchain_openai import ChatOpenAI\\nfrom langchain_core.prompts import ChatPromptTemplate\\nfrom langchain_core.output_parsers import StrOutputParser\\n\\nfrom dotenv import load_dotenv\\n\\n# API_KEY 정보 로드\\nload_dotenv()\\n\\nst.title(\"방이 Chat GPT Test\")\\n\\n\\n# 처음 1번만 실행하기 위한 코드\\nif \"messages\" not in st.session_state:\\n    # 대화내용을 저장하는 기능\\n    st.session_state[\"messages\"] = []\\n\\n# 사이드바 생성\\nwith st.sidebar:\\n    # 초기화 버튼 생성\\n    clear_btn = st.button(\"대화초기화\")\\n\\n\\n# 이전 대화내용을 출력, 프린트 함수.\\n# for role, message in st.session_state[\"messages\"]:\\n#     st.chat_message(role).write(message)\\ndef print_messages():\\n    for chat_message in st.session_state[\"messages\"]:\\n        st.chat_message(chat_message.role).write(chat_message.content)\\n        # st.write(f\"{chat_message.role}: {chat_message.comtent}\")\\n\\n\\n# 새로운 메세지를 추가하는 함수를 만듬.\\ndef add_message(role, message):\\n    st.session_state[\"messages\"].append(ChatMessage(role=role, content=message))\\n\\n\\n# 체인생성\\ndef create_chain():\\n    # prompt | llm | output_parser\\n    # prompt\\n    prompt = ChatPromptTemplate.from_messages(\\n        {\\n            (\"system\", \"당신은 친절한 AI 어시스턴트 입니다.\"),  # 전역변수로, 지시사항\\n            (\"user\", \"#Question:\\\\n{question}\"),  # 입력\\n        }\\n    )\\n    # model\\n    llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\\n\\n    # 출력 파서\\n    output_parser = StrOutputParser()\\n\\n    # 체인 생성\\n    chain = prompt | llm | output_parser\\n\\n    return chain\\n\\n\\n### 초기화 버튼이 눌리면, 빈 list를 만들어준다.\\nif clear_btn:\\n    st.session_state[\"messages\"] = []\\n\\n### 이전 대화 기록 출력\\nprint_messages()\\n\\n\\n# 사용자 입력창\\nuser_input = st.chat_input(\"궁금한 것을 물어보세요\")\\n\\n# 만약 사용자 입력이 들어오면..\\n# 저장, message의 경우 contrainer안에 담아주는 역할.\\nif user_input:\\n    # 사용자의 입력\\n    st.chat_message(\"user\").write(user_input)  # 입력\\n    # chain을 생성\\n    chain = create_chain()\\n    # ai_answer = chain.invoke({\"question\": user_input})\\n\\n    ## 한글자씩 출력하기 위한 stream이용\\n    response = chain.stream({\"question\": user_input})\\n    with st.chat_message(\"assistant\"):  # 질문을\\n        # 빈 공간(컨테이너)을 만들어서, 여기에 토큰을 스트리밍 출력한다.\\n        container = st.empty()\\n\\n        ai_answer = \"\"\\n        for token in response:  # token을 하나씩 받는다.\\n            ai_answer += token  # answer에다가 token을 누적시킨다.\\n            container.markdown(ai_answer)\\n\\n    # ai_의 답변\\n    # st.chat_message(\"assistant\").write(ai_answer)\\n    # st.chat_message(\"assistant\").write(user_input)  # ai도 그대로 입력\\n\\n    # 대화내용을 저장. 위의 st.session_state[\"messages\"] = []를 받는다.\\n    add_message(\"user\", user_input)\\n    add_message(\"assistant\", ai_answer)\\n    # add_message(\"assistant\", user_input)\\n\\n## Terminal창에서 streamlit을 켜준다.\\n# streamlit run .\\\\19-Streamlit\\\\MyProject\\\\main.py\\n'),\n",
       " Document(metadata={'source': '..\\\\19.Streamlit\\\\MyProject\\\\3.main.py'}, page_content='import streamlit as st\\n\\nfrom langchain_core.messages.chat import ChatMessage\\nfrom langchain_openai import ChatOpenAI\\nfrom langchain_core.prompts import ChatPromptTemplate\\nfrom langchain_core.output_parsers import StrOutputParser\\nfrom langchain_teddynote.prompts import load_prompt\\nfrom langchain import hub\\n\\nfrom dotenv import load_dotenv\\n\\n# API_KEY 정보 로드\\nload_dotenv()\\n\\nst.title(\"방이 Chat GPT Test\")\\n\\n\\n# 처음 1번만 실행하기 위한 코드\\nif \"messages\" not in st.session_state:\\n    # 대화내용을 저장하는 기능\\n    st.session_state[\"messages\"] = []\\n\\n# 사이드바 생성\\nwith st.sidebar:\\n    # 초기화 버튼 생성\\n    clear_btn = st.button(\"대화 초기화\")\\n\\n    # 셀렉트 박스 만들기. 기본모드 , SNS 게시글, 요약 3개가 출력됌.\\n    selected_prompt = st.selectbox(\\n        \"프롬프트를 선택해 주세요\",\\n        (\"기본모드\", \"SNS 게시글\", \"요약\"),\\n        index=0,\\n    )\\n\\n\\n# 이전 대화내용을 출력, 프린트 함수.\\ndef print_messages():\\n    for chat_message in st.session_state[\"messages\"]:\\n        st.chat_message(chat_message.role).write(chat_message.content)\\n        # 역할과, 내용만 출력\\n\\n\\n# 새로운 메세지를 추가하는 함수를 만듬.\\ndef add_message(role, message):\\n    st.session_state[\"messages\"].append(ChatMessage(role=role, content=message))\\n\\n\\n# 체인생성 - 매개변수에 prompt추가\\ndef create_chain(prompt_type):\\n    # prompt | llm | output_parser\\n    # prompt (기본모드)\\n    prompt = ChatPromptTemplate.from_messages(\\n        [\\n            (\\n                \"system\",\\n                \"당신은 친절한 AI 어시스턴트 입니다. 다음의 질문에 간결하게 담변해 주세요.\",\\n            ),  # 전역변수로, 지시사항\\n            (\"user\", \"#Question:\\\\n{question}\"),  # 입력\\n        ]\\n    )\\n\\n    # SNS 마케터 프롬프트\\n    if prompt_type == \"SNS 게시글\":\\n        prompt = load_prompt(\\n            \"19-Streamlit/MyProject/prompts/sns.yaml\", encoding=\"utf-8\"\\n        )\\n        # load_prompt(\"prompts/sns.yaml\", encoding=\"utf-8\")\\n\\n    # 19-Streamlit\\\\MyProject\\\\prompts\\\\sns.yaml\\n\\n    # 요약 prompt\\n    elif prompt_type == \"요약\":\\n        prompt = hub.pull(\"teddynote/chain-of-density-korean:946ed62d\")\\n\\n    # model\\n    # llm = ChatOpenAI(model_name=\"gpt-4o\", temperature=0)\\n    llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\\n\\n    # 출력 파서\\n    output_parser = StrOutputParser()\\n\\n    # 체인 생성\\n    chain = prompt | llm | output_parser\\n\\n    return chain\\n\\n\\n### 초기화 버튼이 눌리면, 빈 list를 만들어준다.\\nif clear_btn:\\n    st.session_state[\"messages\"] = []\\n\\n### 이전 대화 기록 출력\\nprint_messages()\\n\\n\\n# 사용자 입력창\\nuser_input = st.chat_input(\"궁금한 내용을 물어보세요!\")\\n\\n# 만약 사용자 입력이 들어오면..\\n# 저장, message의 경우 contrainer안에 담아주는 역할.\\nif user_input:\\n    # 사용자의 입력\\n    st.chat_message(\"user\").write(user_input)  # 입력\\n\\n    # chain을 생성\\n    chain = create_chain(selected_prompt)\\n    # ai_answer = chain.invoke({\"question\": user_input})\\n\\n    ## 스트리밍 호출:한글자씩 출력하기 위한 stream이용\\n    response = chain.stream({\"question\": user_input})\\n    with st.chat_message(\"assistant\"):  # 질문을\\n        # 빈 공간(컨테이너)을 만들어서, 여기에 토큰을 스트리밍 출력한다.\\n        container = st.empty()\\n\\n        ai_answer = \"\"\\n        for token in response:  # token을 하나씩 받는다.\\n            ai_answer += token  # answer에다가 token을 누적시킨다.\\n            container.markdown(ai_answer)  # ai_answer를 markdown형식으로 저장.\\n\\n    # 대화내용을 저장. 위의 st.session_state[\"messages\"] = []를 받는다.\\n    add_message(\"user\", user_input)\\n    add_message(\"assistant\", ai_answer)\\n    # add_message(\"assistant\", user_input)\\n\\n## Terminal창에서 streamlit을 켜준다.\\n# streamlit run 19-Streamlit\\\\MyProject\\\\main.py\\n'),\n",
       " Document(metadata={'source': '..\\\\19.Streamlit\\\\MyProject\\\\4.main.py'}, page_content='import streamlit as st\\nfrom langchain_core.messages.chat import ChatMessage\\nfrom langchain_core.prompts import ChatPromptTemplate\\nfrom langchain_openai import ChatOpenAI\\nfrom langchain_core.output_parsers import StrOutputParser\\nfrom langchain_teddynote.prompts import load_prompt\\nfrom dotenv import load_dotenv\\nfrom langchain import hub\\n\\n# API KEY 정보로드\\nload_dotenv()\\n\\nst.title(\"나만의 챗GPT💬\")\\n\\n\\n# 처음 1번만 실행하기 위한 코드\\nif \"messages\" not in st.session_state:\\n    # 대화기록을 저장하기 위한 용도로 생성한다.\\n    st.session_state[\"messages\"] = []\\n\\n# 사이드바 생성\\nwith st.sidebar:\\n    # 초기화 버튼 생성\\n    clear_btn = st.button(\"대화 초기화\")\\n\\n    selected_prompt = st.selectbox(\\n        \"프롬프트를 선택해 주세요\",\\n        (\"기본모드\", \"SNS 게시글\", \"요약\"),\\n        index=0,\\n    )\\n\\n\\n# 이전 대화를 출력\\ndef print_messages():\\n    for chat_message in st.session_state[\"messages\"]:\\n        st.chat_message(chat_message.role).write(chat_message.content)\\n\\n\\n# 새로운 메시지를 추가\\ndef add_message(role, message):\\n    st.session_state[\"messages\"].append(ChatMessage(role=role, content=message))\\n\\n\\n# 체인 생성\\ndef create_chain(prompt_type):\\n    # prompt | llm | output_parser\\n    # 프롬프트(기본모드)\\n    prompt = ChatPromptTemplate.from_messages(\\n        [\\n            (\\n                \"system\",\\n                \"당신은 친절한 AI 어시스턴트입니다. 다음의 질문에 간결하게 답변해 주세요.\",\\n            ),\\n            (\"user\", \"#Question:\\\\n{question}\"),\\n        ]\\n    )\\n    if prompt_type == \"SNS 게시글\":\\n        # Windows 사용자 only: 인코딩을 cp949로 설정\\n        prompt = load_prompt(\\n            \"19-Streamlit/MyProject/prompts/sns.yaml\", encoding=\"utf-8\"\\n        )\\n    elif prompt_type == \"요약\":\\n        # 요약 프롬프트\\n        prompt = hub.pull(\"teddynote/chain-of-density-korean:946ed62d\")\\n\\n    # GPT\\n    llm = ChatOpenAI(model_name=\"gpt-4o\", temperature=0)\\n\\n    # 출력 파서\\n    output_parser = StrOutputParser()\\n\\n    # 체인 생성\\n    chain = prompt | llm | output_parser\\n    return chain\\n\\n\\n# 초기화 버튼이 눌리면...\\nif clear_btn:\\n    st.session_state[\"messages\"] = []\\n\\n# 이전 대화 기록 출력\\nprint_messages()\\n\\n# 사용자의 입력\\nuser_input = st.chat_input(\"궁금한 내용을 물어보세요!\")\\n\\n# 만약에 사용자 입력이 들어오면...\\nif user_input:\\n    # 사용자의 입력\\n    st.chat_message(\"user\").write(user_input)\\n    # chain 을 생성\\n    chain = create_chain(selected_prompt)\\n\\n    # 스트리밍 호출\\n    response = chain.stream({\"question\": user_input})\\n    with st.chat_message(\"assistant\"):\\n        # 빈 공간(컨테이너)을 만들어서, 여기에 토큰을 스트리밍 출력한다.\\n        container = st.empty()\\n\\n        ai_answer = \"\"\\n        for token in response:\\n            ai_answer += token\\n            container.markdown(ai_answer)\\n\\n    # 대화기록을 저장한다.\\n    add_message(\"user\", user_input)\\n    add_message(\"assistant\", ai_answer)\\n'),\n",
       " Document(metadata={'source': '..\\\\19.Streamlit\\\\MyProject\\\\5.main.py'}, page_content='import streamlit as st\\nimport glob\\nfrom langchain_core.messages.chat import ChatMessage\\nfrom langchain_core.prompts import ChatPromptTemplate\\nfrom langchain_openai import ChatOpenAI\\nfrom langchain_core.output_parsers import StrOutputParser\\nfrom langchain_teddynote.prompts import load_prompt\\nfrom dotenv import load_dotenv\\n\\n# API KEY 정보로드\\nload_dotenv()\\n\\nst.title(\"나만의 챗GPT💬\")\\n\\n\\n# 처음 1번만 실행하기 위한 코드\\nif \"messages\" not in st.session_state:\\n    # 대화기록을 저장하기 위한 용도로 생성한다.\\n    st.session_state[\"messages\"] = []\\n\\nif \"chain\" not in st.session_state:\\n    st.session_state[\"chain\"] = None\\n\\n# 사이드바 생성\\nwith st.sidebar:\\n    # 초기화 버튼 생성\\n    clear_btn = st.button(\"대화 초기화\")\\n\\n    # import glob\\n\\n    # 특정 문자열 규칙을 주면, 그 규칙 안에있는 특정 파일을 쭉 가져와서 list로 저장해준디.\\n    prompt_files = glob.glob(\"prompts/*.yaml\")\\n    selected_prompt = st.selectbox(\"프롬프트를 선택해 주세요\", prompt_files, index=0)\\n    task_input = st.text_input(\"TASK입력\", \"\")\\n\\n\\n# 이전 대화를 출력\\ndef print_messages():\\n    for chat_message in st.session_state[\"messages\"]:\\n        st.chat_message(chat_message.role).write(chat_message.content)\\n\\n\\n# 새로운 메시지를 추가\\ndef add_message(role, message):\\n    st.session_state[\"messages\"].append(ChatMessage(role=role, content=message))\\n\\n\\n# def print_messages():\\n#     for chat_message in st.session_state[\"messages\"]:\\n#         st.chat_message(chat_message.role).write(chat_message.content)\\n\\n\\n# def add_message(role, message):\\n#     st.session_state[\"mesasges\"].append(ChatMessage(role=role, content=message))\\n\\n\\n# 체인 생성\\ndef create_chain(prompt_filepath, task=\"\"):\\n\\n    # prompt | llm | output_parser\\n    prompt = load_prompt(prompt_filepath, encoding=\"utf-8\")\\n\\n    if task:\\n        prompt = prompt.partial(task=task)\\n\\n    # GPT\\n    llm = ChatOpenAI(model_name=\"gpt-4o\", temperature=0)\\n\\n    # 출력 파서\\n    output_parser = StrOutputParser()\\n\\n    # 체인 생성\\n    chain = prompt | llm | output_parser\\n    return chain\\n\\n\\n# 초기화 버튼이 눌리면...\\nif clear_btn:\\n    st.session_state[\"messages\"] = []\\n\\n# 이전 대화 기록 출력\\nprint_messages()\\n\\n# 사용자의 입력\\nuser_input = st.chat_input(\"궁금한 내용을 물어보세요!\")\\n\\n# 만약에 사용자 입력이 들어오면...\\nif user_input:\\n    # 사용자의 입력\\n    st.chat_message(\"user\").write(user_input)\\n    # chain 을 생성\\n    chain = create_chain(selected_prompt, task=task_input)\\n\\n    # 스트리밍 호출\\n    response = chain.stream({\"question\": user_input})\\n    with st.chat_message(\"assistant\"):\\n        # 빈 공간(컨테이너)을 만들어서, 여기에 토큰을 스트리밍 출력한다.\\n        container = st.empty()\\n\\n        ai_answer = \"\"\\n        for token in response:\\n            ai_answer += token\\n            container.markdown(ai_answer)\\n\\n    # 대화기록을 저장한다.\\n    add_message(\"user\", user_input)\\n    add_message(\"assistant\", ai_answer)\\n'),\n",
       " Document(metadata={'source': '..\\\\19.Streamlit\\\\MyProject\\\\6.main.py'}, page_content='import streamlit as st\\nimport glob\\nfrom langchain_core.messages.chat import ChatMessage\\nfrom langchain_core.prompts import ChatPromptTemplate\\nfrom langchain_openai import ChatOpenAI\\nfrom langchain_core.output_parsers import StrOutputParser\\nfrom langchain_teddynote.prompts import load_prompt\\nfrom dotenv import load_dotenv\\n\\n# API KEY 정보로드\\nload_dotenv()\\n\\nst.title(\"나만의 챗GPT💬\")\\n\\n\\n# 처음 1번만 실행하기 위한 코드\\nif \"messages\" not in st.session_state:\\n    # 대화기록을 저장하기 위한 용도로 생성한다.\\n    st.session_state[\"messages\"] = []\\n\\nif \"chain\" not in st.session_state:\\n    st.session_state[\"chain\"] = None\\n\\n# 사이드바 생성\\nwith st.sidebar:\\n    # 초기화 버튼 생성\\n    clear_btn = st.button(\"대화 초기화\")\\n\\n    # import glob\\n\\n    # 특정 문자열 규칙을 주면, 그 규칙 안에있는 특정 파일을 쭉 가져와서 list로 저장해준디.\\n    prompt_files = glob.glob(\"prompts/*.yaml\")\\n    selected_prompt = st.selectbox(\"프롬프트를 선택해 주세요\", prompt_files, index=0)\\n    task_input = st.text_input(\"TASK입력\", \"\")\\n\\n\\n# 이전 대화를 출력\\ndef print_messages():\\n    for chat_message in st.session_state[\"messages\"]:\\n        st.chat_message(chat_message.role).write(chat_message.content)\\n\\n\\n# 새로운 메시지를 추가\\ndef add_message(role, message):\\n    st.session_state[\"messages\"].append(ChatMessage(role=role, content=message))\\n\\n\\n# def print_messages():\\n#     for chat_message in st.session_state[\"messages\"]:\\n#         st.chat_message(chat_message.role).write(chat_message.content)\\n\\n\\n# def add_message(role, message):\\n#     st.session_state[\"mesasges\"].append(ChatMessage(role=role, content=message))\\n\\n\\n# 체인 생성\\ndef create_chain(prompt_filepath, task=\"\"):\\n\\n    # prompt | llm | output_parser\\n    prompt = load_prompt(prompt_filepath, encoding=\"utf-8\")\\n\\n    if task:\\n        prompt = prompt.partial(task=task)\\n\\n    # GPT\\n    llm = ChatOpenAI(model_name=\"gpt-4o\", temperature=0)\\n\\n    # 출력 파서\\n    output_parser = StrOutputParser()\\n\\n    # 체인 생성\\n    chain = prompt | llm | output_parser\\n    return chain\\n\\n\\n# 초기화 버튼이 눌리면...\\nif clear_btn:\\n    st.session_state[\"messages\"] = []\\n\\n# 이전 대화 기록 출력\\nprint_messages()\\n\\n# 사용자의 입력\\nuser_input = st.chat_input(\"궁금한 내용을 물어보세요!\")\\n\\n# 만약에 사용자 입력이 들어오면...\\nif user_input:\\n    # 사용자의 입력\\n    st.chat_message(\"user\").write(user_input)\\n    # chain 을 생성\\n    chain = create_chain(selected_prompt, task=task_input)\\n\\n    # 스트리밍 호출\\n    response = chain.stream({\"question\": user_input})\\n    with st.chat_message(\"assistant\"):\\n        # 빈 공간(컨테이너)을 만들어서, 여기에 토큰을 스트리밍 출력한다.\\n        container = st.empty()\\n\\n        ai_answer = \"\"\\n        for token in response:\\n            ai_answer += token\\n            container.markdown(ai_answer)\\n\\n    # 대화기록을 저장한다.\\n    add_message(\"user\", user_input)\\n    add_message(\"assistant\", ai_answer)\\n'),\n",
       " Document(metadata={'source': '..\\\\19.Streamlit\\\\MyProject\\\\pages\\\\01_PDF.py'}, page_content='import streamlit as st\\nimport os\\nfrom dotenv import load_dotenv\\n\\nfrom langchain_teddynote import logging\\nfrom langchain_core.messages import ChatMessage\\nfrom langchain_openai import ChatOpenAI, OpenAIEmbeddings\\nfrom langchain_core.output_parsers import StrOutputParser\\nfrom langchain_core.runnables import RunnablePassthrough\\nfrom langchain_teddynote.prompts import load_prompt\\nfrom langchain_community.document_loaders import PDFPlumberLoader\\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\\nfrom langchain_community.vectorstores import FAISS\\n\\nlogging.langsmith(\"[Project] PDF RAG\")\\n# api key 가져오기\\nload_dotenv()\\n\\n# 캐시 디렉토리 생성 .은 숨김파일로 처리한다.\\nif not os.path.exists(\".cache\"):\\n    os.mkdir(\".cache\")\\n\\nif not os.path.exists(\".cache/embeddings\"):\\n    os.mkdir(\".cache/embeddings\")\\n\\nif not os.path.exists(\".cache/files\"):\\n    os.mkdir(\".cache/files\")\\n\\n\\nst.title(\"PDF기반 QA💬\")\\n\\n\\nif \"messages\" not in st.session_state:\\n    st.session_state[\"messages\"] = []\\n\\nif \"chain\" not in st.session_state:\\n    st.session_state[\"chain\"] = None\\n\\nwith st.sidebar:\\n    clear_btn = st.button(\"대화초기화\")\\n\\n    uploaded_file = st.file_uploader(\"파일 업로드\", type=[\"pdf\"])\\n\\n    # selected_prompt = \"prompts/pdf-rag.yaml\"\\n\\n    selected_model = st.selectbox(\\n        \"model선택\", [\"gpt-4o-mini\", \"gpt-4o\", \"gpt-3.5-turbo\"]\\n    )\\n\\n\\ndef print_messages():\\n    for chat_message in st.session_state[\"messages\"]:\\n        st.chat_message(chat_message.role).write(chat_message.content)\\n\\n\\ndef add_message(role, message):\\n    st.session_state[\"messages\"].append(ChatMessage(role=role, content=message))\\n\\n\\n# 파일을 캐시 저장.(시간이 오래걸리는 작업을 처리할 예정)\\n@st.cache_resource(show_spinner=\"업로드한 파일을 처리중입니다.\")\\ndef embed_file(file):\\n    file_content = file.read()\\n    file_path = f\"./.cache/files/{file.name}\"\\n    with open(file_path, \"wb\") as f:\\n        f.write(file_content)\\n    ############ indexing\\n    loader = PDFPlumberLoader(file_path)\\n    docs = loader.load()\\n\\n    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=50)\\n\\n    documents = text_splitter.split_documents(docs)\\n\\n    embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\\n\\n    vector_store = FAISS.from_documents(documents, embeddings)\\n    retriever = vector_store.as_retriever()\\n    return retriever\\n\\n\\n# 체인 생성\\ndef create_chain(retriever, model_name=\"gpt-4o-mini\"):\\n    prompt = load_prompt(\"prompts/pdf-rag.yaml\", encoding=\"utf-8\")\\n\\n    llm = ChatOpenAI(model=model_name, temperature=0)\\n\\n    output_parser = StrOutputParser()\\n\\n    chain = (\\n        {\"context\": retriever, \"question\": RunnablePassthrough()}\\n        | prompt\\n        | llm\\n        | output_parser\\n    )\\n    return chain\\n\\n\\nif clear_btn:\\n    st.session_state[\"messages\"] = []\\n\\nif uploaded_file:\\n    # 파일 업로드 후 retriever생성. 작업시간이 오래걸릴 예정\\n    retriever = embed_file(uploaded_file)\\n    chain = create_chain(retriever, model_name=selected_model)\\n    st.session_state[\"chain\"] = chain\\n\\nprint_messages()\\n\\nuser_input = st.chat_input(\"메세지를 입력해 주세요\")\\n\\n# 빈 영역을 잡아주는 역할. 경고 메세지를 띄우기 위함\\nwarning_msg = st.empty()\\n\\nif user_input:\\n\\n    # 체인생성\\n    chain = st.session_state[\"chain\"]\\n\\n    if chain is not None:\\n        # 사용자의 입력\\n        st.chat_message(\"user\").write(user_input)\\n        # 스트리밍 호출\\n        response = chain.stream(user_input)\\n\\n        with st.chat_message(\"assistant\"):\\n            container = st.empty()\\n\\n            ai_answer = \"\"\\n            for token in response:\\n                ai_answer += token\\n                container.markdown(ai_answer)\\n\\n        # 대화기록 저장\\n        add_message(\"user\", user_input)\\n        add_message(\"assistant\", ai_answer)\\n\\n    # 파일을 업로드하는 경고메세지\\n    else:\\n        warning_msg.error(\"파일을 업로드 해 주세요\")\\n'),\n",
       " Document(metadata={'source': '..\\\\19.Streamlit\\\\practice\\\\1.py'}, page_content='import streamlit as st\\nfrom dotenv import load_dotenv\\nfrom langchain_openai import ChatOpenAI\\nfrom langchain_core.prompts import PromptTemplate\\nfrom langchain_core.messages import ChatMessage\\nfrom langchain_core.output_parsers import StrOutputParser\\nfrom langchain_teddynote.prompts import load_prompt\\n\\nload_dotenv()\\n\\nst.title(\"뎨방의 GPT\")\\n\\nif \"messages\" not in st.session_state:\\n    st.session_state[\"messages\"] = []\\n\\n\\ndef print_history():\\n    for chat_message in st.session_state[\"messages\"]:\\n        st.chat_message(chat_message.role).write(chat_message.content)\\n\\n\\ndef add_message(role, content):\\n    st.session_state[\"messages\"].append(ChatMessage(role=role, content=content))\\n\\n\\ndef print_history():\\n    for chat_message in st.session_state[\"messages\"]:\\n        st.chat_message(chat_message.role).write(chat_message.content)\\n\\n\\ndef add_message(role, content):\\n    st.session_state[\"messages\"].append(ChatMessage(role=role, conmtent=content))\\n\\n\\nif \"messages\" not in st.session_state:\\n    st.session_state[\"messages\"] = []\\n\\n\\ndef print_history():\\n    for chat_message in st.session_state[\"messages\"]:\\n        st.chat_message(chat_message.role).write(chat_message.content)\\n\\n\\ndef add_message(role, content):\\n    st.session_state[\"messages\"].append(ChatMessage(role=role, content=content))\\n\\n\\ndef create_chain(prompt, model):\\n    model = ChatOpenAI(model=model)\\n\\n    chain = prompt | model | StrOutputParser()\\n    return chain\\n\\n\\nwith st.sidebar:\\n    clear_btn = st.buttion(\"대화내용 초기화\")\\n    tab1, tab2 = st.tabs([\"프롬프트\", \"프리셋\"])\\n    prompt = \"\"\"당신은 친절한 AI 어시스턴트입니다. 사용자의 질문에 대해서 간단히 답해주세요.\"\"\"\\n    user_text_prompt = tab1.text_area(\"프롬프트\", value=prompt)\\n    user_text_apply_btn = tab1.button(\"프롬프트 적용\", key=\"apply1\")\\n\\n    if user_text_apply_btn:\\n        tab1.markdown(f\"르폼르트가 적용되었습니다.\")\\n        prompt_template = user_text_prompt + \"\\\\n\\\\n#Question:\\\\n{question}\\\\n\\\\n#Answer:\"\\n        prompt = PromptTemplate.from_template(prompt_template)\\n        st.session_state[\"chain\"] = create_chain(prompt, \"gpt-4o-mini\")\\n\\n    user_selected_prompt = tab2.selectbox(\"프리셋 선택\", [\"sns\", \"번역\", \"요약\"])\\n    user_selected_apply_btn = tab2.button(\"프롬프트 적용\", key=\"apply2\")\\n    if user_selected_apply_btn:\\n        tab2.markdown(\"프롬프트가 적용되었습니다.\")\\n        prompt = load_prompt(f\"prompts/{user_selected_prompt}.yaml\", encoding=\"utf-8\")\\n        st.session_state[\"chain\"] = create_chain(prompt, \"gpt-4o-mini\")\\n\\nif clear_btn:\\n    retriever = st.session_state[\"messages\"].clear()\\n\\nprint_history\\n'),\n",
       " Document(metadata={'source': '..\\\\19.Streamlit\\\\practice\\\\pdf_rag.py'}, page_content='import os\\nimport streamlit as st\\nfrom dotenv import load_dotenv\\nfrom langchain_community.document_loaders import PDFPlumberLoader\\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\\nfrom langchain_community.vectorstores import FAISS\\nfrom langchain_openai import ChatOpenAI, OpenAIEmbeddings\\nfrom langchain_core.messages import ChatMessage\\nfrom langchain_core.runnables import RunnablePassthrough\\nfrom langchain_core.output_parsers import StrOutputParser\\n\\nfrom langchain_teddynote.prompts import load_prompt\\n\\nload_dotenv()\\n\\n\\n# 캐시 디렉토리 생성\\nif not os.path.exists(\"cache\"):\\n    os.mkdir(\"cache\")\\n\\n# 파일 업로드 전용 폴더\\nif not os.path.exists(\"cache/files\"):\\n    os.mkdir(\"cache/files\")\\n\\nif not os.path.exists(\"cache/embeddings\"):\\n    os.mkdir(\"cache/embeddings\")\\n\\n#########\\nst.title(\"PDF기반 QA\")\\n\\n# 대화기록 저장을 위한 session_state 생성\\nif \"messages\" not in st.session_state:\\n    st.session_state[\"messages\"] = []\\n\\n# 아무런 파일을 업로드 하지 않은 경우\\nif \"chain\" not in st.session_state:\\n    st.session_state[\"chain\"] = None\\n\\n\\n# 출력\\ndef print_messages():\\n    for chat_message in st.session_state[\"messages\"]:\\n        st.chat_message(chat_message.role).write(chat_message.content)\\n\\n\\n# 새로운 메세지 추가\\ndef add_messages(role, content):\\n    st.session_state[\"messages\"].append(ChatMessage(role=role, content=content))\\n\\n\\n# 사이드바 생성\\nwith st.sidebar:\\n    # 초기화 버튼 생성\\n    clear_btn = st.button(\"대화 초기화\")\\n\\n    # 파일 업로드\\n    uploaded_file = st.file_uploader(\"파일 업로드\", type=[\"pdf\"])\\n\\n    # 모델 메뉴 선택\\n    selected_model = st.selectbox(\\n        \"모델 선택\", [\"gpt-4o-mini\", \"gpt-4o\", \"gpt-3.5-turbo\"]\\n    )\\n\\n\\n# 파일을 캐시 저장 + indexing\\n@st.cache_resource(show_spinner=\"업로드한 파일을 처리중입니다\")\\ndef embed_file(file):\\n    file_content = file.read()\\n    file_path = f\"cache/files/{file.name}\"\\n    with open(file_path, \"wb\") as f:\\n        f.write(file_content)\\n\\n    # ----------- Indexing\\n    loader = PDFPlumberLoader(file_path)\\n    docs = loader.load()\\n\\n    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=50)\\n    documents = text_splitter.split_documents(docs)\\n\\n    embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\\n\\n    vector_store = FAISS.from_documents(documents, embeddings)\\n\\n    retriever = vector_store.as_retriever()\\n    return retriever\\n\\n\\n# 체인생성\\ndef create_chain(retriever, model_name=\"gpt-4o-mini\"):\\n\\n    prompt = load_prompt(\"prompts/pdf-rag.yaml\", encoding=\"utf-8\")\\n\\n    llm = ChatOpenAI(model=model_name)\\n\\n    chain = (\\n        {\"context\": retriever, \"question\": RunnablePassthrough()}\\n        | prompt\\n        | llm\\n        | StrOutputParser()\\n    )\\n    return chain\\n\\n\\n# 파일이 업로드 되었을 때,\\nif uploaded_file:\\n    retriever = embed_file(uploaded_file)\\n    chain = create_chain(retriever, model_name=selected_model)\\n    st.session_state[\"chain\"] = chain\\n\\n# 초기화 버튼이 눌렸을 때\\nif clear_btn:\\n    st.session_state[\"messages\"] = []\\n\\n# 이전대화기록 출력\\nprint_messages()\\n\\n# 사용자의 입력\\nuser_input = st.chat_input(\"궁금한 내용을 물어보세요\")\\n\\n# 경고메세지를 띄우기 위한 빈 영역\\nwarning_msg = st.empty()\\n\\n# 만약 사용자의 입력이 들어오면,\\nif user_input:\\n    chain = st.session_state[\"chain\"]\\n\\n    if chain is not None:\\n        # 사용자의 입력\\n        st.chat_message(\"user\").write(user_input)\\n        # 스트리밍 호출\\n        response = chain.stream(\"assistant\")\\n        with st.chat_message(\"assistant\"):\\n            # 빈 컨테이너를 만들어, 이 컨테이너에 토큰을 스트리밍\\n            container = st.empty()\\n\\n            ai_answer = \"\"\\n            for token in response:\\n                ai_answer += token\\n                container.markdown(ai_answer)\\n\\n        # 대화 기록을 저장\\n        add_messages(\"user\", user_input)\\n        add_messages(\"assistant\", ai_answer)\\n\\n    else:\\n        warning_msg.error(\"파일을 업로드 해 주세요\")\\n\\n# if \"messages\" not in st.session_state:\\n#     st.session_state[\"messages\"] = []\\n\\n# if \"chain\" not in st.session_state:\\n#     st.session_state[\"chain\"] = None\\n\\n# def print_messages():\\n#     for chat_message in st.session_state[\"messages\"]:\\n#         st.chat_message(chat_message.role).write(chat_message.content)\\n\\n# def add_messages(role, content):\\n#     st.session_state[\"messages\"].append(ChatMessage(role=role, content=content))\\n\\n# @st.cache_resource(show_spinner=\"업로드한 파일을 처리중입니다.\")\\n# def embed_file(file):\\n#     file_content = file.read()\\n#     file_path = f\"cache/files/{file.name}\"\\n#     with open(file_path, \"wb\") as f:\\n#         f.write(file_content)\\n# \\'wb is write binary, rb is read binary, ab is add binary\\n\\n\\n# if \"messages\" not in st.session_state:\\n#     st.session_state[\"messages\"] = []\\n\\n# if \"chain\" not in st.session_state:\\n#     st.session_state[\"chain\"] = None\\n\\n\\n# def print_messages():\\n#     for chat_message in st.session_state[\"messages\"]:\\n#         st.chat_message(chat_message.role).write(chat_message.content)\\n\\n\\n# def add_messages(role, content):\\n#     st.session_state[\"messages\"].append(ChatMessage(role=role, content=content))\\n\\n\\n# @st.cache_resource(show_spinner=\"업로드한 파일을 처리중입니다.\")\\n# def embed_file(file):\\n#     file_content = file.read()\\n#     file_path = f\"cache/files/{file.name}\"\\n#     with open(file_path, \"wb\") as f:\\n#         f.write(file_content)\\n'),\n",
       " Document(metadata={'source': '..\\\\19.Streamlit\\\\practice\\\\pdf_rag2.py'}, page_content='import os\\nimport streamlit as st\\nfrom dotenv import load_dotenv\\nfrom langchain_core.messages import ChatMessage\\nfrom langchain_community.document_loaders import PyPDFLoader\\nfrom langchain_community.document_loaders import PyPDFLoader\\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\\nfrom langchain_community.vectorstores import FAISS\\nfrom langchain_openai import ChatOpenAI, OpenAIEmbeddings\\nfrom langchain_core.prompts import PromptTemplate\\nfrom langchain_teddynote.prompts import load_prompt\\nfrom langchain_core.runnables import RunnablePassthrough\\nfrom langchain_core.output_parsers import StrOutputParser\\n\\nload_dotenv()\\n\\nif not os.path.exists(\"cache\"):\\n    os.mkdir(\"cache\")\\n\\nif not os.path.exists(\"cache/files\"):\\n    os.mkdir(\"cache/files\")\\n\\nif not os.path.exists(\"cache/embeddings\"):\\n    os.mkdir(\"cache/embeddings\")\\n\\nst.title(\"pdf기반 qa\")\\n\\n# 대화기록 저장을 위한 session_state\\nif \"messages\" not in st.session_state:\\n    st.session_state[\"messages\"] = []\\n\\nif \"chain\" not in st.session_state:\\n    st.session_state[\"chain\"] = None\\n\\n\\ndef print_messages():\\n    for chat_message in st.session_state[\"messages\"]:\\n        st.chat_message(chat_message.role).write(chat_message.content)\\n\\n\\ndef add_messages(role, content):\\n    st.session_state[\"messages\"].append(ChatMessage(role=role, content=content))\\n\\n\\nwith st.sidebar:\\n    # 초기화버튼\\n    clear_btn = st.button(\"대화 초기화\")\\n\\n    # 파일 업로드\\n    uploaded_file = st.file_uploader(\"파일 업로드\", type=[\"pdf\"])\\n\\n    # 모델 메뉴 선택\\n    selected_model = st.selectbox(\\n        \"모델선택\", [\"gpt-4o-mini\", \"gpt-4o\", \"gpt-3.5-turbo\"]\\n    )\\n\\n\\n# 파일 업로드 및 인덱싱\\n@st.cache_resource(show_spinner=\"업로드한 파일을 처리중 입니다.\")\\ndef embed_file(file):\\n    file_content = file.read()\\n    file_path = f\"cache/files/{file.name}\"\\n    with open(file_path, \"wb\") as f:\\n        f.write(file_content)\\n\\n    loader = PyPDFLoader(file_path)\\n    docs = loader.load()\\n\\n    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=50)\\n    documents = text_splitter.split_documents(docs)\\n\\n    embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\\n\\n    vector_store = FAISS.from_documents(documents, embeddings)\\n    retriever = vector_store.as_retriever()\\n    return retriever\\n\\n\\ndef create_chain(retriever, model_name=\"gpt-4o-mini\"):\\n    prompt = load_prompt(\"prompts/pdf-rag.yaml\", encoding=\"utf-8\")\\n\\n    llm = ChatOpenAI(model=model_name, temperature=0)\\n\\n    chain = (\\n        {\"context\": retriever, \"question\": RunnablePassthrough()}\\n        | prompt\\n        | llm\\n        | StrOutputParser()\\n    )\\n\\n    return chain\\n\\n\\n#########################################\\n# 파일이 업로드 되면\\nif uploaded_file:\\n    retriever = embed_file(uploaded_file)\\n    chain = create_chain(retriever, model_name=selected_model)\\n    # chain 객체를 st.session_state[\"chain\"]에 저장\\n    st.session_state[\"chain\"] = chain\\n\\n# 대화 초기화 버튼을 누르면\\nif clear_btn:\\n    st.session_state[\"messages\"] = []\\n\\nprint_messages()\\n\\nuser_input = st.chat_input(\"궁금한 내용을 물어보세요.\")\\n\\n# 경고 메세지를 띄우기 위한 빈 영역\\nwarning_msg = st.empty()\\n\\nif user_input:\\n    # 체인생성\\n    chain = st.session_state[\"chain\"]\\n\\n    if chain is not None:\\n        # 사용자의 입력\\n        st.chat_message(\"user\").write(user_input)\\n        # 스트리밍 호출\\n        response = chain.stream(user_input)\\n\\n        with st.chat_message(\"assistant\"):\\n            container = st.empty()\\n\\n            ai_answer = \"\"\\n            for token in response:\\n                ai_answer += token\\n                container.markdown(ai_answer)\\n\\n        add_messages(\"user\", user_input)\\n        add_messages(\"assistant\", ai_answer)\\n\\n    else:\\n        warning_msg.error(\"파일을 업로드 해주세요\")\\n'),\n",
       " Document(metadata={'source': '..\\\\19.Streamlit\\\\practice\\\\pdf_rag3.py'}, page_content='import os\\nimport streamlit as st\\nfrom dotenv import load_dotenv\\nfrom langchain_core.messages import ChatMessage\\nfrom langchain_community.document_loaders import PyPDFLoader\\nfrom langchain_community.document_loaders import PyPDFLoader\\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\\nfrom langchain_community.vectorstores import FAISS\\nfrom langchain_openai import ChatOpenAI, OpenAIEmbeddings\\nfrom langchain_core.prompts import PromptTemplate\\nfrom langchain_teddynote.prompts import load_prompt\\nfrom langchain_core.runnables import RunnablePassthrough\\nfrom langchain_core.output_parsers import StrOutputParser\\n\\nload_dotenv()\\n\\nif not os.path.exists(\"cache\"):\\n    os.mkdir(\"cache\")\\n\\nif not os.path.exists(\"cache/files\"):\\n    os.mkdir(\"cache/files\")\\n\\nif not os.path.exists(\"cache/embeddings\"):\\n    os.mkdir(\"cache/embeddings\")\\n\\nst.title(\"pdf기반 qa\")\\n\\n# 대화기록 저장을 위한 session_state\\nif \"messages\" not in st.session_state:\\n    st.session_state[\"messages\"] = []\\n\\nif \"chain\" not in st.session_state:\\n    st.session_state[\"chain\"] = None\\n\\n\\ndef print_messages():\\n    for chat_message in st.session_state[\"messages\"]:\\n        st.chat_message(chat_message.role).write(chat_message.content)\\n\\n\\ndef add_messages(role, content):\\n    st.session_state[\"messages\"].append(ChatMessage(role=role, content=content))\\n\\n\\nwith st.sidebar:\\n    # 초기화버튼\\n    clear_btn = st.button(\"대화 초기화\")\\n\\n    # 파일 업로드\\n    uploaded_file = st.file_uploader(\"파일 업로드\", type=[\"pdf\"])\\n\\n    # 모델 메뉴 선택\\n    selected_model = st.selectbox(\\n        \"모델선택\", [\"gpt-4o-mini\", \"gpt-4o\", \"gpt-3.5-turbo\"]\\n    )\\n\\n\\n# 파일 업로드 및 인덱싱\\n@st.cache_resource(show_spinner=\"업로드한 파일을 처리중 입니다.\")\\ndef embed_file(file):\\n    file_content = file.read()\\n    file_path = f\"cache/files/{file.name}\"\\n    with open(file_path, \"wb\") as f:\\n        f.write(file_content)\\n\\n    loader = PyPDFLoader(file_path)\\n    docs = loader.load()\\n\\n    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=50)\\n    documents = text_splitter.split_documents(docs)\\n\\n    embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\\n\\n    vector_store = FAISS.from_documents(documents, embeddings)\\n    retriever = vector_store.as_retriever()\\n    return retriever\\n\\n\\ndef create_chain(retriever, model_name=\"gpt-4o-mini\"):\\n    prompt = load_prompt(\"prompts/pdf-rag.yaml\", encoding=\"utf-8\")\\n\\n    llm = ChatOpenAI(model=model_name, temperature=0)\\n\\n    chain = (\\n        {\"context\": retriever, \"question\": RunnablePassthrough()}\\n        | prompt\\n        | llm\\n        | StrOutputParser()\\n    )\\n\\n    return chain\\n\\n\\n#########################################\\n# 파일이 업로드 되면\\nif uploaded_file:\\n    retriever = embed_file(uploaded_file)\\n    chain = create_chain(retriever, model_name=selected_model)\\n    # chain 객체를 st.session_state[\"chain\"]에 저장\\n    st.session_state[\"chain\"] = chain\\n\\n# 대화 초기화 버튼을 누르면\\nif clear_btn:\\n    st.session_state[\"messages\"] = []\\n\\nprint_messages()\\n\\nuser_input = st.chat_input(\"궁금한 내용을 물어보세요.\")\\n\\n# 경고 메세지를 띄우기 위한 빈 영역\\nwarning_msg = st.empty()\\n\\nif user_input:\\n    # 체인생성\\n    chain = st.session_state[\"chain\"]\\n\\n    if chain is not None:\\n        # 사용자의 입력\\n        st.chat_message(\"user\").write(user_input)\\n        # 스트리밍 호출\\n        response = chain.stream(user_input)\\n\\n        with st.chat_message(\"assistant\"):\\n            container = st.empty()\\n\\n            ai_answer = \"\"\\n            for token in response:\\n                ai_answer += token\\n                container.markdown(ai_answer)\\n\\n        add_messages(\"user\", user_input)\\n        add_messages(\"assistant\", ai_answer)\\n\\n    else:\\n        warning_msg.error(\"파일을 업로드 해주세요\")\\n'),\n",
       " Document(metadata={'source': '..\\\\31.Agent-CrewAI\\\\coding\\\\extract_keywords.py'}, page_content='# filename: extract_keywords.py\\nimport nltk\\nfrom nltk.tokenize import word_tokenize\\nfrom nltk.corpus import stopwords\\n\\n# NLTK 리소스 다운로드 (처음 한 번만 실행)\\nnltk.download(\\'punkt\\')\\nnltk.download(\\'stopwords\\')\\n\\n# 예시 문장\\nsentence = \"Artificial intelligence is transforming the way we live and work.\"\\n\\n# 단어 토큰화\\nwords = word_tokenize(sentence)\\n\\n# 불용어 제거\\nstop_words = set(stopwords.words(\\'english\\'))\\nkeywords = [word for word in words if word.isalnum() and word.lower() not in stop_words]\\n\\nprint(keywords)'),\n",
       " Document(metadata={'source': '..\\\\31.Agent-CrewAI\\\\coding\\\\find_primes.py'}, page_content='# filename: find_primes.py\\ndef is_prime(n):\\n    if n <= 1:\\n        return False\\n    for i in range(2, int(n**0.5) + 1):\\n        if n % i == 0:\\n            return False\\n    return True\\n\\nprimes = [num for num in range(1, 101) if is_prime(num)]\\nprint(primes)'),\n",
       " Document(metadata={'source': '..\\\\32-Autogen\\\\coding\\\\extract_keywords.py'}, page_content='# filename: extract_keywords.py\\nimport nltk\\nfrom nltk.tokenize import word_tokenize\\nfrom nltk.corpus import stopwords\\n\\n# NLTK 리소스 다운로드\\nnltk.download(\\'punkt\\')\\nnltk.download(\\'stopwords\\')\\n\\nsentence = \"The quick brown fox jumps over the lazy dog.\"\\ntokens = word_tokenize(sentence)\\nstop_words = set(stopwords.words(\\'english\\'))\\n\\nkeywords = [word for word in tokens if word.isalnum() and word.lower() not in stop_words]\\nprint(keywords)'),\n",
       " Document(metadata={'source': '..\\\\32-Autogen\\\\coding\\\\extract_keywords_new.py'}, page_content='# filename: extract_keywords_new.py\\nimport nltk\\nfrom nltk.tokenize import word_tokenize\\nfrom nltk.corpus import stopwords\\n\\n# NLTK 리소스 다운로드 (이미 다운로드된 경우 주석 처리)\\n# nltk.download(\\'punkt\\')\\n# nltk.download(\\'stopwords\\')\\n\\nsentence = \"Artificial intelligence is transforming the way we live and work.\"\\ntokens = word_tokenize(sentence)\\nstop_words = set(stopwords.words(\\'english\\'))\\n\\nkeywords = [word for word in tokens if word.isalnum() and word.lower() not in stop_words]\\nprint(keywords)'),\n",
       " Document(metadata={'source': '..\\\\32-Autogen\\\\coding\\\\find_primes.py'}, page_content='# filename: find_primes.py\\ndef is_prime(n):\\n    if n <= 1:\\n        return False\\n    for i in range(2, int(n**0.5) + 1):\\n        if n % i == 0:\\n            return False\\n    return True\\n\\nprimes = [num for num in range(1, 101) if is_prime(num)]\\nprint(primes)'),\n",
       " Document(metadata={'source': '..\\\\32-Autogen\\\\coding\\\\install_and_extract_keywords.py'}, page_content='# filename: install_and_extract_keywords.py\\nimport subprocess\\nimport sys\\n\\n# NLTK 설치\\nsubprocess.check_call([sys.executable, \\'-m\\', \\'pip\\', \\'install\\', \\'nltk\\'])\\n\\nimport nltk\\nnltk.download(\\'punkt\\')\\nnltk.download(\\'stopwords\\')\\n\\nfrom nltk.tokenize import word_tokenize\\nfrom nltk.corpus import stopwords\\n\\nsentence = \"The quick brown fox jumps over the lazy dog.\"\\ntokens = word_tokenize(sentence)\\nstop_words = set(stopwords.words(\\'english\\'))\\n\\nkeywords = [word for word in tokens if word.isalnum() and word.lower() not in stop_words]\\nprint(keywords)'),\n",
       " Document(metadata={'source': '..\\\\32-Autogen\\\\group_chat\\\\download_and_check.py'}, page_content='# filename: download_and_check.py\\nimport pandas as pd\\n\\n# 데이터 다운로드\\nurl = \"https://github.com/mwaskom/seaborn-data/raw/master/titanic.csv\"\\ndata = pd.read_csv(url)\\n\\n# 데이터셋의 열 출력\\nprint(data.columns.tolist())'),\n",
       " Document(metadata={'source': '..\\\\32-Autogen\\\\group_chat\\\\download_and_plot.py'}, page_content='# filename: download_and_plot.py\\nimport pandas as pd\\nimport matplotlib.pyplot as plt\\n\\n# 데이터 다운로드\\nurl = \"https://github.com/mwaskom/seaborn-data/raw/master/titanic.csv\"\\ndata = pd.read_csv(url)\\n\\n# 데이터셋의 열 출력\\nprint(data.columns.tolist())\\n\\n# \\'age\\'와 \\'pclass\\' 간의 관계 시각화\\nplt.figure(figsize=(10, 6))\\nplt.scatter(data[\\'pclass\\'], data[\\'age\\'], alpha=0.5)\\nplt.title(\\'Relationship between Age and Passenger Class\\')\\nplt.xlabel(\\'Passenger Class (pclass)\\')\\nplt.ylabel(\\'Age\\')\\nplt.grid(True)\\n\\n# 차트 파일로 저장\\nplt.savefig(\\'age_pclass_relationship.png\\')\\nplt.close()'),\n",
       " Document(metadata={'source': '..\\\\32-Autogen\\\\group_chat\\\\download_and_plot_with_check.py'}, page_content='# filename: download_and_plot_with_check.py\\ntry:\\n    import pandas as pd\\n    import matplotlib.pyplot as plt\\nexcept ImportError as e:\\n    print(\"필요한 라이브러리가 설치되지 않았습니다. 다음 명령어를 사용하여 설치하세요:\")\\n    print(\"pip install pandas matplotlib\")\\n    raise e\\n\\n# 데이터 다운로드\\nurl = \"https://github.com/mwaskom/seaborn-data/raw/master/titanic.csv\"\\ndata = pd.read_csv(url)\\n\\n# 데이터셋의 열 출력\\nprint(data.columns.tolist())\\n\\n# \\'age\\'와 \\'pclass\\' 간의 관계 시각화\\nplt.figure(figsize=(10, 6))\\nplt.scatter(data[\\'pclass\\'], data[\\'age\\'], alpha=0.5)\\nplt.title(\\'Relationship between Age and Passenger Class\\')\\nplt.xlabel(\\'Passenger Class (pclass)\\')\\nplt.ylabel(\\'Age\\')\\nplt.grid(True)\\n\\n# 차트 파일로 저장\\nplt.savefig(\\'age_pclass_relationship.png\\')\\nplt.close()'),\n",
       " Document(metadata={'source': '..\\\\32-Autogen\\\\group_chat\\\\download_and_plot_with_enhanced_check.py'}, page_content='# filename: download_and_plot_with_enhanced_check.py\\nimport subprocess\\nimport sys\\n\\n# 필요한 라이브러리 설치\\ntry:\\n    import pandas as pd\\n    import matplotlib.pyplot as plt\\nexcept ImportError:\\n    print(\"필요한 라이브러리가 설치되지 않았습니다. 다음 명령어를 사용하여 설치하세요:\")\\n    print(\"pip install pandas matplotlib\")\\n    sys.exit(1)\\n\\n# 데이터 다운로드\\nurl = \"https://github.com/mwaskom/seaborn-data/raw/master/titanic.csv\"\\ndata = pd.read_csv(url)\\n\\n# 데이터셋의 열 출력\\nprint(data.columns.tolist())\\n\\n# \\'age\\'와 \\'pclass\\' 간의 관계 시각화\\nplt.figure(figsize=(10, 6))\\nplt.scatter(data[\\'pclass\\'], data[\\'age\\'], alpha=0.5)\\nplt.title(\\'Relationship between Age and Passenger Class\\')\\nplt.xlabel(\\'Passenger Class (pclass)\\')\\nplt.ylabel(\\'Age\\')\\nplt.grid(True)\\n\\n# 차트 파일로 저장\\nplt.savefig(\\'age_pclass_relationship.png\\')\\nplt.close()\\n\\nprint(\"차트가 \\'age_pclass_relationship.png\\'로 저장되었습니다.\")'),\n",
       " Document(metadata={'source': '..\\\\32-Autogen\\\\group_chat\\\\download_and_plot_with_final_check.py'}, page_content='# filename: download_and_plot_with_final_check.py\\nimport subprocess\\nimport sys\\n\\ntry:\\n    import pandas as pd\\n    import matplotlib.pyplot as plt\\nexcept ImportError:\\n    print(\"필요한 라이브러리가 설치되지 않았습니다. 다음 명령어를 사용하여 설치하세요:\")\\n    print(\"pip install pandas matplotlib\")\\n    sys.exit(1)\\n\\n# 데이터 다운로드\\nurl = \"https://github.com/mwaskom/seaborn-data/raw/master/titanic.csv\"\\ndata = pd.read_csv(url)\\n\\n# 데이터셋의 열 출력\\nprint(data.columns.tolist())\\n\\n# \\'age\\'와 \\'pclass\\' 간의 관계 시각화\\nplt.figure(figsize=(10, 6))\\nplt.scatter(data[\\'pclass\\'], data[\\'age\\'], alpha=0.5)\\nplt.title(\\'Relationship between Age and Passenger Class\\')\\nplt.xlabel(\\'Passenger Class (pclass)\\')\\nplt.ylabel(\\'Age\\')\\nplt.grid(True)\\n\\n# 차트 파일로 저장\\nplt.savefig(\\'age_pclass_relationship.png\\')\\nplt.close()\\n\\nprint(\"차트가 \\'age_pclass_relationship.png\\'로 저장되었습니다.\")'),\n",
       " Document(metadata={'source': '..\\\\32-Autogen\\\\group_chat\\\\tmp_code_eb0c39b18ee354c7f3b81f78817036ce.py'}, page_content='     import pandas as pd\\n     import matplotlib.pyplot as plt\\n     print(pd.__version__)\\n     print(plt.__version__)'),\n",
       " Document(metadata={'source': '..\\\\langchain-kr\\\\07-DocumentLoader\\\\data\\\\audio_utils.py'}, page_content='import re\\nimport os\\nfrom pytube import YouTube\\nfrom moviepy.editor import AudioFileClip, VideoFileClip\\nfrom pydub import AudioSegment\\nfrom pydub.silence import detect_nonsilent\\n\\n\\ndef extract_abr(abr):\\n    youtube_audio_pattern = re.compile(r\"\\\\d+\")\\n    kbps = youtube_audio_pattern.search(abr)\\n    if kbps:\\n        kbps = kbps.group()\\n        return int(kbps)\\n    else:\\n        return 0\\n\\n\\ndef get_audio_filepath(filename):\\n    # audio 폴더가 없으면 생성\\n    if not os.path.isdir(\"audio\"):\\n        os.mkdir(\"audio\")\\n\\n    # 현재 스크립트의 절대 경로 얻기\\n    current_directory = os.path.abspath(\"\")\\n\\n    # 파일 경로 생성\\n    audio_file_path = os.path.join(current_directory, \"audio\", filename)\\n\\n    return audio_file_path\\n\\n\\ndef convert_mp4_to_wav(mp4_file_path, wav_file_path):\\n    # MP4 파일 로드\\n    audio_clip = AudioFileClip(mp4_file_path)\\n\\n    # WAV 형식으로 오디오 추출 및 저장\\n    audio_clip.write_audiofile(wav_file_path, fps=44100, nbytes=2, codec=\"pcm_s16le\")\\n\\n\\ndef download_audio_from_youtube(link):\\n    # YouTube 객체 생성\\n    yt = YouTube(link)\\n\\n    # mp4 오디오만 필터링\\n    mp4_files = dict()\\n\\n    # \"audio/mp4\" 타입의 스트림만 필터링\\n    for stream in yt.streams.filter(only_audio=True):\\n        mime_type = stream.mime_type\\n        abr = stream.abr\\n        if mime_type == \"audio/mp4\":\\n            abr = extract_abr(abr)\\n            mp4_files[abr] = stream\\n\\n    # 키를 기준으로 정렬\\n    sorted_keys = sorted(mp4_files.keys())\\n    # 가장 큰 키를 사용하여 값 가져오기\\n    largest_value = mp4_files[sorted_keys[-1]]\\n    filename = largest_value.download()\\n\\n    # 현재 스크립트의 절대 경로 얻기\\n    current_directory = os.path.abspath(\"\")\\n\\n    new_filename = os.path.basename(filename.replace(\".mp4\", \".wav\"))\\n\\n    new_filepath = os.path.join(current_directory, \"audio\", new_filename)\\n\\n    # mp4 파일을 wav 파일로 변환\\n    convert_mp4_to_wav(filename, new_filepath)\\n\\n    # mp4 파일 삭제\\n    os.remove(filename)\\n    return new_filepath\\n\\n\\ndef extract_audio_from_video(video_filepath):\\n    # MP4 파일 로드\\n    video = VideoFileClip(video_filepath)\\n    audio_filepath = get_audio_filepath(video_filepath.replace(\".mp4\", \".wav\"))\\n    video.audio.write_audiofile(audio_filepath)\\n    return audio_filepath\\n\\n\\nclass AudioChunk:\\n    def __init__(self, filepath, min_silence_len=350, silence_thresh=-35):\\n        self.audio = AudioSegment.from_file(filepath, format=\"wav\")\\n        self.filepath = filepath\\n        self.min_silence_len = min_silence_len\\n        self.silence_thresh = silence_thresh\\n        self.detect_nonsilent_from_audio()\\n\\n    @staticmethod\\n    def make_audio_chunks(audio, non_silent_times):\\n        audio_chunks = []\\n        for start, end in non_silent_times:\\n            audio_chunks.append((audio[start:end], start, end))\\n        return audio_chunks\\n\\n    def detect_nonsilent_from_audio(self):\\n        non_silent_audio_times = detect_nonsilent(\\n            self.audio,\\n            min_silence_len=self.min_silence_len,\\n            silence_thresh=self.silence_thresh,\\n        )\\n\\n        non_silent_audios_output = AudioSegment.empty()\\n\\n        for i in range(len(non_silent_audio_times)):\\n            non_silent_audios_output += self.audio[\\n                non_silent_audio_times[i][0] : non_silent_audio_times[i][1]\\n            ]\\n        self.audio_chunks = self.make_audio_chunks(self.audio, non_silent_audio_times)\\n        self.non_silent_audios_output = non_silent_audios_output\\n        print(f\"분석에 사용할 전체 오디오 조각 개수: {len(non_silent_audio_times)}\")\\n\\n    def audio_splits(self, split_time=100):\\n        splits = int(self.audio.duration_seconds // split_time + 1)\\n        audios = []\\n        for s in range(splits):\\n            start = s * split_time * 1000\\n            end = start + split_time * 1000\\n            audios.append(self.audio[start:end])\\n        return audios\\n'),\n",
       " Document(metadata={'source': '..\\\\langchain-kr\\\\12-RAG\\\\data\\\\audio_utils.py'}, page_content='import re\\nimport os\\nfrom pytube import YouTube\\nfrom moviepy.editor import AudioFileClip, VideoFileClip\\nfrom pydub import AudioSegment\\nfrom pydub.silence import detect_nonsilent\\n\\n\\ndef extract_abr(abr):\\n    youtube_audio_pattern = re.compile(r\"\\\\d+\")\\n    kbps = youtube_audio_pattern.search(abr)\\n    if kbps:\\n        kbps = kbps.group()\\n        return int(kbps)\\n    else:\\n        return 0\\n\\n\\ndef get_audio_filepath(filename):\\n    # audio 폴더가 없으면 생성\\n    if not os.path.isdir(\"audio\"):\\n        os.mkdir(\"audio\")\\n\\n    # 현재 스크립트의 절대 경로 얻기\\n    current_directory = os.path.abspath(\"\")\\n\\n    # 파일 경로 생성\\n    audio_file_path = os.path.join(current_directory, \"audio\", filename)\\n\\n    return audio_file_path\\n\\n\\ndef convert_mp4_to_wav(mp4_file_path, wav_file_path):\\n    # MP4 파일 로드\\n    audio_clip = AudioFileClip(mp4_file_path)\\n\\n    # WAV 형식으로 오디오 추출 및 저장\\n    audio_clip.write_audiofile(wav_file_path, fps=44100, nbytes=2, codec=\"pcm_s16le\")\\n\\n\\ndef download_audio_from_youtube(link):\\n    # YouTube 객체 생성\\n    yt = YouTube(link)\\n\\n    # mp4 오디오만 필터링\\n    mp4_files = dict()\\n\\n    # \"audio/mp4\" 타입의 스트림만 필터링\\n    for stream in yt.streams.filter(only_audio=True):\\n        mime_type = stream.mime_type\\n        abr = stream.abr\\n        if mime_type == \"audio/mp4\":\\n            abr = extract_abr(abr)\\n            mp4_files[abr] = stream\\n\\n    # 키를 기준으로 정렬\\n    sorted_keys = sorted(mp4_files.keys())\\n    # 가장 큰 키를 사용하여 값 가져오기\\n    largest_value = mp4_files[sorted_keys[-1]]\\n    filename = largest_value.download()\\n\\n    # 현재 스크립트의 절대 경로 얻기\\n    current_directory = os.path.abspath(\"\")\\n\\n    new_filename = os.path.basename(filename.replace(\".mp4\", \".wav\"))\\n\\n    new_filepath = os.path.join(current_directory, \"audio\", new_filename)\\n\\n    # mp4 파일을 wav 파일로 변환\\n    convert_mp4_to_wav(filename, new_filepath)\\n\\n    # mp4 파일 삭제\\n    os.remove(filename)\\n    return new_filepath\\n\\n\\ndef extract_audio_from_video(video_filepath):\\n    # MP4 파일 로드\\n    video = VideoFileClip(video_filepath)\\n    audio_filepath = get_audio_filepath(video_filepath.replace(\".mp4\", \".wav\"))\\n    video.audio.write_audiofile(audio_filepath)\\n    return audio_filepath\\n\\n\\nclass AudioChunk:\\n    def __init__(self, filepath, min_silence_len=350, silence_thresh=-35):\\n        self.audio = AudioSegment.from_file(filepath, format=\"wav\")\\n        self.filepath = filepath\\n        self.min_silence_len = min_silence_len\\n        self.silence_thresh = silence_thresh\\n        self.detect_nonsilent_from_audio()\\n\\n    @staticmethod\\n    def make_audio_chunks(audio, non_silent_times):\\n        audio_chunks = []\\n        for start, end in non_silent_times:\\n            audio_chunks.append((audio[start:end], start, end))\\n        return audio_chunks\\n\\n    def detect_nonsilent_from_audio(self):\\n        non_silent_audio_times = detect_nonsilent(\\n            self.audio,\\n            min_silence_len=self.min_silence_len,\\n            silence_thresh=self.silence_thresh,\\n        )\\n\\n        non_silent_audios_output = AudioSegment.empty()\\n\\n        for i in range(len(non_silent_audio_times)):\\n            non_silent_audios_output += self.audio[\\n                non_silent_audio_times[i][0] : non_silent_audio_times[i][1]\\n            ]\\n        self.audio_chunks = self.make_audio_chunks(self.audio, non_silent_audio_times)\\n        self.non_silent_audios_output = non_silent_audios_output\\n        print(f\"분석에 사용할 전체 오디오 조각 개수: {len(non_silent_audio_times)}\")\\n\\n    def audio_splits(self, split_time=100):\\n        splits = int(self.audio.duration_seconds // split_time + 1)\\n        audios = []\\n        for s in range(splits):\\n            start = s * split_time * 1000\\n            end = start + split_time * 1000\\n            audios.append(self.audio[start:end])\\n        return audios\\n'),\n",
       " Document(metadata={'source': '..\\\\langchain-kr\\\\16-Use-Cases\\\\telegram-langchain-bot\\\\telegram-langchain-bot.py'}, page_content='import os\\nfrom dotenv import load_dotenv\\nfrom langchain_text_splitters import Language\\nfrom langchain_community.document_loaders.generic import GenericLoader\\nfrom langchain_community.document_loaders.parsers import LanguageParser\\nfrom langchain_community.document_loaders import TextLoader\\nfrom langchain_community.document_loaders import PyPDFLoader\\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\\n\\nload_dotenv()\\n\\n# Python 파일을 로드하고 문서를 분할합니다.\\nrepo_root = \"/home/hellocosmos/telegram-bot/langchain/libs\"\\nrepo_core = repo_root + \"/core/langchain_core\"\\nrepo_community = repo_root + \"/community/langchain_community\"\\nrepo_experimental = repo_root + \"/experimental/langchain_experimental\"\\nrepo_partners = repo_root + \"/partners\"\\nrepo_text_splitter = repo_root + \"/text_splitters/langchain_text_splitters\"\\nrepo_cookbook = repo_root + \"/cookbook\"\\n\\npy_documents = []\\nfor path in [repo_core, repo_community, repo_experimental, repo_partners, repo_cookbook]:\\n    loader = GenericLoader.from_filesystem(\\n        path, glob=\"**/*\", suffixes=[\".py\"],\\n        parser=LanguageParser(language=Language.PYTHON, parser_threshold=30),\\n    )\\n    py_documents.extend(loader.load())\\nprint(f\".py 파일의 개수: {len(py_documents)}\")\\n\\npy_splitter = RecursiveCharacterTextSplitter.from_language(\\n    language=Language.PYTHON, chunk_size=2000, chunk_overlap=200\\n)\\npy_docs = py_splitter.split_documents(py_documents)\\nprint(f\"분할된 .py 파일의 개수: {len(py_docs)}\")\\n\\n# MDX 파일을 로드하고 문서를 분할합니다.\\nroot_dir = \"/home/hellocosmos/telegram-bot/langchain/\"\\n\\nmdx_documents = []\\nfor dirpath, dirnames, filenames in os.walk(root_dir):\\n    for file in filenames:\\n        if (file.endswith(\".mdx\")) and \"*venv/\" not in dirpath:\\n            try:\\n                loader = TextLoader(os.path.join(dirpath, file), encoding=\"utf-8\")\\n                mdx_documents.extend(loader.load())\\n            except Exception:\\n                pass\\nprint(f\".mdx 파일의 개수: {len(mdx_documents)}\")\\n\\nmdx_splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=200)\\nmdx_docs = mdx_splitter.split_documents(mdx_documents)\\nprint(f\"분할된 .mdx 파일의 개수: {len(mdx_docs)}\")\\n\\n# Teddy님의 랭체인노트를 로드하고 문서를 분할합니다.\\nimport pandas as pd\\nfrom langchain.schema import Document\\n\\ndf = pd.read_csv(\\'data_list_with_content.csv\\')\\ndf_splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=200)\\nteddy_docs = []\\nfor index, row in df.iterrows():\\n    if pd.isna(row[\\'content\\']):\\n        continue\\n    chunks = df_splitter.split_text(row[\\'content\\'])\\n    for chunk in chunks:\\n        teddy_docs.append(Document(page_content=chunk, metadata={\"title\": row[\\'title\\'], \"source\": row[\\'source\\']}))\\nprint(f\"분할된 .df 파일 개수: {len(teddy_docs)}\")\\n\\n# PDF 파일로드 및 텍스트 분할합니다. (PDF 파일은 유료구매하셔야 합니다)\\npdf_docs = []\\ndocument = PyPDFLoader(\"data/Generative_Al_with_LangChain.pdf\").load_and_split()\\npdf_splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=200)\\npdf_docs = pdf_splitter.split_documents(document)\\nprint(f\"분할된 .pdf 파일의 개수: {len(pdf_docs)}\")\\n\\n# 파이썬 문서, MDX 문서, PDF 문서, 테디노트(Langhchin-KR) 문서를 결합합니다.\\ncombined_documents = teddy_docs + py_docs + mdx_docs + pdf_docs\\nprint(f\"총 도큐먼트 개수: {len(combined_documents)}\")\\n\\n# 필요한 임베딩과 캐싱설정을 수행합니다. \\nfrom langchain_openai import OpenAIEmbeddings\\nfrom langchain.embeddings import CacheBackedEmbeddings\\nfrom langchain.storage import LocalFileStore\\n\\nstore = LocalFileStore(\"./cache/\")\\nembeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\", disallowed_special=())\\ncached_embeddings = CacheBackedEmbeddings.from_bytes_store(embeddings, store, namespace=embeddings.model)\\n\\n# Kiwi Tokenizer를 설정합니다.\\nfrom kiwipiepy import Kiwi\\n\\nkiwi = Kiwi()\\ndef kiwi_tokenize(text):\\n    return [token.form for token in kiwi.tokenize(text)]\\n\\n# FAISS 클래스를 가져와 검색 모델 인스턴스를 생성합니다.\\nfrom langchain_community.vectorstores import FAISS, Chroma\\n\\nFAISS_DB_INDEX = \"langchain_faiss\"\\ndb = FAISS.from_documents(combined_documents, cached_embeddings)\\ndb.save_local(folder_path=FAISS_DB_INDEX)\\ndb = FAISS.load_local(FAISS_DB_INDEX, cached_embeddings, allow_dangerous_deserialization=True)\\nfaiss_retriever = db.as_retriever(search_type=\"mmr\", search_kwargs={\"k\": 10})\\n\\n# BM25Retriever 클래스를 가져와 검색 모델 인스턴스를 생성합니다.\\nfrom langchain_community.retrievers import BM25Retriever\\n\\nkiwi_bm25_retriever = BM25Retriever.from_documents(combined_documents, preprocess_func=kiwi_tokenize, k=10)\\n\\n# EnsembleRetriever 클래스를 사용하여 검색 모델을 결합하여 사용합니다.\\nfrom langchain.retrievers import EnsembleRetriever\\n\\nensemble_retriever = EnsembleRetriever(\\n    retrievers=[kiwi_bm25_retriever, faiss_retriever],\\n    weights=[0.7, 0.3], search_type=\"mmr\",\\n)\\n\\n# PromptTemplate을 생성하여 프롬프트를 설정합니다.\\nfrom langchain_core.prompts import PromptTemplate\\n\\nprompt = PromptTemplate.from_template(\\n\"\"\"\\n당신은 20년차 AI 개발자이자 파이썬 전문가입니다. 당신의 임무는 주어진 질문에 대하여 최대한 문서의 정보를 활용하여 답변하는 것입니다. 아래의 숫자가 적힌 순서대로 절차를 지켜서 단계적으로 생각하고 진행하세요.\\n\\n1.주어진 문서에 기반하여 답변하는 경우, \"문서를 기반으로 답변드리겠습니다\" 라고 시작한다. Python 코드에 대한 상세한 code snippet을 포함해야 하며, 코드 설명에 대한 주석도 작성해주세요. 답변은 자세하게 설명하고, 한글로 작성해 주세요. 주어진 문서에서 정보를 찾아 답변하는 경우 출처(source)를 반드시 표기해야 합니다. 출처는 절대경로로 출력되는 경우 \"/home/hellocosmos/telegram-bot\"은 생략하고 출력해주세요. 출처가 PDF 파일인 경우 \"출처 소스, 페이지\"를 형식으로 표기해야 합니다. 메타데이터의 title이 빈공백이 아닌 경우 반드시 \"title, source\" 형식으로 표기해야 합니다.\\n2.주어진 문서에 기반해 답변을 찾을 수 없는 경우에는 AI, Langchain 및 파이썬 전문가로써 당신이 알고 있는 관련 지식만을 활용해야 합니다. \"문서에 관련 정보가 없지만, 알고 있는 지식을 활용해 답변드리겠습니다.\"라고 시작한다. 최대한 자세하게 답변하고, 출처는 당신이 알고 있는 출처를 표기해주세요.\\n3.주어진 문서에 기반해 답변을 찾을 수 없고, Langchain 및 파이썬 전문가로써 당신이 알고 있는 관련 지식만을 활용해도 답변을 찾을 수 없습니다. 이 경우에는 \"AI 및 Langchain에 대해 문의해주세요 😂\"라고 답변해야 하며, 출처는 생략 해주세요.\\n\\n#참고문서:\\n{context}\\n\\n#질문:\\n{question}\\n\\n#답변: \\n\\n#출처:\\n- source1\\n- source2\\n- ...\\n\"\"\"\\n)\\n\\nfrom langchain.callbacks.base import BaseCallbackHandler\\nfrom langchain_core.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\\nfrom langchain_core.callbacks.manager import CallbackManager\\nfrom langchain_core.output_parsers import StrOutputParser\\nfrom langchain_core.runnables import RunnablePassthrough\\nfrom langchain_openai import ChatOpenAI\\n\\nclass StreamCallback(BaseCallbackHandler):\\n    def on_llm_new_token(self, token: str, **kwargs):\\n        print(token, end=\"\", flush=True)\\n\\n# LLM 모델을 설정합니다.\\nllm = ChatOpenAI(model=\"gpt-4o\", temperature=0, streaming=True, callbacks=[StreamCallback()])\\n\\n# Retriever 문서를 포맷팅합니다.\\ndef format_docs(documents):\\n    formatted_list = []\\n    for doc in documents:\\n        title = doc.metadata.get(\\'title\\', \\'\\')  # title이 있으면 가져오고, 없으면 빈 문자열로 설정\\n        formatted_list.append(\\n            f\"<doc><content>{doc.page_content}</content><title>{title}</title><source>{doc.metadata[\\'source\\']}</source></doc>\"\\n        )\\n    return formatted_list\\n\\n# 체인을 생성합니다.\\nrag_chain = (\\n    {\"context\": ensemble_retriever | format_docs, \"question\": RunnablePassthrough()}\\n    | prompt | llm | StrOutputParser()\\n)\\n\\n# 날짜와 시간 함수\\nfrom datetime import datetime\\n\\ndef get_current_datetime():\\n    now = datetime.now()\\n    formatted_datetime = now.strftime(\"%Y-%m-%d %H:%M:%S\")\\n    return formatted_datetime\\n\\n# 텔레그램 봇 설정 및 핸들러 정의\\nimport telegram\\nfrom telegram.ext import Application, CommandHandler, MessageHandler, filters\\nfrom telegram.constants import ChatAction, ParseMode\\n\\n# 텔레그램 봇 토큰을 환경 변수에서 가져옵니다.\\nbot = telegram.Bot(os.getenv(\"BOT_TOKEN\"))\\n\\n# RAG 체인을 사용하여 답변을 생성하는 함수\\ndef generate_response(message):\\n    return rag_chain.invoke(message)\\n\\n# 텍스트를 Telegram Markdown V2 형식으로 이스케이프하는 함수\\ndef escape_markdown_v2(text):\\n    escape_chars = r\\'\\\\`*_{}[]()#+-.!|>=\\'\\n    return \\'\\'.join([\\'\\\\\\\\\\' + char if char in escape_chars else char for char in text])\\n\\n# 응답을 나누어 마크다운 V2 형식으로 포맷팅하는 함수\\ndef split_response(response):\\n    parts = response.split(\"```\")\\n    result = []\\n    for i, part in enumerate(parts):\\n        if i % 2 == 0:\\n            result.append(escape_markdown_v2(part))\\n        else:\\n            result.append(f\"```{part}```\")\\n    return result\\n\\n# 봇의 /start 명령에 대한 핸들러 함수\\nasync def start(update, context):\\n    await context.bot.send_message(chat_id=update.effective_chat.id, text=\"안녕하세요, Langchain 챗봇입니다! 🧑\\u200d💻\")\\n\\n# 텔레그램 메시지에 대한 응답을 생성하는 핸들러 함수\\nasync def answer_openai(update, context):\\n    message = update.message.text\\n\\n    # 유저 이름 또는 사용자명 추출\\n    user = update.message.from_user  # 유저 정보 추출\\n    user_id = update.message.from_user.id  # 유저 ID 추출\\n    user_identifier = user.username if user.username else f\"{user.first_name} {user.last_name if user.last_name else \\'\\'}\"\\n    date_time = get_current_datetime()\\n    print(f\"\\\\n[User_Info] uid: {user_id},  name: {user_identifier}, date: {date_time}\")\\n    print(f\"\\\\n[Question] {message}\\\\n[Answer]\\\\n\")    \\n\\n    chat_id = update.effective_chat.id\\n\\n    loading_message = await context.bot.send_message(chat_id=chat_id, text=\"처리 중입니다... 🧑\\u200d💻\")\\n    await context.bot.send_chat_action(chat_id=update.effective_chat.id, action=ChatAction.TYPING)\\n    try:\\n        response = generate_response(message)\\n        print(\"\\\\n\\\\n\")\\n    except Exception as e:\\n        await context.bot.delete_message(chat_id=chat_id, message_id=loading_message.message_id)\\n        await context.bot.send_message(chat_id=chat_id, text=f\"오류가 발생했습니다: {str(e)}\")\\n        return\\n    \\n    await context.bot.delete_message(chat_id=chat_id, message_id=loading_message.message_id)\\n    \\n    # 코드 블록으로 감싸고 마크다운 V2 이스케이프 처리\\n    formatted_response_parts = split_response(response)\\n    \\n    # 디버ps깅 출력을 추가하여 이스케이프된 텍스트 확인\\n    # for part in formatted_response_parts: print(part)\\n    \\n    # 메시지가 너무 길면 나누어서 보내기\\n    for part in formatted_response_parts:\\n        if part.strip():  # part가 비어있지 않은 경우에만 메시지 전송\\n            await context.bot.send_message(chat_id=update.effective_chat.id, text=part, parse_mode=ParseMode.MARKDOWN_V2)\\n# 텔레그램 봇 애플리케이션 생성 및 핸들러 추가\\n\\napplication = Application.builder().token(os.getenv(\"BOT_TOKEN\")).build()\\napplication.add_handler(CommandHandler(\\'start\\', start))\\napplication.add_handler(MessageHandler(filters.TEXT & ~filters.COMMAND, answer_openai))\\n\\n# 봇 실행\\napplication.run_polling()'),\n",
       " Document(metadata={'source': '..\\\\langchain-kr\\\\17-LangGraph\\\\rag\\\\base.py'}, page_content='from langchain import hub\\nfrom langchain_community.vectorstores import FAISS\\nfrom langchain_core.output_parsers import StrOutputParser\\nfrom langchain_upstage import UpstageEmbeddings\\nfrom langchain_openai import ChatOpenAI\\n\\nfrom langchain_core.output_parsers import StrOutputParser\\nfrom abc import ABC, abstractmethod\\nfrom operator import itemgetter\\n\\n\\nclass RetrievalChain(ABC):\\n    def __init__(self):\\n        self.source_uri = None\\n        self.k = 5\\n\\n    @abstractmethod\\n    def load_documents(self, source_uris):\\n        \"\"\"loader를 사용하여 문서를 로드합니다.\"\"\"\\n        pass\\n\\n    @abstractmethod\\n    def create_text_splitter(self):\\n        \"\"\"text splitter를 생성합니다.\"\"\"\\n        pass\\n\\n    def split_documents(self, docs, text_splitter):\\n        \"\"\"text splitter를 사용하여 문서를 분할합니다.\"\"\"\\n        return text_splitter.split_documents(docs)\\n\\n    def create_embedding(self):\\n        return UpstageEmbeddings(model=\"solar-embedding-1-large\")\\n\\n    def create_vectorstore(self, split_docs):\\n        return FAISS.from_documents(\\n            documents=split_docs, embedding=self.create_embedding()\\n        )\\n\\n    def create_retriever(self, vectorstore):\\n        # MMR을 사용하여 검색을 수행하는 retriever를 생성합니다.\\n        dense_retriever = vectorstore.as_retriever(\\n            search_type=\"mmr\", search_kwargs={\"k\": self.k}\\n        )\\n        return dense_retriever\\n\\n    def create_model(self):\\n        return ChatOpenAI(model_name=\"gpt-4-turbo\", temperature=0)\\n\\n    def create_prompt(self):\\n        return hub.pull(\"teddynote/rag-korean-with-source\")\\n\\n    @staticmethod\\n    def format_docs(docs):\\n        return \"\\\\n\".join(docs)\\n\\n    def create_chain(self):\\n        docs = self.load_documents(self.source_uri)\\n        text_splitter = self.create_text_splitter()\\n        split_docs = self.split_documents(docs, text_splitter)\\n        self.vectorstore = self.create_vectorstore(split_docs)\\n        self.retriever = self.create_retriever(self.vectorstore)\\n        model = self.create_model()\\n        prompt = self.create_prompt()\\n        self.chain = (\\n            {\"question\": itemgetter(\"question\"), \"context\": itemgetter(\"context\")}\\n            | prompt\\n            | model\\n            | StrOutputParser()\\n        )\\n        return self\\n'),\n",
       " Document(metadata={'source': '..\\\\langchain-kr\\\\17-LangGraph\\\\rag\\\\pdf.py'}, page_content='from rag.base import RetrievalChain\\nfrom langchain_community.document_loaders import PDFPlumberLoader\\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\\nfrom typing import List\\n\\n\\nclass PDFRetrievalChain(RetrievalChain):\\n    def __init__(self, source_uri):\\n        self.source_uri = source_uri\\n        self.k = 5\\n\\n    def load_documents(self, source_uris: List[str]):\\n        docs = []\\n        for source_uri in source_uris:\\n            loader = PDFPlumberLoader(source_uri)\\n            docs.extend(loader.load())\\n\\n        return docs\\n\\n    def create_text_splitter(self):\\n        return RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\\n'),\n",
       " Document(metadata={'source': '..\\\\langchain-kr\\\\17-LangGraph\\\\rag\\\\utils.py'}, page_content='def format_docs(docs):\\n    return \"\\\\n\".join(\\n        [\\n            f\"<document><content>{doc.page_content}</content><source>{doc.metadata[\\'source\\']}</source><page>{int(doc.metadata[\\'page\\'])+1}</page></document>\"\\n            for doc in docs\\n        ]\\n    )\\n\\n\\ndef format_searched_docs(docs):\\n    return \"\\\\n\".join(\\n        [\\n            f\"<document><content>{doc[\\'content\\']}</content><source>{doc[\\'url\\']}</source></document>\"\\n            for doc in docs\\n        ]\\n    )\\n\\n\\ndef format_task(tasks):\\n    # 결과를 저장할 빈 리스트 생성\\n    task_time_pairs = []\\n\\n    # 리스트를 순회하면서 각 항목을 처리\\n    for item in tasks:\\n        # 콜론(:) 기준으로 문자열을 분리\\n        task, time_str = item.rsplit(\":\", 1)\\n        # \\'시간\\' 문자열을 제거하고 정수로 변환\\n        time = int(time_str.replace(\"시간\", \"\").strip())\\n        # 할 일과 시간을 튜플로 만들어 리스트에 추가\\n        task_time_pairs.append((task, time))\\n\\n    # 결과 출력\\n    return task_time_pairs\\n'),\n",
       " Document(metadata={'source': '..\\\\langchain-kr\\\\19-Streamlit\\\\00-Chat-Template\\\\chat_history.py'}, page_content='import streamlit as st\\nfrom langchain_core.output_parsers import StrOutputParser\\nfrom langchain_core.messages import ChatMessage\\nfrom langchain_core.prompts import PromptTemplate\\nfrom langchain_openai import ChatOpenAI\\nfrom langchain_core.prompts import load_prompt\\n\\n\\nst.set_page_config(page_title=\"나만의 ChatGPT 💬\", page_icon=\"💬\")\\nst.title(\"나만의 ChatGPT 💬\")\\n\\nif \"messages\" not in st.session_state:\\n    st.session_state[\"messages\"] = []\\n\\n\\ndef print_history():\\n    for msg in st.session_state[\"messages\"]:\\n        st.chat_message(msg.role).write(msg.content)\\n\\n\\ndef add_history(role, content):\\n    st.session_state[\"messages\"].append(ChatMessage(role=role, content=content))\\n\\n\\n# 체인을 생성합니다.\\ndef create_chain(prompt, model):\\n    chain = prompt | ChatOpenAI(model_name=model) | StrOutputParser()\\n    return chain\\n\\n\\nwith st.sidebar:\\n    clear_btn = st.button(\"대화내용 초기화\")\\n    tab1, tab2 = st.tabs([\"프롬프트\", \"프리셋\"])\\n    prompt = \"\"\"당신은 친절한 AI 어시스턴트 입니다. 사용자의 질문에 간결하게 답변해 주세요.\"\"\"\\n    user_text_prompt = tab1.text_area(\"프롬프트\", value=prompt)\\n    user_text_apply_btn = tab1.button(\"프롬프트 적용\", key=\"apply1\")\\n    if user_text_apply_btn:\\n        tab1.markdown(f\"✅ 프롬프트가 적용되었습니다\")\\n        prompt_template = user_text_prompt + \"\\\\n\\\\n#Question:\\\\n{question}\\\\n\\\\n#Answer:\"\\n        prompt = PromptTemplate.from_template(prompt_template)\\n        st.session_state[\"chain\"] = create_chain(prompt, \"gpt-3.5-turbo\")\\n\\n    user_selected_prompt = tab2.selectbox(\"프리셋 선택\", [\"sns\", \"번역\", \"요약\"])\\n    user_selected_apply_btn = tab2.button(\"프롬프트 적용\", key=\"apply2\")\\n    if user_selected_apply_btn:\\n        tab2.markdown(f\"✅ 프롬프트가 적용되었습니다\")\\n        prompt = load_prompt(f\"prompts/{user_selected_prompt}.yaml\", encoding=\"utf8\")\\n        st.session_state[\"chain\"] = create_chain(prompt, \"gpt-3.5-turbo\")\\n\\nif clear_btn:\\n    retriever = st.session_state[\"messages\"].clear()\\n\\nprint_history()\\n\\n\\nif \"chain\" not in st.session_state:\\n    # user_prompt\\n    prompt_template = user_text_prompt + \"\\\\n\\\\n#Question:\\\\n{question}\\\\n\\\\n#Answer:\"\\n    prompt = PromptTemplate.from_template(prompt_template)\\n    st.session_state[\"chain\"] = create_chain(prompt, \"gpt-3.5-turbo\")\\n\\nif user_input := st.chat_input():\\n    add_history(\"user\", user_input)\\n    st.chat_message(\"user\").write(user_input)\\n    with st.chat_message(\"assistant\"):\\n        chat_container = st.empty()\\n\\n        stream_response = st.session_state[\"chain\"].stream(\\n            {\"question\": user_input}\\n        )  # 문서에 대한 질의\\n        ai_answer = \"\"\\n        for chunk in stream_response:\\n            ai_answer += chunk\\n            chat_container.markdown(ai_answer)\\n        add_history(\"ai\", ai_answer)\\n'),\n",
       " Document(metadata={'source': '..\\\\langchain-kr\\\\19-Streamlit\\\\00-Chat-Template\\\\main.py'}, page_content='import streamlit as st\\nfrom langchain_core.output_parsers import StrOutputParser\\nfrom langchain_core.messages import ChatMessage\\nfrom langchain_core.prompts import PromptTemplate\\nfrom langchain_openai import ChatOpenAI\\nfrom langchain_core.prompts import load_prompt\\n\\n\\nst.set_page_config(page_title=\"나만의 ChatGPT 💬\", page_icon=\"💬\")\\nst.title(\"나만의 ChatGPT 💬\")\\n\\nif \"messages\" not in st.session_state:\\n    st.session_state[\"messages\"] = []\\n\\n\\ndef print_history():\\n    for msg in st.session_state[\"messages\"]:\\n        st.chat_message(msg.role).write(msg.content)\\n\\n\\ndef add_history(role, content):\\n    st.session_state[\"messages\"].append(ChatMessage(role=role, content=content))\\n\\n\\n# 체인을 생성합니다.\\ndef create_chain(prompt, model):\\n    chain = prompt | ChatOpenAI(model_name=model) | StrOutputParser()\\n    return chain\\n\\n\\nwith st.sidebar:\\n    clear_btn = st.button(\"대화내용 초기화\")\\n    tab1, tab2 = st.tabs([\"프롬프트\", \"프리셋\"])\\n    prompt = \"\"\"당신은 친절한 AI 어시스턴트 입니다. 사용자의 질문에 간결하게 답변해 주세요.\"\"\"\\n    user_text_prompt = tab1.text_area(\"프롬프트\", value=prompt)\\n    user_text_apply_btn = tab1.button(\"프롬프트 적용\", key=\"apply1\")\\n    if user_text_apply_btn:\\n        tab1.markdown(f\"✅ 프롬프트가 적용되었습니다\")\\n        prompt_template = user_text_prompt + \"\\\\n\\\\n#Question:\\\\n{question}\\\\n\\\\n#Answer:\"\\n        prompt = PromptTemplate.from_template(prompt_template)\\n        st.session_state[\"chain\"] = create_chain(prompt, \"gpt-3.5-turbo\")\\n\\n    user_selected_prompt = tab2.selectbox(\"프리셋 선택\", [\"sns\", \"번역\", \"요약\"])\\n    user_selected_apply_btn = tab2.button(\"프롬프트 적용\", key=\"apply2\")\\n    if user_selected_apply_btn:\\n        tab2.markdown(f\"✅ 프롬프트가 적용되었습니다\")\\n        prompt = load_prompt(f\"prompts/{user_selected_prompt}.yaml\", encoding=\"utf8\")\\n        st.session_state[\"chain\"] = create_chain(prompt, \"gpt-3.5-turbo\")\\n\\nif clear_btn:\\n    retriever = st.session_state[\"messages\"].clear()\\n\\nprint_history()\\n\\n\\nif \"chain\" not in st.session_state:\\n    # user_prompt\\n    prompt_template = user_text_prompt + \"\\\\n\\\\n#Question:\\\\n{question}\\\\n\\\\n#Answer:\"\\n    prompt = PromptTemplate.from_template(prompt_template)\\n    st.session_state[\"chain\"] = create_chain(prompt, \"gpt-3.5-turbo\")\\n\\nif user_input := st.chat_input():\\n    add_history(\"user\", user_input)\\n    st.chat_message(\"user\").write(user_input)\\n    with st.chat_message(\"assistant\"):\\n        chat_container = st.empty()\\n\\n        stream_response = st.session_state[\"chain\"].stream(\\n            {\"question\": user_input}\\n        )  # 문서에 대한 질의\\n        ai_answer = \"\"\\n        for chunk in stream_response:\\n            ai_answer += chunk\\n            chat_container.markdown(ai_answer)\\n        add_history(\"ai\", ai_answer)\\n'),\n",
       " Document(metadata={'source': '..\\\\langchain-kr\\\\19-Streamlit\\\\01-MyProject\\\\retriever.py'}, page_content='from langchain_text_splitters import RecursiveCharacterTextSplitter\\nfrom langchain_community.document_loaders import PDFPlumberLoader\\nfrom langchain_community.vectorstores import FAISS\\nfrom langchain_openai import OpenAIEmbeddings\\n\\n\\ndef create_retriever(file_path):\\n    # 단계 1: 문서 로드(Load Documents)\\n    loader = PDFPlumberLoader(file_path)\\n    docs = loader.load()\\n\\n    # 단계 2: 문서 분할(Split Documents)\\n    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=50)\\n    split_documents = text_splitter.split_documents(docs)\\n\\n    # 단계 3: 임베딩(Embedding) 생성\\n    embeddings = OpenAIEmbeddings()\\n\\n    # 단계 4: DB 생성(Create DB) 및 저장\\n    # 벡터스토어를 생성합니다.\\n    vectorstore = FAISS.from_documents(documents=split_documents, embedding=embeddings)\\n\\n    # 단계 5: 검색기(Retriever) 생성\\n    # 문서에 포함되어 있는 정보를 검색하고 생성합니다.\\n    retriever = vectorstore.as_retriever()\\n    return retriever\\n'),\n",
       " Document(metadata={'source': '..\\\\langchain-kr\\\\19-Streamlit\\\\01-MyProject\\\\pages\\\\01_PDF.py'}, page_content='import streamlit as st\\nfrom langchain_core.messages.chat import ChatMessage\\nfrom langchain_openai import ChatOpenAI\\nfrom langchain_core.output_parsers import StrOutputParser\\nfrom langchain_teddynote.prompts import load_prompt\\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\\nfrom langchain_community.document_loaders import PDFPlumberLoader\\nfrom langchain_community.vectorstores import FAISS\\nfrom langchain_core.output_parsers import StrOutputParser\\nfrom langchain_core.runnables import RunnablePassthrough\\nfrom langchain_openai import ChatOpenAI, OpenAIEmbeddings\\nfrom langchain_teddynote import logging\\nfrom dotenv import load_dotenv\\nimport os\\n\\n# API KEY 정보로드\\nload_dotenv()\\n\\n# 프로젝트 이름을 입력합니다.\\nlogging.langsmith(\"[Project] PDF RAG\")\\n\\n# 캐시 디렉토리 생성\\nif not os.path.exists(\".cache\"):\\n    os.mkdir(\".cache\")\\n\\n# 파일 업로드 전용 폴더\\nif not os.path.exists(\".cache/files\"):\\n    os.mkdir(\".cache/files\")\\n\\nif not os.path.exists(\".cache/embeddings\"):\\n    os.mkdir(\".cache/embeddings\")\\n\\nst.title(\"PDF 기반 QA💬\")\\n\\n# 처음 1번만 실행하기 위한 코드\\nif \"messages\" not in st.session_state:\\n    # 대화기록을 저장하기 위한 용도로 생성한다.\\n    st.session_state[\"messages\"] = []\\n\\nif \"chain\" not in st.session_state:\\n    # 아무런 파일을 업로드 하지 않을 경우\\n    st.session_state[\"chain\"] = None\\n\\n# 사이드바 생성\\nwith st.sidebar:\\n    # 초기화 버튼 생성\\n    clear_btn = st.button(\"대화 초기화\")\\n\\n    # 파일 업로드\\n    uploaded_file = st.file_uploader(\"파일 업로드\", type=[\"pdf\"])\\n\\n    # 모델 선택 메뉴\\n    selected_model = st.selectbox(\\n        \"LLM 선택\", [\"gpt-4o\", \"gpt-4-turbo\", \"gpt-4o-mini\"], index=0\\n    )\\n\\n\\n# 이전 대화를 출력\\ndef print_messages():\\n    for chat_message in st.session_state[\"messages\"]:\\n        st.chat_message(chat_message.role).write(chat_message.content)\\n\\n\\n# 새로운 메시지를 추가\\ndef add_message(role, message):\\n    st.session_state[\"messages\"].append(ChatMessage(role=role, content=message))\\n\\n\\n# 파일을 캐시 저장(시간이 오래 걸리는 작업을 처리할 예정)\\n@st.cache_resource(show_spinner=\"업로드한 파일을 처리 중입니다...\")\\ndef embed_file(file):\\n    # 업로드한 파일을 캐시 디렉토리에 저장합니다.\\n    file_content = file.read()\\n    file_path = f\"./.cache/files/{file.name}\"\\n    with open(file_path, \"wb\") as f:\\n        f.write(file_content)\\n\\n    # 단계 1: 문서 로드(Load Documents)\\n    loader = PDFPlumberLoader(file_path)\\n    docs = loader.load()\\n\\n    # 단계 2: 문서 분할(Split Documents)\\n    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=50)\\n    split_documents = text_splitter.split_documents(docs)\\n\\n    # 단계 3: 임베딩(Embedding) 생성\\n    embeddings = OpenAIEmbeddings()\\n\\n    # 단계 4: DB 생성(Create DB) 및 저장\\n    # 벡터스토어를 생성합니다.\\n    vectorstore = FAISS.from_documents(documents=split_documents, embedding=embeddings)\\n\\n    # 단계 5: 검색기(Retriever) 생성\\n    # 문서에 포함되어 있는 정보를 검색하고 생성합니다.\\n    retriever = vectorstore.as_retriever()\\n    return retriever\\n\\n\\n# 체인 생성\\ndef create_chain(retriever, model_name=\"gpt-4o\"):\\n    # 단계 6: 프롬프트 생성(Create Prompt)\\n    # 프롬프트를 생성합니다.\\n    prompt = load_prompt(\"prompts/pdf-rag.yaml\", encoding=\"utf-8\")\\n\\n    # 단계 7: 언어모델(LLM) 생성\\n    # 모델(LLM) 을 생성합니다.\\n    llm = ChatOpenAI(model_name=model_name, temperature=0)\\n\\n    # 단계 8: 체인(Chain) 생성\\n    chain = (\\n        {\"context\": retriever, \"question\": RunnablePassthrough()}\\n        | prompt\\n        | llm\\n        | StrOutputParser()\\n    )\\n    return chain\\n\\n\\n# 파일이 업로드 되었을 때\\nif uploaded_file:\\n    # 파일 업로드 후 retriever 생성 (작업시간이 오래 걸릴 예정...)\\n    retriever = embed_file(uploaded_file)\\n    chain = create_chain(retriever, model_name=selected_model)\\n    st.session_state[\"chain\"] = chain\\n\\n# 초기화 버튼이 눌리면...\\nif clear_btn:\\n    st.session_state[\"messages\"] = []\\n\\n# 이전 대화 기록 출력\\nprint_messages()\\n\\n# 사용자의 입력\\nuser_input = st.chat_input(\"궁금한 내용을 물어보세요!\")\\n\\n# 경고 메시지를 띄우기 위한 빈 영역\\nwarning_msg = st.empty()\\n\\n# 만약에 사용자 입력이 들어오면...\\nif user_input:\\n    # chain 을 생성\\n    chain = st.session_state[\"chain\"]\\n\\n    if chain is not None:\\n        # 사용자의 입력\\n        st.chat_message(\"user\").write(user_input)\\n        # 스트리밍 호출\\n        response = chain.stream(user_input)\\n        with st.chat_message(\"assistant\"):\\n            # 빈 공간(컨테이너)을 만들어서, 여기에 토큰을 스트리밍 출력한다.\\n            container = st.empty()\\n\\n            ai_answer = \"\"\\n            for token in response:\\n                ai_answer += token\\n                container.markdown(ai_answer)\\n\\n        # 대화기록을 저장한다.\\n        add_message(\"user\", user_input)\\n        add_message(\"assistant\", ai_answer)\\n    else:\\n        # 파일을 업로드 하라는 경고 메시지 출력\\n        warning_msg.error(\"파일을 업로드 해주세요.\")\\n'),\n",
       " Document(metadata={'source': '..\\\\langchain-kr\\\\19-Streamlit\\\\01-MyProject\\\\pages\\\\02_Local_RAG.py'}, page_content='from pyexpat import model\\nimport streamlit as st\\nfrom langchain_core.messages.chat import ChatMessage\\nfrom langchain_openai import ChatOpenAI\\nfrom langchain_core.output_parsers import StrOutputParser\\nfrom langchain_teddynote.prompts import load_prompt\\nfrom langchain_core.output_parsers import StrOutputParser\\nfrom langchain_core.runnables import RunnablePassthrough\\nfrom langchain_openai import ChatOpenAI\\nfrom langchain_community.chat_models import ChatOllama\\nfrom langchain_teddynote import logging\\nfrom dotenv import load_dotenv\\nimport os\\nfrom retriever import create_retriever\\n\\n# API KEY 정보로드\\nload_dotenv()\\n\\n# 프로젝트 이름을 입력합니다.\\nlogging.langsmith(\"[Project] PDF RAG\")\\n\\n# 캐시 디렉토리 생성\\nif not os.path.exists(\".cache\"):\\n    os.mkdir(\".cache\")\\n\\n# 파일 업로드 전용 폴더\\nif not os.path.exists(\".cache/files\"):\\n    os.mkdir(\".cache/files\")\\n\\nif not os.path.exists(\".cache/embeddings\"):\\n    os.mkdir(\".cache/embeddings\")\\n\\nst.title(\"Local 모델 기반 RAG 💬\")\\n\\n# 처음 1번만 실행하기 위한 코드\\nif \"messages\" not in st.session_state:\\n    # 대화기록을 저장하기 위한 용도로 생성한다.\\n    st.session_state[\"messages\"] = []\\n\\nif \"chain\" not in st.session_state:\\n    # 아무런 파일을 업로드 하지 않을 경우\\n    st.session_state[\"chain\"] = None\\n\\n# 사이드바 생성\\nwith st.sidebar:\\n    # 초기화 버튼 생성\\n    clear_btn = st.button(\"대화 초기화\")\\n\\n    # 파일 업로드\\n    uploaded_file = st.file_uploader(\"파일 업로드\", type=[\"pdf\"])\\n\\n    # 모델 선택 메뉴\\n    selected_model = st.selectbox(\"LLM 선택\", [\"xionic\", \"ollama\"], index=0)\\n\\n\\n# 이전 대화를 출력\\ndef print_messages():\\n    for chat_message in st.session_state[\"messages\"]:\\n        st.chat_message(chat_message.role).write(chat_message.content)\\n\\n\\n# 새로운 메시지를 추가\\ndef add_message(role, message):\\n    st.session_state[\"messages\"].append(ChatMessage(role=role, content=message))\\n\\n\\n# 파일을 캐시 저장(시간이 오래 걸리는 작업을 처리할 예정)\\n@st.cache_resource(show_spinner=\"업로드한 파일을 처리 중입니다...\")\\ndef embed_file(file):\\n    # 업로드한 파일을 캐시 디렉토리에 저장합니다.\\n    file_content = file.read()\\n    file_path = f\"./.cache/files/{file.name}\"\\n    with open(file_path, \"wb\") as f:\\n        f.write(file_content)\\n\\n    return create_retriever(file_path)\\n\\n\\ndef format_doc(document_list):\\n    return \"\\\\n\\\\n\".join([doc.page_content for doc in document_list])\\n\\n\\n# 체인 생성\\ndef create_chain(retriever, model_name=\"xionic\"):\\n    # 단계 6: 프롬프트 생성(Create Prompt)\\n    # 프롬프트를 생성합니다.\\n    if model_name == \"xionic\":\\n        # 단계 6: 프롬프트 생성(Create Prompt)\\n        prompt = load_prompt(\"prompts/pdf-rag-xionic.yaml\", encoding=\"utf-8\")\\n\\n        # 단계 7: 언어모델(LLM) 생성\\n        llm = ChatOpenAI(\\n            model_name=\"xionic-1-72b-20240610\",\\n            base_url=\"https://sionic.chat/v1/\",\\n            api_key=\"934c4bbc-c384-4bea-af82-1450d7f8128d\",\\n        )\\n    elif model_name == \"ollama\":\\n        # 단계 6: 프롬프트 생성(Create Prompt)\\n        prompt = load_prompt(\"prompts/pdf-rag-ollama.yaml\", encoding=\"utf-8\")\\n\\n        # 단계 7: 언어모델(LLM) 생성\\n        # Ollama 모델을 불러옵니다.\\n        llm = ChatOllama(model=\"EEVE-Korean-10.8B:latest\", temperature=0)\\n\\n    # 단계 8: 체인(Chain) 생성\\n    chain = (\\n        {\"context\": retriever | format_doc, \"question\": RunnablePassthrough()}\\n        | prompt\\n        | llm\\n        | StrOutputParser()\\n    )\\n    return chain\\n\\n\\n# 파일이 업로드 되었을 때\\nif uploaded_file:\\n    # 파일 업로드 후 retriever 생성 (작업시간이 오래 걸릴 예정...)\\n    retriever = embed_file(uploaded_file)\\n    chain = create_chain(retriever, model_name=selected_model)\\n    st.session_state[\"chain\"] = chain\\n\\n# 초기화 버튼이 눌리면...\\nif clear_btn:\\n    st.session_state[\"messages\"] = []\\n\\n# 이전 대화 기록 출력\\nprint_messages()\\n\\n# 사용자의 입력\\nuser_input = st.chat_input(\"궁금한 내용을 물어보세요!\")\\n\\n# 경고 메시지를 띄우기 위한 빈 영역\\nwarning_msg = st.empty()\\n\\n# 만약에 사용자 입력이 들어오면...\\nif user_input:\\n    # chain 을 생성\\n    chain = st.session_state[\"chain\"]\\n\\n    if chain is not None:\\n        # 사용자의 입력\\n        st.chat_message(\"user\").write(user_input)\\n        # 스트리밍 호출\\n        response = chain.stream(user_input)\\n        with st.chat_message(\"assistant\"):\\n            # 빈 공간(컨테이너)을 만들어서, 여기에 토큰을 스트리밍 출력한다.\\n            container = st.empty()\\n\\n            ai_answer = \"\"\\n            for token in response:\\n                ai_answer += token\\n                container.markdown(ai_answer)\\n\\n        # 대화기록을 저장한다.\\n        add_message(\"user\", user_input)\\n        add_message(\"assistant\", ai_answer)\\n    else:\\n        # 파일을 업로드 하라는 경고 메시지 출력\\n        warning_msg.error(\"파일을 업로드 해주세요.\")\\n'),\n",
       " Document(metadata={'source': '..\\\\langchain-kr\\\\19-Streamlit\\\\01-MyProject\\\\pages\\\\03_Multi_Modal.py'}, page_content='import streamlit as st\\nfrom langchain_core.messages.chat import ChatMessage\\nfrom langchain_openai import ChatOpenAI\\nfrom langchain_teddynote import logging\\nfrom langchain_teddynote.models import MultiModal\\n\\nfrom dotenv import load_dotenv\\nimport os\\n\\n\\n# API KEY 정보로드\\nload_dotenv()\\n\\n# 프로젝트 이름을 입력합니다.\\nlogging.langsmith(\"[Project] 이미지 인식\")\\n\\n# 캐시 디렉토리 생성\\nif not os.path.exists(\".cache\"):\\n    os.mkdir(\".cache\")\\n\\n# 파일 업로드 전용 폴더\\nif not os.path.exists(\".cache/files\"):\\n    os.mkdir(\".cache/files\")\\n\\nif not os.path.exists(\".cache/embeddings\"):\\n    os.mkdir(\".cache/embeddings\")\\n\\nst.title(\"이미지 인식 기반 챗봇 💬\")\\n\\n# 처음 1번만 실행하기 위한 코드\\nif \"messages\" not in st.session_state:\\n    # 대화기록을 저장하기 위한 용도로 생성한다.\\n    st.session_state[\"messages\"] = []\\n\\n# 탭을 생성\\nmain_tab1, main_tab2 = st.tabs([\"이미지\", \"대화내용\"])\\n\\n\\n# 사이드바 생성\\nwith st.sidebar:\\n    # 초기화 버튼 생성\\n    clear_btn = st.button(\"대화 초기화\")\\n\\n    # 이미지 업로드\\n    uploaded_file = st.file_uploader(\"이미지 업로드\", type=[\"jpg\", \"jpeg\", \"png\"])\\n\\n    # 모델 선택 메뉴\\n    selected_model = st.selectbox(\"LLM 선택\", [\"gpt-4o\", \"gpt-4o-mini\"], index=0)\\n\\n    # 시스템 프롬프트 추가\\n    system_prompt = st.text_area(\\n        \"시스템 프롬프트\",\\n        \"당신은 표(재무제표) 를 해석하는 금융 AI 어시스턴트 입니다.\\\\n당신의 임무는 주어진 테이블 형식의 재무제표를 바탕으로 흥미로운 사실을 정리하여 친절하게 답변하는 것입니다.\",\\n        height=200,\\n    )\\n\\n\\n# 이전 대화를 출력\\ndef print_messages():\\n    for chat_message in st.session_state[\"messages\"]:\\n        main_tab2.chat_message(chat_message.role).write(chat_message.content)\\n\\n\\n# 새로운 메시지를 추가\\ndef add_message(role, message):\\n    st.session_state[\"messages\"].append(ChatMessage(role=role, content=message))\\n\\n\\n# 이미지을 캐시 저장(시간이 오래 걸리는 작업을 처리할 예정)\\n@st.cache_resource(show_spinner=\"업로드한 이미지를 처리 중입니다...\")\\ndef process_imagefile(file):\\n    # 업로드한 파일을 캐시 디렉토리에 저장합니다.\\n    file_content = file.read()\\n    file_path = f\"./.cache/files/{file.name}\"\\n\\n    with open(file_path, \"wb\") as f:\\n        f.write(file_content)\\n\\n    return file_path\\n\\n\\n# 체인 생성\\ndef generate_answer(image_filepath, system_prompt, user_prompt, model_name=\"gpt-4o\"):\\n    # 객체 생성\\n    llm = ChatOpenAI(\\n        temperature=0,\\n        model_name=model_name,  # 모델명\\n    )\\n\\n    # 멀티모달 객체 생성\\n    multimodal = MultiModal(llm, system_prompt=system_prompt, user_prompt=user_prompt)\\n\\n    # 이미지 파일로 부터 질의(스트림 방식)\\n    answer = multimodal.stream(image_filepath)\\n    return answer\\n\\n\\n# 초기화 버튼이 눌리면...\\nif clear_btn:\\n    st.session_state[\"messages\"] = []\\n\\n# 이전 대화 기록 출력\\nprint_messages()\\n\\n# 사용자의 입력\\nuser_input = st.chat_input(\"궁금한 내용을 물어보세요!\")\\n\\n# 경고 메시지를 띄우기 위한 빈 영역\\nwarning_msg = main_tab2.empty()\\n\\n# 이미지가 업로드가 된다면...\\nif uploaded_file:\\n    # 이미지 파일을 처리\\n    image_filepath = process_imagefile(uploaded_file)\\n    main_tab1.image(image_filepath)\\n\\n# 만약에 사용자 입력이 들어오면...\\nif user_input:\\n    # 파일이 업로드 되었는지 확인\\n    if uploaded_file:\\n        # 이미지 파일을 처리\\n        image_filepath = process_imagefile(uploaded_file)\\n        # 답변 요청\\n        response = generate_answer(\\n            image_filepath, system_prompt, user_input, selected_model\\n        )\\n\\n        # 사용자의 입력\\n        main_tab2.chat_message(\"user\").write(user_input)\\n\\n        with main_tab2.chat_message(\"assistant\"):\\n            # 빈 공간(컨테이너)을 만들어서, 여기에 토큰을 스트리밍 출력한다.\\n            container = st.empty()\\n\\n            ai_answer = \"\"\\n            for token in response:\\n                ai_answer += token.content\\n                container.markdown(ai_answer)\\n\\n        # 대화기록을 저장한다.\\n        add_message(\"user\", user_input)\\n        add_message(\"assistant\", ai_answer)\\n    else:\\n        # 이미지를 업로드 하라는 경고 메시지 출력\\n        warning_msg.error(\"이미지를 업로드 해주세요.\")\\n'),\n",
       " Document(metadata={'source': '..\\\\langchain-kr\\\\19-Streamlit\\\\01-MyProject\\\\pages\\\\04_Multi_Turn.py'}, page_content='from urllib import response\\nfrom requests import session\\nimport streamlit as st\\nfrom langchain_core.messages.chat import ChatMessage\\nfrom langchain_openai import ChatOpenAI\\nfrom langchain_teddynote import logging\\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\\nfrom langchain_community.chat_message_histories import ChatMessageHistory\\nfrom langchain_core.chat_history import BaseChatMessageHistory\\nfrom langchain_core.runnables.history import RunnableWithMessageHistory\\nfrom langchain_openai import ChatOpenAI\\nfrom langchain_core.output_parsers import StrOutputParser\\n\\nfrom dotenv import load_dotenv\\nimport os\\n\\n\\n# API KEY 정보로드\\nload_dotenv()\\n\\n# 프로젝트 이름을 입력합니다.\\nlogging.langsmith(\"[Project] Multi Tuen 챗봇\")\\n\\n# 캐시 디렉토리 생성\\nif not os.path.exists(\".cache\"):\\n    os.mkdir(\".cache\")\\n\\n# 파일 업로드 전용 폴더\\nif not os.path.exists(\".cache/files\"):\\n    os.mkdir(\".cache/files\")\\n\\nif not os.path.exists(\".cache/embeddings\"):\\n    os.mkdir(\".cache/embeddings\")\\n\\nst.title(\"대화내용을 기억하는 챗봇 💬\")\\n\\n# 처음 1번만 실행하기 위한 코드\\nif \"messages\" not in st.session_state:\\n    # 대화기록을 저장하기 위한 용도로 생성한다.\\n    st.session_state[\"messages\"] = []\\n\\nif \"store\" not in st.session_state:\\n    st.session_state[\"store\"] = {}\\n\\n\\n# 사이드바 생성\\nwith st.sidebar:\\n    # 초기화 버튼 생성\\n    clear_btn = st.button(\"대화 초기화\")\\n\\n    # 모델 선택 메뉴\\n    selected_model = st.selectbox(\"LLM 선택\", [\"gpt-4o\", \"gpt-4o-mini\"], index=0)\\n\\n    # 세션 ID 를 지정하는 메뉴\\n    session_id = st.text_input(\"세션 ID를 입력하세요.\", \"abc123\")\\n\\n\\n# 이전 대화를 출력\\ndef print_messages():\\n    for chat_message in st.session_state[\"messages\"]:\\n        st.chat_message(chat_message.role).write(chat_message.content)\\n\\n\\n# 새로운 메시지를 추가\\ndef add_message(role, message):\\n    st.session_state[\"messages\"].append(ChatMessage(role=role, content=message))\\n\\n\\n# 세션 ID를 기반으로 세션 기록을 가져오는 함수\\ndef get_session_history(session_ids):\\n    if session_ids not in st.session_state[\"store\"]:  # 세션 ID가 store에 없는 경우\\n        # 새로운 ChatMessageHistory 객체를 생성하여 store에 저장\\n        st.session_state[\"store\"][session_ids] = ChatMessageHistory()\\n    return st.session_state[\"store\"][session_ids]  # 해당 세션 ID에 대한 세션 기록 반환\\n\\n\\n# 체인 생성\\ndef create_chain(model_name=\"gpt-4o\"):\\n\\n    # 프롬프트 정의\\n    prompt = ChatPromptTemplate.from_messages(\\n        [\\n            (\\n                \"system\",\\n                \"당신은 Question-Answering 챗봇입니다. 주어진 질문에 대한 답변을 제공해주세요.\",\\n            ),\\n            # 대화기록용 key 인 chat_history 는 가급적 변경 없이 사용하세요!\\n            MessagesPlaceholder(variable_name=\"chat_history\"),\\n            (\"human\", \"#Question:\\\\n{question}\"),  # 사용자 입력을 변수로 사용\\n        ]\\n    )\\n\\n    # llm 생성\\n    llm = ChatOpenAI(model_name=\"gpt-4o\")\\n\\n    # 일반 Chain 생성\\n    chain = prompt | llm | StrOutputParser()\\n\\n    chain_with_history = RunnableWithMessageHistory(\\n        chain,\\n        get_session_history,  # 세션 기록을 가져오는 함수\\n        input_messages_key=\"question\",  # 사용자의 질문이 템플릿 변수에 들어갈 key\\n        history_messages_key=\"chat_history\",  # 기록 메시지의 키\\n    )\\n    return chain_with_history\\n\\n\\n# 초기화 버튼이 눌리면...\\nif clear_btn:\\n    st.session_state[\"messages\"] = []\\n\\n# 이전 대화 기록 출력\\nprint_messages()\\n\\n# 사용자의 입력\\nuser_input = st.chat_input(\"궁금한 내용을 물어보세요!\")\\n\\n# 경고 메시지를 띄우기 위한 빈 영역\\nwarning_msg = st.empty()\\n\\nif \"chain\" not in st.session_state:\\n    st.session_state[\"chain\"] = create_chain(model_name=selected_model)\\n\\n\\n# 만약에 사용자 입력이 들어오면...\\nif user_input:\\n    chain = st.session_state[\"chain\"]\\n    if chain is not None:\\n        response = chain.stream(\\n            # 질문 입력\\n            {\"question\": user_input},\\n            # 세션 ID 기준으로 대화를 기록합니다.\\n            config={\"configurable\": {\"session_id\": session_id}},\\n        )\\n\\n        # 사용자의 입력\\n        st.chat_message(\"user\").write(user_input)\\n\\n        with st.chat_message(\"assistant\"):\\n            # 빈 공간(컨테이너)을 만들어서, 여기에 토큰을 스트리밍 출력한다.\\n            container = st.empty()\\n\\n            ai_answer = \"\"\\n            for token in response:\\n                ai_answer += token\\n                container.markdown(ai_answer)\\n\\n            # 대화기록을 저장한다.\\n            add_message(\"user\", user_input)\\n            add_message(\"assistant\", ai_answer)\\n    else:\\n        # 이미지를 업로드 하라는 경고 메시지 출력\\n        warning_msg.error(\"이미지를 업로드 해주세요.\")\\n'),\n",
       " Document(metadata={'source': '..\\\\langchain-kr\\\\19-Streamlit\\\\02-Email\\\\main.py'}, page_content='import os\\nfrom dotenv import load_dotenv\\nimport streamlit as st\\nfrom langchain_core.messages.chat import ChatMessage\\nfrom langchain_core.prompts import PromptTemplate\\nfrom langchain_core.output_parsers import StrOutputParser\\nfrom langchain_core.pydantic_v1 import BaseModel, Field\\nfrom langchain_core.output_parsers import PydanticOutputParser\\nfrom langchain_openai import ChatOpenAI\\nfrom langchain_community.utilities import SerpAPIWrapper\\nfrom langchain_teddynote.prompts import load_prompt\\n\\n\\n# 검색을 위한 API KEY 설정\\nos.environ[\"SERPAPI_API_KEY\"] = (\\n    \"e76de14ee240e0051ed8bb05d5db568dd1dc9cfcaa2b51fd83613829a85bf244\"\\n)\\n\\n\\n# 이메일 본문으로부터 주요 엔티티 추출\\nclass EmailSummary(BaseModel):\\n    person: str = Field(description=\"메일을 보낸 사람\")\\n    company: str = Field(description=\"메일을 보낸 사람의 회사 정보\")\\n    email: str = Field(description=\"메일을 보낸 사람의 이메일 주소\")\\n    subject: str = Field(description=\"메일 제목\")\\n    summary: str = Field(description=\"메일 본문을 요약한 텍스트\")\\n    date: str = Field(description=\"메일 본문에 언급된 미팅 날짜와 시간\")\\n\\n\\n# API KEY 정보로드\\nload_dotenv()\\n\\nst.title(\"Email 요약기 💬\")\\n\\n\\n# 처음 1번만 실행하기 위한 코드\\nif \"messages\" not in st.session_state:\\n    # 대화기록을 저장하기 위한 용도로 생성한다.\\n    st.session_state[\"messages\"] = []\\n\\n# 사이드바 생성\\nwith st.sidebar:\\n    # 초기화 버튼 생성\\n    clear_btn = st.button(\"대화 초기화\")\\n\\n\\n# 이전 대화를 출력\\ndef print_messages():\\n    for chat_message in st.session_state[\"messages\"]:\\n        st.chat_message(chat_message.role).write(chat_message.content)\\n\\n\\n# 새로운 메시지를 추가\\ndef add_message(role, message):\\n    st.session_state[\"messages\"].append(ChatMessage(role=role, content=message))\\n\\n\\n# 체인 생성\\ndef create_email_parsing_chain():\\n    # PydanticOutputParser 생성\\n    output_parser = PydanticOutputParser(pydantic_object=EmailSummary)\\n\\n    prompt = PromptTemplate.from_template(\\n        \"\"\"\\n    You are a helpful assistant. Please answer the following questions in KOREAN.\\n\\n    #QUESTION:\\n    다음의 이메일 내용 중에서 주요 내용을 추출해 주세요.\\n\\n    #EMAIL CONVERSATION:\\n    {email_conversation}\\n\\n    #FORMAT:\\n    {format}\\n    \"\"\"\\n    )\\n\\n    # format 에 PydanticOutputParser의 부분 포맷팅(partial) 추가\\n    prompt = prompt.partial(format=output_parser.get_format_instructions())\\n\\n    # 체인 생성\\n    chain = prompt | ChatOpenAI(model=\"gpt-4-turbo\") | output_parser\\n\\n    return chain\\n\\n\\ndef create_report_chain():\\n    prompt = load_prompt(\"prompts/email.yaml\", encoding=\"utf-8\")\\n\\n    # 출력 파서\\n    output_parser = StrOutputParser()\\n\\n    # 체인 생성\\n    chain = prompt | ChatOpenAI(model=\"gpt-4-turbo\") | output_parser\\n\\n    return chain\\n\\n\\n# 초기화 버튼이 눌리면...\\nif clear_btn:\\n    st.session_state[\"messages\"] = []\\n\\n# 이전 대화 기록 출력\\nprint_messages()\\n\\n# 사용자의 입력\\nuser_input = st.chat_input(\"궁금한 내용을 물어보세요!\")\\n\\n# 만약에 사용자 입력이 들어오면...\\nif user_input:\\n    # 사용자의 입력\\n    st.chat_message(\"user\").write(user_input)\\n\\n    # 1) 이메일을 파싱하는 chain 을 생성\\n    email_chain = create_email_parsing_chain()\\n    # email 에서 주요 정보를 추출하는 체인을 실행\\n    answer = email_chain.invoke({\"email_conversation\": user_input})\\n\\n    # 2) 보낸 사람의 추가 정보 수집(검색)\\n    params = {\"engine\": \"google\", \"gl\": \"kr\", \"hl\": \"ko\", \"num\": \"3\"}  # 검색 파라미터\\n    search = SerpAPIWrapper(params=params)  # 검색 객체 생성\\n    search_query = f\"{answer.person} {answer.company} {answer.email}\"  # 검색 쿼리\\n    search_result = search.run(search_query)  # 검색 실행\\n    search_result = eval(search_result)  # list 형태로 변환\\n\\n    # 검색 결과(합치기)\\n    search_result_string = \"\\\\n\".join(search_result)\\n\\n    # 3) 이메일 요약 리포트 생성\\n    report_chain = create_report_chain()\\n    report_chain_input = {\\n        \"sender\": answer.person,\\n        \"additional_information\": search_result_string,\\n        \"company\": answer.company,\\n        \"email\": answer.email,\\n        \"subject\": answer.subject,\\n        \"summary\": answer.summary,\\n        \"date\": answer.date,\\n    }\\n\\n    # 스트리밍 호출\\n    response = report_chain.stream(report_chain_input)\\n    with st.chat_message(\"assistant\"):\\n        # 빈 공간(컨테이너)을 만들어서, 여기에 토큰을 스트리밍 출력한다.\\n        container = st.empty()\\n\\n        ai_answer = \"\"\\n        for token in response:\\n            ai_answer += token\\n            container.markdown(ai_answer)\\n\\n    # 대화기록을 저장한다.\\n    add_message(\"user\", user_input)\\n    add_message(\"assistant\", ai_answer)\\n'),\n",
       " Document(metadata={'source': '..\\\\MyProject\\\\1.project\\\\main.py'}, page_content='import streamlit as st\\nfrom dotenv import load_dotenv\\n\\nload_dotenv()\\n\\nif \"messages\" not in st.session_state:\\n    st.session_state[\"messages\"] = []\\n\\nif \"chain\" not in st.session_state:\\n    st.session_state[\"chain\"] = None\\n\\nwith st.sidebar:\\n    clear_btn = st.button(\"대화초기화\")\\n\\n    selected_prompt = st.selectbox(\\n        \"프롬프트를 선택해 주세요\", (\"기본모드\", \"SNS 게시글\", \"요약\")\\n    )\\n\\n    selected_model = st.selectbox(\\n        \"model을 선택해 주세요\", [\"gpt-4o-mini\", \"gpt-4o\", \"gpt-3.5-turbo\"]\\n    )\\n\\n\\ndef print_messags():\\n    for chat_message in st.session_state[\"messages\"]:\\n        st.chat_message(chat_message.role).write(chat_message.content)\\n\\n\\nfrom langchain_core.messages import ChatMessage\\n\\n\\ndef add_message(role, message):\\n    st.session_state[\"messages\"].append(ChatMessage(role=role, content=message))\\n\\n\\nfrom langchain_core.prompts import ChatPromptTemplate\\nfrom langchain_teddynote.prompts import load_prompt\\nfrom langchain_openai import ChatOpenAI, OpenAIEmbeddings\\nfrom langchain_core.output_parsers import StrOutputParser\\nfrom langchain import hub\\n\\n\\n# 체인 생성\\ndef create_chain(prompt_type, model_name=\"gpt-4o-mini\"):\\n\\n    # prompt 기본 타입\\n    prompt = ChatPromptTemplate.from_messages(\\n        [\\n            (\\n                \"system\",\\n                \"당신은 친절한 AI assistant입니다. 다음의 대답에 간결하게 답해주세요.\",\\n            ),\\n            (\"user\", \"#Question: \\\\n{question}\"),\\n        ]\\n    )\\n\\n    if prompt_type == \"SNS 게시글\":\\n        prompt = load_prompt(\"prompts/sns.yaml\", encoding=\"utf-8\")\\n\\n    elif prompt_type == \"요약\":\\n        prompt = load_prompt(\"prompts/summary2.yaml\", encoding=\"utf-8\")\\n\\n    llm = ChatOpenAI(model=model_name)\\n\\n    chain = prompt | llm | StrOutputParser()\\n    return chain\\n\\n\\n# 초기화 버튼\\nif clear_btn:\\n    st.session_state[\"messages\"] = []\\n\\n# 이전 대화기록 출력\\nprint_messags()\\n\\n# 사용자의 입력\\nuser_input = st.chat_input(\"궁금한 내용을 물어보세요\")\\n\\n# 사용자의 입력이 들어오면\\n\\nif user_input:\\n    # 사용자의 입력\\n    st.chat_message(\"user\").write(user_input)\\n    # 체인 생성\\n    chain = create_chain(selected_prompt, selected_model)\\n\\n    response = chain.stream({\"question\": user_input})\\n\\n    with st.chat_message(\"assistant\"):\\n        container = st.empty()\\n\\n        ai_answer = \"\"\\n        for token in response:\\n            ai_answer += token\\n            container.markdown(ai_answer)\\n\\n    # 대화 기록을 저장\\n    add_message(\"user\", user_input)\\n    add_message(\"assistant\", ai_answer)\\n\\n# streamlit run\\n'),\n",
       " Document(metadata={'source': '..\\\\MyProject\\\\1.project\\\\pages\\\\image_rag.py'}, page_content=''),\n",
       " Document(metadata={'source': '..\\\\MyProject\\\\1.project\\\\pages\\\\pdf_rag.py'}, page_content='import os\\nimport streamlit as st\\nfrom dotenv import load_dotenv\\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\\nfrom langchain_core.messages import ChatMessage\\nfrom langchain_community.document_loaders import PDFPlumberLoader\\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\\nfrom langchain_openai import OpenAIEmbeddings, ChatOpenAI\\nfrom langchain_community.vectorstores import FAISS\\nfrom langchain_teddynote.prompts import load_prompt\\nfrom langchain_core.runnables import RunnablePassthrough\\nfrom langchain_core.output_parsers import StrOutputParser\\n\\nload_dotenv()\\n\\n\\nif not os.path.exists(\".cache\"):\\n    os.mkdir(\".cache\")\\n\\nif not os.path.exists(\".cache/files\"):\\n    os.mkdir(\".cache/files\")\\n\\nif not os.path.exists(\".cache/embeddings\"):\\n    os.mkdir(\".cache/embeddings\")\\n\\nif \"messages\" not in st.session_state:\\n    st.session_state[\"messages\"] = []\\n\\nif \"chain\" not in st.session_state:\\n    st.session_state[\"chain\"] = None\\n\\n\\ndef print_messages():\\n    for chat_message in st.session_state[\"messages\"]:\\n        st.chat_message(chat_message.role).write(chat_message.content)\\n\\n\\ndef add_messages(role, message):\\n    st.session_state[\"messages\"].append(ChatMessage(role=role, content=message))\\n\\n\\nst.title(\"재호의 RAG\")\\n\\nwith st.sidebar:\\n    clear_btn = st.button(\"초기화버튼\")\\n\\n    uploaded_file = st.file_uploader(\"파일을 업로드\", type=[\"pdf\"])\\n\\n    selected_model = st.selectbox(\\n        \"모델선택\", [\"gpt-4o-mini\", \"gpt-4o\", \"gpt-3.5-turbo\"]\\n    )\\n\\n\\n@st.cache_resource(show_spinner=\"업로드한 파일을 처리중 입니다.\")\\ndef embed_file(file):\\n    file_content = file.read()\\n    file_path = f\".cache/files/{file.name}\"\\n    with open(file_path, \"wb\") as f:\\n        f.write(file_content)\\n\\n    loader = PDFPlumberLoader(file_path)\\n    docs = loader.load()\\n\\n    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\\n    documents = text_splitter.split_documents(docs)\\n\\n    embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\\n\\n    vector_store = FAISS.from_documents(documents, embeddings)\\n    retriever = vector_store.as_retriever()\\n\\n    return retriever\\n\\n\\ndef create_chain(retriever, model_name=\"gpt-4o-mini\"):\\n    prompt = load_prompt(\"../prompts/pdf-rag.yaml\")\\n\\n    llm = ChatOpenAI(model=model_name)\\n\\n    chain = (\\n        {\"context\": retriever, \"question\": RunnablePassthrough()}\\n        | prompt\\n        | llm\\n        | StrOutputParser()\\n    )\\n    return chain\\n\\n\\nif uploaded_file:\\n    retriever = embed_file(uploaded_file)\\n    chain = create_chain(retriever, model_name=selected_model)\\n    st.session_state[\"chain\"] = chain\\n\\nif clear_btn:\\n    st.session_state[\"messages\"] = []\\n\\nprint_messages()\\n\\nuser_input = st.chat_input(\"메세지를 입력해주세요\")\\n\\nwarning_msg = st.empty()\\n\\nif user_input:\\n    chain = st.session_state[\"chain\"]\\n\\n    if chain is not None:\\n        st.chat_message(\"user\").write(user_input)\\n        response = chain.stream(user_input)\\n\\n        with st.chat_message(\"assistant\"):\\n            container = st.empty()\\n\\n            ai_answer = \"\"\\n            for token in response:\\n                ai_answer += token\\n                container.markdown(ai_answer)\\n\\n        add_messages(\"user\", user_input)\\n        add_messages(\"assistant\", ai_answer)\\n\\n    else:\\n        warning_msg.error(\"파일을 업로드 해주세요.\")\\n'),\n",
       " Document(metadata={'source': '..\\\\MyProject\\\\7월 라이브\\\\CustomLoader.py'}, page_content='from typing import Iterator\\nfrom typing import Any, Dict, List, Optional\\nfrom langchain_core.document_loaders import BaseLoader\\nfrom langchain_core.documents import Document\\n\\nclass CustomDocumentLoader(BaseLoader):\\n    def __init__(self, file_path: str) - > None:\\n        \"\"\" 로더를 파일 경로와 함께 초기화 한다.\\n        \\n        Args:\\n            file_path: 로드할 파일의 경로\\n        \"\"\"\\n\\n        self.file_path = file_path\\n\\n    def lazy_load(self) -> Iterator[Document]:\\n        \\n    \\n        with open'),\n",
       " Document(metadata={'source': '..\\\\MyProject\\\\7월 라이브\\\\LlamaParseLoader.py'}, page_content='import os\\nfrom typing import Iterator\\nfrom typing import Any, Dict, List, Optional\\nfrom langchain_core.document_loaders import BaseLoader\\nfrom langchain_core.documents import Document\\nfrom llama_parse import LlamaParse\\nfrom llama_index.core import SimpleDirectoryReader\\nimport nest_asyncio\\n\\nnest_asyncio.apply()\\n\\n\\nclass LlamaParseLoader(BaseLoader):\\n    \"\"\"파일을 한 줄씩 읽어오는 문서 로더의 예시입니다.\"\"\"\\n\\n    def __init__(self, file_paths: List[str], parsing_instructions=\"\") -> None:\\n        \"\"\"로더를 파일 경로와 함께 초기화합니다.\\n        Args:\\n            file_paths: 로드할 파일의 경로입니다.\\n        \"\"\"\\n        # LlamaParse 설정\\n        parser = LlamaParse(\\n            # api_key=\"llx-...\",  # API 키 (환경 변수 LLAMA_CLOUD_API_KEY에 저장 가능)\\n            result_type=\"markdown\",  # 결과 타입: \"markdown\" 또는 \"text\"\\n            num_workers=4,  # 여러 파일 처리 시 API 호출 분할 수\\n            verbose=True,\\n            language=\"ko\",  # 언어 설정 (기본값: \\'en\\')\\n            invalidate_cache=True,\\n            skip_diagonal_text=True,\\n            use_vendor_multimodal_model=True,\\n            vendor_multimodal_model_name=\"openai-gpt4o\",\\n            vendor_multimodal_api_key=os.environ.get(\"OPENAI_API_KEY\"),\\n            parsing_instruction=parsing_instructions,\\n        )\\n\\n        file_extractor = {\".pdf\": parser}\\n\\n        self.document_reader = SimpleDirectoryReader(\\n            input_files=file_paths,\\n            file_extractor=file_extractor,\\n        )\\n\\n    def lazy_load(self) -> Iterator[Document]:  # <-- 인자를 받지 않습니다\\n        \"\"\"파일을 한 줄씩 읽어오는 지연 로더입니다.\\n\\n        지연 로드 메소드를 구현할 때는, 문서를 하나씩 생성하여 반환하는 제너레이터를 사용해야 합니다.\\n        \"\"\"\\n        documents = self.document_reader.load_data()\\n        langchain_documents = [doc.to_langchain_format() for doc in documents]\\n        return langchain_documents\\n\\n\\n# <딜레마>\\n# 멀티모달이 개입되면 hallucination 이 필연적으로 발생\\n# 100% 검수를 하려면 추가 토큰이 필요\\n# 대용량 데이터에서 기업이 비용 감내 X\\n'),\n",
       " Document(metadata={'source': '..\\\\과제\\\\02.free_pdf_rag.py'}, page_content='# - PDF 파일 업로드\\n# - FAISS 벡터스토어에 데이터 저장\\n# - 업로드한 파일 기반 Q&A 챗봇\\n# - 유료 모델 사용금지!!\\n\\n############################\\nimport os\\nimport streamlit as st\\nfrom dotenv import load_dotenv\\nfrom langchain_core.messages import ChatMessage\\nfrom langchain_community.document_loaders import PDFPlumberLoader\\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\\nfrom langchain_experimental.text_splitter import SemanticChunker\\nfrom langchain_huggingface import HuggingFaceEndpointEmbeddings\\nfrom langchain_community.vectorstores import FAISS\\nfrom langchain_core.prompts import PromptTemplate\\nfrom langchain_teddynote.prompts import load_prompt\\nfrom langchain_community.chat_models import ChatOllama\\nfrom langchain_core.runnables import RunnablePassthrough\\nfrom langchain_core.output_parsers import StrOutputParser\\n\\nload_dotenv()\\n\\nif not os.path.exists(\".cache\"):\\n    os.mkdir(\".cache\")\\n\\nif not os.path.exists(\".cache/files\"):\\n    os.mkdir(\".cache/files\")\\n\\nif \"messages\" not in st.session_state:\\n    st.session_state[\"messages\"] = []\\n\\nif \"chain\" not in st.session_state:\\n    st.session_state[\"chain\"] = None\\n\\n\\ndef print_messages():\\n    for chat_message in st.session_state[\"messages\"]:\\n        st.chat_message(chat_message.role).write(chat_message.content)\\n\\n\\ndef add_message(role, message):\\n    st.session_state[\"messages\"].append(ChatMessage(role=role, content=message))\\n\\n\\nst.title(\"PDF기반 RAG\")\\n\\nwith st.sidebar:\\n    clear_btn = st.button(\"초기화버튼\")\\n\\n    uploaded_file = st.file_uploader(\"PDF파일 업로드\", type=[\"pdf\"])\\n\\n    selected_model = st.selectbox(\"모델선택\", [\"llama3.1\"])\\n\\n\\n@st.cache_resource(show_spinner=\"파일을 업로드중 입니다.\")\\ndef embed_file(file):\\n    file_content = file.read()\\n    file_path = f\"./.cache/files/{file.name}\"\\n\\n    with open(file_path, \"wb\") as f:\\n        f.write(file_content)\\n\\n    # ------------ indexing\\n    # Text Split\\n    loader = PDFPlumberLoader(file_path)\\n    docs = loader.load()\\n\\n    # Embedding Model\\n    embeddings = HuggingFaceEndpointEmbeddings(\\n        model=\"BAAI/bge-m3\", task=\"feature-extraction\"\\n    )\\n\\n    # Document loader\\n    text_splitter = SemanticChunker(embeddings)\\n    documents = text_splitter.split_documents(docs)\\n    # text_splitter.split_documents(file)\\n\\n    # Vector Store\\n    vector_store = FAISS.from_documents(documents, embeddings)\\n    retriever = vector_store.as_retriever()\\n    return retriever\\n\\n\\ndef create_chain(retriever, model_name=\"llama3.1\"):\\n    prompt = load_prompt(\"prompts/pdf-rag-ollama.yaml\", encoding=\"utf-8\")\\n    # llm\\n    llm = ChatOllama(model=model_name)\\n\\n    chain = (\\n        {\"context\": retriever, \"question\": RunnablePassthrough()}\\n        | prompt\\n        | llm\\n        | StrOutputParser()\\n    )\\n    return chain\\n\\n\\nif uploaded_file:\\n    retriever = embed_file(uploaded_file)\\n    chain = create_chain(retriever, selected_model)\\n    st.session_state[\"chain\"] = chain\\n\\nif clear_btn:\\n    st.session_state[\"messages\"] = []\\n\\nprint_messages()\\n\\nuser_input = st.chat_input(\"궁금한 내용을 물어보세요\")\\n\\nwarning_msg = st.empty()\\n\\nif user_input:\\n    chain = st.session_state[\"chain\"]\\n\\n    if chain is not None:\\n        st.chat_message(\"user\").write(user_input)\\n\\n        response = chain.stream(user_input)\\n\\n        with st.chat_message(\"assistant\"):\\n            container = st.empty()\\n\\n            ai_answer = \"\"\\n            for token in response:\\n                ai_answer += token\\n                container.markdown(ai_answer)\\n\\n        add_message(\"user\", user_input)\\n        add_message(\"assistant\", ai_answer)\\n\\n    else:\\n        warning_msg.error(\"파일을 업로드해 주세요\")\\n'),\n",
       " Document(metadata={'source': '..\\\\꼭 복습\\\\streamlit.py'}, page_content='import streamlit as st\\nfrom langchain_core.messages.chat import ChatMessage\\nfrom langchain_core.prompts import ChatPromptTemplate\\nfrom langchain_community.document_loaders import PyPDFDirectoryLoader\\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\\nfrom langchain_openai import ChatOpenAI, OpenAIEmbeddings\\nfrom langchain_community.vectorstores import FAISS\\nfrom langchain_core.runnables import RunnablePassthrough, RunnableParallel\\nfrom langchain_core.output_parsers import StrOutputParser\\nfrom langchain_teddynote.prompts import load_prompt\\nfrom langchain import hub\\nfrom dotenv import load_dotenv\\n\\nload_dotenv()\\n\\nst.title(\"Pratice streamlit ChatGPT💬\")\\n\\n\\n# 처음 1번만 실행하기 위한 코드, st.session_state: 변수를 저장하기 위함.\\nif \"messages\" not in st.session_state:\\n    st.session_state[\"messages\"] = []\\n\\n# 사이드바\\nwith st.sidebar:\\n    # 대화 초기화 버튼\\n    clear_btn = st.button(\"대화 초기화\")\\n\\n    # 셀렉트박스\\n    selected_prompt = st.selectbox(\\n        \"프롬프트를 선택해주세요\", (\"기본모드\", \"SNS 게시글\", \"요약\"), index=0\\n    )\\n\\n\\n# 이전 대화를 출력\\ndef print_messages():\\n    for chat_message in st.session_state[\"messages\"]:\\n        st.chat_message(chat_message.role).write(\\n            chat_message.content\\n        )  # 롤을 작성하고 내용을 작성.\\n\\n\\n# 새로운 메세지를 추가\\n# from langchain_core.messages.chat import ChatMessage\\ndef add_message(role, message):\\n    st.session_state[\"messages\"].append(ChatMessage(role=role, content=message))\\n\\n\\ndef create_chain(prompt_type):\\n\\n    # 프롬프트-기본모드\\n    prompt = ChatPromptTemplate.from_messages(\\n        [\\n            (\\n                \"system\",\\n                \"당신은 친절한 AI 어시스턴트 입니다. 다음의 질문에 간결하게 답변해 주세요.\",\\n            ),\\n            (\\n                \"user\",\\n                \"\"\"\\n                \"#Context: \\\\n {context}\"\\n                \"#Question: \\\\n {question}\"\\n                \"\"\",\\n            ),\\n        ]\\n    )\\n\\n    if prompt_type == \"SNS 게시글\":\\n        prompt = load_prompt(\"prompts/sns.yaml\", encoding=\"utf-8\")\\n\\n    elif prompt_type == \"요약\":\\n        prompt = hub.pull(\"teddynote/chain-of-density-korean:946ed62d\")\\n\\n    # Load data\\n    docs = PyPDFDirectoryLoader(\"data/RAG\")\\n    docs = docs.load()\\n\\n    # Split data\\n    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\\n    documents = text_splitter.split_documents(docs)\\n\\n    # Embedding\\n    embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\\n\\n    # Vector index\\n    vector_store = FAISS.from_documents(documents, embeddings)\\n    retriever = vector_store.as_retriever()\\n\\n    # Model\\n    llm = ChatOpenAI(model_name=\"gpt-4o-mini\")\\n\\n    # 출력파서\\n    output_parser = StrOutputParser()\\n    chain = (\\n        {\"context\": retriever, \"question\": RunnablePassthrough()}\\n        | prompt\\n        | llm\\n        | output_parser\\n    )\\n    return chain\\n\\n\\n# 초기화 버튼이 눌리면\\nif clear_btn:\\n    st.session_state[\"messages\"] = []\\n\\n# 이전 대화기록 출력\\nprint_messages()\\n\\n# 사용자의 입력\\nuser_input = st.chat_input(\"궁금한 내용을 물어보세요!\")\\n\\n# 만약, 사용자의 입력이 들어오면\\nif user_input:\\n    # 사용자의 입력\\n    st.chat_message(\"user\").write(user_input)\\n    # chain을 생성\\n    chain = create_chain(selected_prompt)\\n\\n    # 스트리밍 호출\\n    # response = chain.stream({\"question\": user_input})\\n    response = chain.stream(user_input)\\n    with st.chat_message(\"assistant\"):\\n\\n        # 빈 공간(컨테이너를) 만들어서, 여기에 토큰을 생성한다.\\n        container = st.empty()\\n\\n        ai_answer = \"\"\\n        for token in response:\\n            ai_answer += token\\n            container.markdown(ai_answer)\\n\\n    # 대화 기록을 저장한다.\\n    add_message(\"user\", user_input)\\n    add_message(\"assistant\", ai_answer)\\n\\n\\n# https://github.com/teddylee777/langchain-kr/blob/main/19-Streamlit/01-MyProject/main.py\\n# streamlit run 19-Streamlit\\\\MyProject\\\\main.py\\n'),\n",
       " Document(metadata={'source': '..\\\\별도\\\\2.Streamlit을 활용한 프로젝트\\\\app.py'}, page_content='import streamlit as st\\nimport yfinance as yf\\nimport matplotlib.pyplot as plt\\nimport pandas as pd\\n\\n\\nfrom yfinance_data import 재무제표처리, 재무제표시각화, 가치주, 우량주, 거래량\\nfrom backend import AI_report\\nfrom meilisearch_search import search_stocks\\n\\n\\n# 심볼과 네김 가져오기\\nclass SearchResults:\\n    def __init__(self, item):\\n        self.item = item\\n\\n    def __getitem__(self, key):\\n        return self.item[key]\\n\\n    def __str__(self):\\n        return f\\'{self.item[\"Symbol\"]} - {self.item[\"Name\"]}\\'\\n\\n\\n# 주식 티커\\nticker = \"AAPL\"\\n\\nstock = yf.Ticker(ticker)\\n\\nst.title(\"주식 정보 대시보드\")\\n\\nsearch_query = st.text_input(\"검색창\")\\nhits = search_stocks(search_query)[\"hits\"]\\nsearch_results = [SearchResults(hit) for hit in hits]\\n\\nselected = st.selectbox(\"검색결과 리스트\", search_results)\\n\\ntabs = [\"회사 기본 정보\", \"AI 분석 보고서\", \"종목 토론실\"]\\ntab1, tab2, tab3 = st.tabs(tabs)\\n\\nwith tab1:\\n    st.write(f\"선택된 항목:{selected[\\'Symbol\\']}\")\\n    재무제표데이터 = 재무제표처리()\\n    st.header(\"회사 기본 정보\")\\n\\n    # Balance Sheet 시각화\\n    st.subheader(f\"{ticker} - Balance Sheet\")\\n    balance_sheet_df = pd.DataFrame([재무제표데이터[\"Balance Sheet\"]])\\n    ax = balance_sheet_df.plot(kind=\"bar\", figsize=(10, 6))\\n    plt.title(f\"{ticker} - Balance Sheet\")\\n    plt.ylabel(\"Value in USD\")\\n    plt.xlabel(\"Metrics\")\\n    plt.xticks(rotation=45)\\n    plt.tight_layout()\\n    st.pyplot(fig=ax.figure)\\n\\n    # Income Statement 시각화\\n    st.subheader(f\"{ticker} - Income Statement\")\\n    income_statement_df = pd.DataFrame([재무제표데이터[\"Income Statement\"]])\\n    ax = income_statement_df.plot(kind=\"bar\", figsize=(10, 6))\\n    plt.title(f\"{ticker} - Income Statement\")\\n    plt.ylabel(\"Value in USD\")\\n    plt.xlabel(\"Metrics\")\\n    plt.xticks(rotation=45)\\n    plt.tight_layout()\\n    st.pyplot(fig=ax.figure)\\n\\n    # Cash Flow Statement 시각화\\n    st.subheader(f\"{ticker} - Cash Flow Statement\")\\n    cash_flow_df = pd.DataFrame([재무제표데이터[\"Cash Flow Statement\"]])\\n    ax = cash_flow_df.plot(kind=\"bar\", figsize=(10, 6))\\n    plt.title(f\"{ticker} - Cash Flow Statement\")\\n    plt.ylabel(\"Value in USD\")\\n    plt.xlabel(\"Metrics\")\\n    plt.xticks(rotation=45)\\n    plt.tight_layout()\\n    st.pyplot(fig=ax.figure)\\n\\n    # 가치주 출력##################################################################################################\\n    st.subheader(f\"Value Metrics\")\\n    # 가치주() 함수 실행\\n    data = 가치주()\\n\\n    # 지표 값 추출\\n    pe_ratio = data[\"pe_ratio\"]\\n    pb_ratio = data[\"pb_ratio\"]\\n    peg_ratio = data[\"peg_ratio\"]\\n    dividend_yield = data[\"dividend_yield\"]\\n    ev_ebitda_ratio = data[\"ev_ebitda_ratio\"]\\n\\n    # 시각화할 값과 레이블 설정\\n    labels = [\"P/E Ratio\", \"P/B Ratio\", \"PEG Ratio\", \"Dividend Yield\", \"EV/EBITDA\"]\\n    values = [pe_ratio, pb_ratio, peg_ratio, dividend_yield, ev_ebitda_ratio]\\n\\n    # Matplotlib 막대 그래프 생성\\n    fig, ax = plt.subplots()\\n    ax.bar(labels, values, color=[\"blue\", \"green\", \"red\", \"purple\", \"orange\"])\\n    ax.set_title(\"Stock Valuation Ratios\")\\n    ax.set_ylabel(\"Ratio Value\")\\n    ax.set_xticklabels(labels, rotation=45)\\n\\n    # 그래프를 Streamlit에 표시\\n    st.pyplot(fig)\\n\\n    # 데이터 프레임으로 생성\\n    # 데이터 프레임 생성\\n    df = pd.DataFrame(data.items(), columns=[\"Metric\", \"Value\"])\\n\\n    # Streamlit에 데이터 프레임 표시\\n    st.subheader(\"가치주 지표 데이터 프레임\")\\n    st.dataframe(df)\\n\\n    # # 데이터 프레임을 이용한 막대 차트\\n    # st.subheader(\"가치주 지표 막대 차트\")\\n    # st.bar_chart(df.set_index(\"Metric\"))\\n\\n    # 우량주 출력 ##################################################################################################\\n    st.title(\"우량주 지표 시각화\")\\n\\n    # 우량주() 함수 실행\\n    data2 = 우량주()\\n\\n    # 데이터 프레임 생성\\n    df = pd.DataFrame(data2.items(), columns=[\"Metric\", \"Value\"])\\n\\n    # Streamlit에 데이터 프레임 표시\\n    st.subheader(\"우량주 지표 데이터 프레임\")\\n    st.dataframe(df)\\n\\n    # # 데이터 프레임을 이용한 막대 차트\\n    # st.subheader(\"우량주 지표 막대 차트\")\\n    # st.bar_chart(df.set_index(\"Metric\"))\\n\\n    # 거래량 출력 ##################################################################################################\\n    st.title(\"거래량 지표 시각화\")\\n    # 거래량 데이터만 추출\\n    st.subheader(f\"{ticker} 주식 가격\")\\n    data3 = 거래량()\\n\\n    # Matplotlib를 사용하여 시각화\\n    fig, ax = plt.subplots(figsize=(10, 6))\\n\\n    # 날짜별로 Open, High, Low, Close 데이터 표시\\n    ax.plot(data3[\"volume\"].index, data3[\"open\"], label=\"Open\", color=\"blue\")\\n    ax.plot(data3[\"volume\"].index, data3[\"high\"], label=\"High\", color=\"green\")\\n    ax.plot(data3[\"volume\"].index, data3[\"low\"], label=\"Low\", color=\"red\")\\n    ax.plot(data3[\"volume\"].index, data3[\"close\"], label=\"Close\", color=\"purple\")\\n\\n    ax.set_xlabel(\"Date\")\\n    ax.set_ylabel(\"Price (USD)\")\\n    ax.set_title(f\"{ticker} stock price,(latest 1month)\")\\n    ax.legend()\\n\\n    # Streamlit에 그래프 출력\\n    st.pyplot(fig)\\n\\n    # 거래량 데이터도 별도로 표시\\n    st.subheader(f\"{ticker} 주식 거래량 데이터\")\\n    st.line_chart(data3[\"volume\"])\\n\\n\\nwith tab2:\\n    st.header(\"AI분석 보고서\")\\n    if st.button(\"보고서 불러오기\"):\\n        with st.spinner(text=\"In progress\"):\\n            st.write(AI_report())\\n        st.success(\"Done\")\\n    # st.write(\"여기에 AI분석 보고서를 추가하세요.\")\\n\\nwith tab3:\\n    st.header(\"종목 토론실\")\\n    st.write(\"여기에 종목 토론실 내용을 추가하세요.\")\\n\\n# yfinance python 라이브러리에서 주요 항목에 데이터 시각화를 하려고 합니다. 일반적인 방법을 알려주세요\\n'),\n",
       " Document(metadata={'source': '..\\\\별도\\\\2.Streamlit을 활용한 프로젝트\\\\app2.py'}, page_content='import streamlit as st\\nimport sqlite3\\n\\nfrom stock_info import stock\\nfrom backend import AI_report\\nfrom meilisearch_search import search_stocks\\nfrom comments import create_connection, create_table, insert_comment, get_all_comments\\n\\n# from comment import create_connection, create_table, insert_comment, get_all_comments\\n\\n\\n# 캐시를 이용한다.\\n@st.cache_data\\ndef cache_AI_report(ticker):\\n    return AI_report(ticker)\\n\\n\\nclass SearchResult:\\n    def __init__(self, item):\\n        self.item = item\\n\\n    @property\\n    def symbol(self):\\n        return self.item[\"Symbol\"]\\n\\n    @property\\n    def name(self):\\n        return self.item[\"Name\"]\\n\\n    def __str__(self):\\n        return f\"{self.symbol}: {self.name}\"\\n\\n\\n# Set the page title\\nst.title(\"주식 정보 분석 대시보드\")\\n\\n# Create a text input for search\\nsearch_query = st.text_input(\"검색창\")\\nhits = search_stocks(search_query)[\"hits\"]\\nsearch_results = [SearchResult(hit) for hit in hits]\\n\\n# Create a select box for search results list\\n\\nselected = st.selectbox(\"검색 결과 리스트\", search_results)\\n\\n# Create tabs for different sections\\ntabs = [\"회사 기본 정보\", \"AI 분석 보고서\", \"종목 토론실\"]\\ntab1, tab2, tab3 = st.tabs(tabs)\\n\\n# Content for \"회사 기본 정보\" tab\\nwith tab1:\\n    stock = stock(selected.symbol)\\n    st.header(str(selected))\\n    # 거래량 시각화\\n    st.subheader(f\"거래량\")\\n    stock_data = stock.금융정보()\\n    st.line_chart(stock_data[\"history\"][\"Volume\"])\\n\\n    st.header(\"재무제표\")\\n    cols = st.columns(3)\\n    cols[0].subheader(\"매출액\")\\n    cols[0].line_chart(stock_data[\"income_statement\"].loc[\"Total Revenue\"])\\n    cols[1].subheader(\"순이익\")\\n    cols[1].line_chart(stock_data[\"income_statement\"].loc[\"Net Income\"])\\n    cols[2].subheader(\"영업이익\")\\n    cols[2].line_chart(stock_data[\"income_statement\"].loc[\"Operating Income\"])\\n\\n    cols = st.columns(3)\\n    cols[0].subheader(\"자산\")\\n    cols[0].line_chart(stock_data[\"balance_sheet\"].loc[\"Total Assets\"])\\n    cols[1].subheader(\"부채\")\\n    cols[1].line_chart(\\n        stock_data[\"balance_sheet\"].loc[\"Total Liabilities Net Minority Interest\"]\\n    )\\n    cols[2].subheader(\"자본\")\\n    cols[2].line_chart(stock_data[\"balance_sheet\"].loc[\"Stockholders Equity\"])\\n\\n    cols = st.columns(4)\\n    cols[0].subheader(\"영업 현금흐름\")\\n    cols[0].line_chart(stock_data[\"cash_flow\"].loc[\"Operating Cash Flow\"])\\n    cols[1].subheader(\"투자 현금흐름\")\\n    cols[1].line_chart(stock_data[\"cash_flow\"].loc[\"Investing Cash Flow\"])\\n    cols[2].subheader(\"재무 현금흐름\")\\n    cols[2].line_chart(stock_data[\"cash_flow\"].loc[\"Financing Cash Flow\"])\\n    cols[3].subheader(\"순 현금흐름\")\\n    cols[3].line_chart(stock_data[\"cash_flow\"].loc[\"Free Cash Flow\"])\\n\\n# Content for \"AI 분석 보고서\" tab\\nwith tab2:\\n    st.header(\"AI 분석 보고서\")\\n    if st.button(\"보고서 불러오기\"):\\n        with st.spinner(text=\"In progress\"):\\n            data = cache_AI_report(selected.symbol)\\n            st.success(\"Done\")\\n        st.write(data)\\n\\n# Content for \"종목 토론실\" tab\\nwith tab3:\\n    st.header(\"종목 토론실\")\\n    conn = create_connection()\\n    create_table(conn)\\n\\n    for comment in get_all_comments(conn):\\n        comment_time, comment_text = comment\\n        st.write(f\"{comment_time}: {comment_text}\")\\n\\n    # 앞에서부터 그리기 때문에 댓글 입력창이 위에 나옴\\n    new_comment = st.text_area(\"댓글을 입력하세요\")\\n    if st.button(\"댓글 작성\"):\\n        insert_comment(conn, f\"{selected.name} - {new_comment}\")\\n        st.success(\"댓글이 작성되었습니다\")\\n        st.rerun()\\n'),\n",
       " Document(metadata={'source': '..\\\\별도\\\\2.Streamlit을 활용한 프로젝트\\\\backend.py'}, page_content='from dotenv import load_dotenv\\n\\nload_dotenv()\\n\\nfrom langchain_openai import ChatOpenAI\\nfrom langchain_core.prompts import ChatPromptTemplate\\nfrom langchain_core.output_parsers import StrOutputParser\\nfrom langchain_core.runnables import RunnablePassthrough\\n\\n# from yfinance_data import 재무제표, 가치주, 우량주, 거래량\\nfrom stock_info import stock\\n\\nllm = ChatOpenAI(model=\"gpt-4o-mini\")\\n\\n\\ndef AI_report(ticker):\\n    \"\"\"\\n    분석에 필요한 정보를 제공해드립니다.\\n    세 개의 따옴표가 포함된 마크다운 보고서가 제공됩니다.\\n    재무 분석가로서 보고서에 담긴 수치를 자세히 살펴보고\\n    회사의 성장 추세와 재무 안정성을 평가하여 사용자들이 자유롭게 토론할 수 있도록 돕습니다.\\n    사람들이 공개 토론을 할 수 있도록 귀하의 의견을 제공하십시오.\\n    보고서를 한국어로 제출해주세요.\\n    \"\"\"\\n    prompt = ChatPromptTemplate.from_messages(\\n        [\\n            (\\n                \"system\",\\n                \"\"\"\\n        I want you to act as a Financial Analyst.\\n        Want assistance provided by qualified individuals enabled with experience on understanding charts using technical analysis tools while interpreting macroeconomic environment prevailing across world consequently assisting customers acquire long term advantages requires clear verdicts therefore seeking same through informed predictions written down precisely! First statement contains following content- “Can you tell us what future stock market looks like based upon current conditions ?”.\\n        \"\"\",\\n            ),\\n            (\\n                \"user\",\\n                \\'\\'\\' \\n        We provide the information necessary for analysis.\\n        Given markdown reports with triple quotes. \\n        As a Financial Analyst, Take a closer look at the numbers in the report and evaluate the company\\'s growth trends and financial stability to help users discuss freely.\\n        Provide your opinion to people so they can have an open discussion.\\n        Please provide the report in Korean.\\n\\n        \"\"\"\\n        {markdown}\\n        \"\"\"\\n        \\'\\'\\',\\n            ),\\n        ]\\n    )\\n    output_parser = StrOutputParser()\\n\\n    chain = prompt | llm | output_parser\\n    return chain.invoke({\"markdown\": stock(ticker).report_support()})\\n\\n\\n# def AI_report():\\n#     prompt = ChatPromptTemplate.from_messages(\\n#         [\\n#             (\\n#                 \"system\",\\n#                 \"\"\"\\n#             I want you to act as a Fianancial Analyst.\\n#             Want assistance provided by qualified individuals enabled with experience on understanding charts using technical analysis tools while interpreting macroeconomic environment prevailing across world consequently assisting customers acquire long term advantages requires clear verdicts therefore seeking same through informed predictions written down precisely! First statement contains following content- “Can you tell us what future stock market looks like based upon current conditions ?\".\\n#             \"\"\",\\n#             ),\\n#             (\\n#                 \"user\",\\n#                 \"\"\"\\n#                 As a financial analysis expert, please carefully examine these tables and analyze them to enable a free discussion on evaluating the company\\'s growth or value.\\n#                 You Answer Korean.\\n#                 #Balance Sheet: {재무제표}\\n#                 #Value Stock: {가치주}\\n#                 #Blue Chip Stock: {우량주}\\n#                 #Volume: {거래량}\\n#                 #Question: {input}\\n#             \"\"\",\\n#             ),\\n#         ]\\n#     )\\n\\n#     chain = prompt | llm | StrOutputParser()\\n#     return chain.invoke(\\n#         {\\n#             \"input\": \"애플 주간 차트를 분석해 주세요.\",\\n#             \"재무제표\": 재무제표(),\\n#             \"가치주\": 가치주(),\\n#             \"우량주\": 우량주(),\\n#             \"거래량\": 거래량(),\\n#         },\\n#     )\\n'),\n",
       " Document(metadata={'source': '..\\\\별도\\\\2.Streamlit을 활용한 프로젝트\\\\comments.py'}, page_content='import sqlite3\\n\\n\"\"\"\\nstreamlit 으로 간단한 익명 댓글 기능을 구현하려고 합니다.\\nSQLite 를 사용해서 댓글을 보관할 수 있도록 예시코드를 작성해 주세요\\n\"\"\"\\n# import sqlite3\\n\\n\\n# SQLite 데이터베이스 연결 함수\\ndef create_connection():\\n    conn = sqlite3.connect(\"comments.db\")\\n    return conn\\n\\n\\n# 댓글 테이블 생성 함수\\ndef create_table(conn):\\n    with conn:\\n        conn.execute(\\n            \"\"\"\\n            CREATE TABLE IF NOT EXISTS comments (\\n                id INTEGER PRIMARY KEY AUTOINCREMENT,\\n                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\\n                comment TEXT NOT NULL\\n            )\\n        \"\"\"\\n        )\\n\\n\\n# 댓글 삽입 함수\\ndef insert_comment(conn, comment):\\n    with conn:\\n        conn.execute(\"INSERT INTO comments (comment) VALUES (?)\", (comment,))\\n\\n\\n# 모든 댓글 조회 함수\\ndef get_all_comments(conn):\\n    with conn:\\n        comments = conn.execute(\\n            \"SELECT created_at, comment FROM comments ORDER BY created_at DESC\"\\n        ).fetchall()\\n    yield from comments\\n'),\n",
       " Document(metadata={'source': '..\\\\별도\\\\2.Streamlit을 활용한 프로젝트\\\\meilisearch_search.py'}, page_content='# meilisearch-windows-amd64을 켜야함\\n# meilisearch-windows-amd64을 켜야함\\nimport meilisearch\\nimport json\\nimport pandas as pd\\n\\nclient = meilisearch.Client(\"http://localhost:7700\", \"aSampleMasterKey\")\\n\\n# json_file = open(\"movies.json\", encoding=\"utf-8\")\\n# movies = json.load(json_file)\\n# client.index(\"movies\").add_documents(movies)\\n\\n# client.index(\"movies\").search(\"botman\")\\n\\ndf = pd.read_csv(\"nasdaq_screener_1726112812897.csv\", na_filter=False)\\nd = df.to_dict(orient=\"records\")  # column 하나에 row하나 cell하나\\n\\n# 정규식 replace와 strip을 통해 띄어쓰기 제거한것을 id column에 저장\\ndf[\"id\"] = df[\"Symbol\"].str.strip().replace(r\"[/^]\", \"_\", regex=True)\\n\\nclient.index(\"stocks\").add_documents(df.to_dict(orient=\"records\"), primary_key=\"id\")\\n\\n\\n# 인덱스검색 함수\\ndef search_stocks(query):\\n    index = client.index(\"stocks\")\\n    res = index.search(query)\\n    return res\\n'),\n",
       " Document(metadata={'source': '..\\\\별도\\\\2.Streamlit을 활용한 프로젝트\\\\stock_info.py'}, page_content='import pandas as pd\\nimport yfinance as yf\\n\\n\\nclass stock:\\n    def __init__(self, ticker) -> None:\\n        # 주식 티커 설정\\n        self.ticker = ticker\\n\\n        # 티커 데이터 가져오기\\n        self.stock = yf.Ticker(self.ticker)\\n\\n    def 금융정보(self):\\n        return {\\n            \"info\": self.stock.info,\\n            \"income_statement\": self.stock.quarterly_income_stmt,\\n            \"balance_sheet\": self.stock.balance_sheet,\\n            \"cash_flow\": self.stock.cash_flow,\\n            \"history\": self.stock.history(period=\"1mo\"),\\n        }\\n\\n    def report_support(self):\\n        # 금융 전문가의 분석을 보조한 지표들.\\n        # stock.info에 있는것들을 모두 DataFrame으로 바꿨다.\\n\\n        # 숫자만 필터링하는 함수\\n        def is_float(x):\\n            try:\\n                float(x)\\n                return True\\n            except ValueError:\\n                return False\\n            except TypeError:\\n                return False\\n\\n        stock = self.stock\\n        info = pd.DataFrame.from_dict(stock.info, orient=\"index\", columns=[\"Value\"])\\n        info = info[info[\"Value\"].apply(is_float)]\\n\\n        return f\\'\\'\\'\\n        ### Financials\\n        {info.to_markdown()}\\n\\n        #### Quarterly Income Statement\\n        {stock.quarterly_income_stmt.loc[[\\'Total Revenue\\', \\'Gross Profit\\', \\'Operating Income\\', \\'Net Income\\']].to_markdown()}\"\"\"\\n\\n        #### Quarterly Balance Sheet\\n        {stock.quarterly_balance_sheet.loc[[\\'Total Assets\\', \\'Total Liabilities Net Minority Interest\\', \\'Stockholders Equity\\']].to_markdown()}\"\"\"\\n\\n        #### Quarterly Cash Flow\\n        {stock.quarterly_cash_flow.loc[[\\'Operating Cash Flow\\', \\'Investing Cash Flow\\', \\'Financing Cash Flow\\']].to_markdown()}\"\"\"\\n        \\'\\'\\'\\n'),\n",
       " Document(metadata={'source': '..\\\\별도\\\\2.Streamlit을 활용한 프로젝트\\\\yfinance_data.py'}, page_content='import yfinance as yf\\nimport pandas as pd\\n\\n\\n# ChatGPT질문: yfinance라이브러리를 사용하여 재무재표를 가져오는 방법을 알려주헤요\\n\\n\\n# 주식 티커\\nticker = \"AAPL\"\\n\\n# 티커 데이터 가져오기\\nstock = yf.Ticker(ticker)\\n\\n# 재무제표 데이터 가져오기\\nincome_statement = stock.financials\\nbalance_sheet = stock.balance_sheet\\ncash_flow = stock.cashflow\\n\\n\\n# ChatGPT질문: yfinance 라이브러리에서 저평가된 가치주를 보려면 어떤 지표를 봐야 하나요?\\ndef 재무제표():\\n    income_statement = stock.financials\\n    balance_sheet = stock.balance_sheet\\n    cash_flow = stock.cashflow\\n    return {\\n        \"income_statement\": income_statement,\\n        \"balance_sheet\": balance_sheet,\\n        \"cash_flow\": cash_flow,\\n    }\\n\\n\\n# yfinance python 라이브러리에서 주요 항목에 데이터 시각화를 하려고 합니다. 일반적인 방법을 알려주세요\\ndef 재무제표처리():\\n    # 재무제표 데이터를 추출하는 함수 정의\\n    # 1. 티커 심볼을 입력하고 해당 주식의 재무 데이터를 가져옴\\n\\n    # 2. 재무 상태표 (Balance Sheet)\\n    balance_sheet = stock.balance_sheet\\n    balance_sheet_data = {\\n        \"Total Assets\": balance_sheet.loc[\"Total Assets\"].iloc[0],  # 총 자산\\n        \"Total Liabilities Net Minority Interest\": balance_sheet.loc[\\n            \"Total Liabilities Net Minority Interest\"\\n        ].iloc[\\n            0\\n        ],  # 총 부채\\n        \"Stockholders Equity\": balance_sheet.loc[\"Stockholders Equity\"].iloc[0],  # 자본\\n    }\\n\\n    # 3. 손익 계산서 (Income Statement)\\n    income_statement = stock.financials\\n    income_statement_data = {\\n        \"Total Revenue\": income_statement.loc[\"Total Revenue\"].iloc[0],  # 총 매출\\n        \"Net Income\": income_statement.loc[\"Net Income\"].iloc[0],  # 순이익\\n        \"Operating Income\": income_statement.loc[\"Operating Income\"].iloc[\\n            0\\n        ],  # 영업이익\\n    }\\n\\n    # 4. 현금 흐름표 (Cash Flow Statement)\\n    cashflow_statement = stock.cashflow\\n    cashflow_statement_data = {\\n        \"Operating Cash Flow\": cashflow_statement.loc[\\n            \"Cash Flow From Continuing Operating Activities\"\\n        ].iloc[\\n            0\\n        ],  # 영업활동으로 인한 현금흐름\\n        \"Investing Cash Flow\": cashflow_statement.loc[\\n            \"Cash Flow From Continuing Investing Activities\"\\n        ].iloc[\\n            0\\n        ],  # 투자활동으로 인한 현금흐름\\n        \"Free Cash Flow\": cashflow_statement.loc[\"Free Cash Flow\"].iloc[\\n            0\\n        ],  # 자유 현금흐름\\n    }\\n\\n    # 5. 세 가지 재무제표 딕셔너리를 묶어서 반환\\n    financial_data = {\\n        \"Balance Sheet\": balance_sheet_data,\\n        \"Income Statement\": income_statement_data,\\n        \"Cash Flow Statement\": cashflow_statement_data,\\n    }\\n\\n    return financial_data\\n\\n\\nimport matplotlib.pyplot as plt\\n\\n\\ndef 재무제표시각화():\\n    # 재무제표 데이터 가져오기\\n    재무제표데이터 = 재무제표처리()\\n\\n    # 데이터가 딕셔너리 형태로 반환되므로 DataFrame으로 변환\\n    balance_sheet = pd.DataFrame([재무제표데이터[\"Balance Sheet\"]])\\n    income_statement = pd.DataFrame([재무제표데이터[\"Income Statement\"]])\\n    cash_flow = pd.DataFrame([재무제표데이터[\"Cash Flow Statement\"]])\\n\\n    # 1. Balance Sheet 시각화\\n    plt.figure(figsize=(14, 7))\\n    balance_sheet.plot(kind=\"bar\")\\n    plt.title(f\"{ticker} - Balance Sheet\")\\n    plt.ylabel(\"Value in USD\")\\n    plt.xlabel(\"Metrics\")\\n    plt.xticks(rotation=45)\\n    plt.tight_layout()\\n    plt.show()\\n\\n    # 2. Income Statement 시각화\\n    plt.figure(figsize=(14, 7))\\n    income_statement.plot(kind=\"bar\")\\n    plt.title(f\"{ticker} - Income Statement\")\\n    plt.ylabel(\"Value in USD\")\\n    plt.xlabel(\"Metrics\")\\n    plt.xticks(rotation=45)\\n    plt.tight_layout()\\n    plt.show()\\n\\n    # 3. Cash Flow Statement 시각화\\n    plt.figure(figsize=(14, 7))\\n    cash_flow.plot(kind=\"bar\")\\n    plt.title(f\"{ticker} - Cash Flow Statement\")\\n    plt.ylabel(\"Value in USD\")\\n    plt.xlabel(\"Metrics\")\\n    plt.xticks(rotation=45)\\n    plt.tight_layout()\\n    plt.show()\\n\\n    # 3. Cash Flow Statement 시각화 (영업활동, 투자활동, 자유현금흐름)\\n    cash_flow_data = cash_flow.loc[\\n        [\\n            \"Total Cash From Operating Activities\",\\n            \"Total Cashflows From Investing Activities\",\\n            \"Free Cash Flow\",\\n        ]\\n    ].T\\n    cash_flow_data.plot(kind=\"bar\", figsize=(10, 6))\\n    plt.title(f\"{ticker} - Cash Flow Statement\")\\n    plt.ylabel(\"Value in USD\")\\n    plt.xlabel(\"Date\")\\n    plt.xticks(rotation=45)\\n    plt.tight_layout()\\n    plt.show()\\n\\n\\ndef 가치주():\\n    # 과거 P/E 비율\\n    pe_ratio = stock.info.get(\"forwardPE\")\\n    # P/B 비율\\n    pb_ratio = stock.info.get(\"priceToBook\")\\n    # PEG 비율\\n    peg_ratio = stock.info.get(\"pegRatio\")\\n    # 배당수익률\\n    dividend_yield = stock.info.get(\"dividendYield\")\\n    # EV/EBITDA 비율\\n    ev_ebitda_ratio = stock.info.get(\"enterpriseToEbitda\")\\n\\n    # forward_pe = stock.info.get(\\'forwardPE\\')  # 미래 P/E 비율\\n    # roe = stock.info.get(\\'returnOnEquity\\')  # ROE\\n    # current_ratio = stock.info.get(\\'currentRatio\\')  # 유동성 비율\\n    # debt_to_equity = stock.info.get(\\'debtToEquity\\')  # 부채비율\\n    return {\\n        \"pe_ratio\": pe_ratio,\\n        \"pb_ratio\": pb_ratio,\\n        \"peg_ratio\": peg_ratio,\\n        \"dividend_yield\": dividend_yield,\\n        \"ev_ebitda_ratio\": ev_ebitda_ratio,\\n    }\\n\\n\\n# ChatGPT질문: 우량주를 잘 찾으려면어떤 지표를 봐야하나요?\\ndef 우량주():\\n    # ROE (자기자본이익률)\\n    roe = stock.info.get(\"returnOnEquity\")\\n    # ROA (자산수익률ㄹ)\\n    roa = stock.info.get(\"returnOnAssets\")\\n    # 부채비율\\n    debt_to_equity = stock.info.get(\"debtToEquity\")\\n    # 이자 및 세전 이익\\n    interest_coverage = stock.info.get(\"ebitda\")\\n    # 매출 성장률\\n    revenue_growth = stock.info.get(\"revenueGrowth\")\\n    # 배당 지급률\\n    dividend_ratio = stock.info[\"payoutRatio\"]\\n    # EPS (주당순이익)\\n    eps = stock.info.get(\"trailingEps\")\\n    # 현금흐름\\n    operating_cash_flow = stock.info[\"operatingCashflow\"]\\n\\n    return {\\n        \"roe\": roe,\\n        \"roa\": roa,\\n        \"debt_to_equity\": debt_to_equity,\\n        \"interest_coverage\": interest_coverage,\\n        \"revenue_growth\": revenue_growth,\\n        \"dividend_ratio\": dividend_ratio,\\n        \"eps\": eps,\\n        \"operating_cash_flow\": operating_cash_flow,\\n    }\\n\\n\\ndef 거래량():\\n    hist = stock.history(period=\"1mo\")\\n    return {\\n        \"volume\": hist[\"Volume\"],\\n        \"open\": hist[\"Open\"],\\n        \"high\": hist[\"High\"],\\n        \"low\": hist[\"Low\"],\\n        \"close\": hist[\"Close\"],\\n    }\\n'),\n",
       " Document(metadata={'source': '..\\\\별도\\\\2.Streamlit을 활용한 프로젝트2-wine_paring\\\\backend.py'}, page_content='from dotenv import load_dotenv\\n\\nfrom langchain_core.prompts import ChatPromptTemplate\\nfrom langchain_openai import ChatOpenAI\\nfrom langchain_core.output_parsers import StrOutputParser\\n\\n\\nload_dotenv()\\nsystem_template = \"\"\"\\nYou are ChatGPT, a professional sommelier who has gone through a rigorous training process step by step, driven by a deep curiosity about wine. You possess a keen sense of smell, a keen sense of exploration, and an awareness of the many details in wine.\\n\\nYour task is to accurately identify an appropriate wine pairing based on the provided food description in triple quotes.\\n\\nYou anser Korean.\\n\\n### Task Instructions:\\n\\n\\n**Pairing Recommendation**:\\n- Recommend a specific wine (including grape variety, region of origin, and possible vintage) that pairs well with the described food.\\n- Explain why this wine is a suitable match for the food, taking into account factors such as acidity, tannin structure, body, and flavor profile.\\n\\n### Example:\\n\\n**Review**\\n\\n\"\"\"\\n\\nprompt_template = ChatPromptTemplate.from_messages(\\n    [(\"system\", system_template), (\"user\", \"{text}\")]\\n)\\n\\n\\nllm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\\nparser = StrOutputParser()\\n\\nchain = prompt_template | llm | parser\\nresponse = chain.invoke(\\n    {\\n        \"text\": \"와인은 레드 와인으로, 풍부한 과일향이 나며, 부드렆고 깊은맛이 난다. 탄닌이 적당하고 산도가 높다.  어울리는 음식을 추천해주세요.\"\\n    }\\n)\\n'),\n",
       " Document(metadata={'source': '..\\\\별도\\\\2.Streamlit을 활용한 프로젝트2-wine_paring\\\\openai_runnable.py'}, page_content='from openai import OpenAI\\nfrom langchain_core.output_parsers import StrOutputParser\\n\\nclient = OpenAI()\\n\\n\\ndef openai_runnable(query):\\n    response = client.chat.completions.create(\\n        model=\"gpt-4o-mini\",\\n        messages=[\\n            {\\n                \"role\": \"system\",\\n                \"content\": [\\n                    {\\n                        \"type\": \"text\",\\n                        \"text\": \"You are ChatGPT, a professional sommelier who has gone through a rigorous training process step by step, driven by a deep curiosity about wine. You possess a keen sense of smell, a keen sense of exploration, and an awareness of the many details in wine.\\\\r\\\\n\\\\r\\\\nYour task is to accurately identify an appropriate wine pairing based on the provided food description in triple quotes.\\\\r\\\\n\\\\r\\\\nYou anser Korean.\\\\r\\\\n\\\\r\\\\n### Task Instructions:\\\\r\\\\n\\\\r\\\\n\\\\r\\\\n**Pairing Recommendation**:\\\\r\\\\n- Recommend a specific wine (including grape variety, region of origin, and possible vintage) that pairs well with the described food.\\\\r\\\\n- Explain why this wine is a suitable match for the food, taking into account factors such as acidity, tannin structure, body, and flavor profile.\\\\r\\\\n\\\\r\\\\n### Example:\\\\r\\\\n\\\\r\\\\n**Review**\\\\r\\\\n\",\\n                    }\\n                ],\\n            },\\n            {\\n                \"role\": \"user\",\\n                \"content\": [\\n                    {\\n                        \"type\": \"image_url\",\\n                        \"image_url\": {\\n                            \"url\": \"https://images.vivino.com/thumbs/tiV02HEuQPaNoSRcWA3r2g_pb_x600.png\"\\n                        },\\n                    },\\n                    {\\n                        \"type\": \"text\",\\n                        \"text\": query,\\n                    },\\n                ],\\n            },\\n            {\\n                \"role\": \"assistant\",\\n                \"content\": [\\n                    {\\n                        \"type\": \"text\",\\n                        \"text\": \"**음식 추천**: \\\\r\\\\n아마로네 델라 발폴리첼라 와인은 깊고 복합적인 맛을 갖고 있어, 진한 소스가 어우러진 스테이크나 양고기 요리와 잘 어울립니다. 또한, 그릴에 구운 채소나 구운 버섯 같은 풍미가 강한 채소와도 훌륭한 조화를 이룹니다.\\\\n\\\\n**추천 이유**: \\\\r\\\\n아마로네는 강한 바디와 풍부한 타닌을 가지고 있어, 기름진 고기 요리와 함께할 때 그 맛을 더욱 부각시킵니다. 이 와인의 산도는 요리의 기름진 맛을 상쇄해 주며, 과일과 향신료의 복합적인 풍미는 고기와 채소의 맛을 한층 더 깊게 만들어 줍니다.\",\\n                    }\\n                ],\\n            },\\n        ],\\n        temperature=0,\\n        max_tokens=2048,\\n        top_p=1,\\n        frequency_penalty=0,\\n        presence_penalty=0,\\n        response_format={\"type\": \"text\"},\\n    )\\n    # return response\\n    return \"\".join([x.message.content for x in response.choices])\\n\\n\\nfrom langchain_core.runnables import RunnableLambda\\n\\n# 파일을 실행할때  직접적으로 여기 안에 있는것들을 사용\\nif __name__ == \"__main__\":\\n    runnable = RunnableLambda(openai_runnable)\\n\\n    chain = runnable | StrOutputParser()\\n    response = chain.invoke(\"이 와인과 어울리는 음식을 추천해 주세요\")\\n'),\n",
       " Document(metadata={'source': '..\\\\별도\\\\2.Streamlit을 활용한 프로젝트2-wine_paring\\\\retrieval.py'}, page_content='')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PythonLoader\n",
    "\n",
    "# 현재폴더(.) 의 .py 파일을 모두 조회하여 PythonLoader 로 로드\n",
    "loader = DirectoryLoader(\"../\", glob=\"**/*.py\", loader_cls=PythonLoader)\n",
    "\n",
    "# 문서 로드\n",
    "docs = loader.load()\n",
    "docs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-kr-gLkynrUQ-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
