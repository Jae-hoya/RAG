{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader, PDFPlumberLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain.retrievers import BM25Retriever, EnsembleRetriever\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from kiwipiepy import Kiwi\n",
    "\n",
    "# reorder\n",
    "from langchain_community.document_transformers import LongContextReorder\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "# multi-query\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "\n",
    "# reranker\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import CrossEncoderReranker\n",
    "from langchain_community.cross_encoders import HuggingFaceCrossEncoder\n",
    "\n",
    "kiwi = Kiwi()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kiwi 함수\n",
    "def kiwi_tokenize(docs):\n",
    "    return [token.form for token in kiwi.tokenize(docs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaders = [\n",
    "    PDFPlumberLoader(\"data/Adaptive_RAG.pdf\"),\n",
    "    PDFPlumberLoader(\"data/Naive_RAG.pdf\"),\n",
    "    PDFPlumberLoader('data/RAPTOR_RAG.pdf'),\n",
    "    PDFPlumberLoader('data/Self_RAG.pdf')\n",
    "]\n",
    "\n",
    "docs = []\n",
    "\n",
    "for loader in loaders:\n",
    "    docs.extend(loader.load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "87"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensemble Retriever (kiwi bm25 + chroma similarity)\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=50)\n",
    "\n",
    "embedding = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "documents = text_splitter.split_documents(docs)\n",
    "\n",
    "chroma_vector_store = Chroma.from_documents(documents, embedding)\n",
    "chroma_retriever = chroma_vector_store.as_retriever()\n",
    "\n",
    "kiwi_vector_store = BM25Retriever.from_documents(documents, preprocess_func=kiwi_tokenize)\n",
    "\n",
    "ensemble_retriever = EnsembleRetriever(\n",
    "    retrievers=[chroma_retriever, kiwi_vector_store],\n",
    "    weights=[0.5, 0.5],\n",
    "    search_kwargs={\"k\": 10}\n",
    "    #search_type=\"mmr\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MultiQUery Retriever\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "multiquery_retriever = MultiQueryRetriever.from_llm(\n",
    "    retriever = ensemble_retriever,\n",
    "    llm=llm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\skyop\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\langchain-kr-gLkynrUQ-py3.11\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "c:\\Users\\skyop\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\langchain-kr-gLkynrUQ-py3.11\\Lib\\site-packages\\huggingface_hub\\file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Reranker\n",
    "\n",
    "reranker_model = HuggingFaceCrossEncoder(model_name=\"BAAI/bge-reranker-v2-m3\")\n",
    "\n",
    "# 상위 3개 모델 선택\n",
    "compressor = CrossEncoderReranker(model=reranker_model, top_n=3)\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor = compressor, base_retriever = ensemble_retriever\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reorder\n",
    "def reorder_documents(docs):\n",
    "    # 재정렬\n",
    "    reordering = LongContextReorder()\n",
    "    reordered_docs = reordering.transform_documents(docs)\n",
    "    return reordered_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt\n",
    "template = \"\"\"\n",
    "    당신은 주어진 문서에 대해서 QA를 해주는 assistant bot입니다.\n",
    "    주어진 문서를 이용해서 답변해 주세요\n",
    "    만약 주어진 질문에 대해서 모른다면, 모른다고 답변해 주세요\n",
    "    문서의 출처와 페이지 넘버를 작성해주세요\n",
    "    한국어로 답변해주세요\n",
    "\n",
    "    #Example Format:\n",
    "    (brief summary of the answer)\n",
    "    (table)\n",
    "    (detailed answer to the question)\n",
    "\n",
    "    **출처**\n",
    "    - (page source and page number)\n",
    "\n",
    "    # Question:\n",
    "    {question}\n",
    "\n",
    "    # Context:\n",
    "    {context}\n",
    "\n",
    "    # Answer\n",
    "\"\"\"\n",
    "prompt = PromptTemplate.from_template(template, input_variable=[\"question\", \"context\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chain\n",
    "chain = ({\"context\":ensemble_retriever| RunnableLambda(reorder_documents), \"question\": RunnablePassthrough()}\n",
    "         |prompt\n",
    "         |llm\n",
    "         |StrOutputParser()\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adaptive-RAG는 적응형 검색 기반 생성 모델로, 복잡한 질문에 대해 효과적으로 대응하기 위해 설계되었습니다. 이 모델은 질문의 복잡성을 분류하는 오라클 분류기를 사용하여, 단순한 질문은 LLM(대형 언어 모델)만으로 처리하고, 복잡한 질문은 검색기와 LLM을 함께 활용하여 해결합니다. Adaptive-RAG는 성능과 효율성 모두에서 우수한 결과를 보여줍니다.\n",
      "\n",
      "| Adaptive-RAG의 특징 | 설명 |\n",
      "|---------------------|------|\n",
      "| 질문 처리 방식     | 단순 질문은 LLM, 복잡한 질문은 검색기와 LLM 결합 |\n",
      "| 성능                | 기존 모델보다 더 효과적이고 효율적 |\n",
      "| 구현 세부사항       | A100 GPU 사용, PyTorch 및 Transformers 라이브러리 활용 |\n",
      "\n",
      "Adaptive-RAG는 복잡한 질문에 대한 적응형 접근 방식을 통해, 다양한 질문 유형에 대해 최적의 성능을 발휘하도록 설계되었습니다. 이 모델은 질문의 복잡성을 기반으로 적절한 처리 전략을 선택하여, 효율적인 정보 검색과 생성 과정을 가능하게 합니다.\n",
      "\n",
      "**출처**\n",
      "- Adaptive_RAG.pdf, 페이지 6, 7, 13\n"
     ]
    }
   ],
   "source": [
    "print(chain.invoke(\"AdaptiveRAG에 대해서 설명해 주세요\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Self-RAG(자기 반영적 검색 증강 생성)는 대형 언어 모델(LLM)의 품질과 사실성을 향상시키기 위해 설계된 프레임워크입니다. 이 프레임워크는 검색과 자기 반영을 통해 LLM이 생성한 텍스트의 질을 높이고, 필요할 경우 검색된 구문을 기반으로 텍스트를 생성하며, 생성된 결과를 비판적으로 평가할 수 있도록 학습합니다. Self-RAG는 일반적인 RAG 접근 방식과 달리, 검색된 자료의 완전한 지원을 보장하지 않고 무작위로 구문을 검색하는 대신, 필요에 따라 검색을 수행하고 결과의 관련성을 확인하는 과정을 포함합니다.\n",
      "\n",
      "| Self-RAG의 주요 특징 |\n",
      "|---------------------|\n",
      "| 검색과 자기 반영을 통한 품질 향상 |\n",
      "| 필요에 따라 검색된 구문을 기반으로 텍스트 생성 |\n",
      "| 생성된 결과에 대한 비판적 평가 |\n",
      "| 사용자 선호에 맞춘 모델 동작 조정 가능 |\n",
      "\n",
      "Self-RAG는 LLM이 자신의 생성 과정을 반영하고, 특정 작업 입력에 대해 작업 출력을 생성하는 동시에 중간에 특별한 토큰을 생성하여 검색의 필요성을 신호합니다. 이러한 방식은 LLM의 창의성과 다재다능성을 해치지 않으면서도 사실적 정확성을 높이는 데 기여합니다.\n",
      "\n",
      "**출처**\n",
      "- Self-RAG.pdf, 페이지 2, 0, 15\n"
     ]
    }
   ],
   "source": [
    "print(chain.invoke(\"Self-RAG에 대해서 설명해 주세요\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-kr-gLkynrUQ-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
